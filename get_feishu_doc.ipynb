{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1f5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import lark_oapi as lark\n",
    "from lark_oapi.api.docs.v1 import *\n",
    "import constants\n",
    "reload(constants)\n",
    "from constants import *\n",
    "from utils import *\n",
    "import json\n",
    "\n",
    "import lark_oapi as lark\n",
    "from lark_oapi.api.docs.v1 import *\n",
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9219fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81006898",
   "metadata": {},
   "source": [
    "## 获取评分 SOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0263cb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app_access_token': 't-g1046ghVNG4AX3E3TKFSVVZCBAG7EJZKPMCWUQ4V', 'code': 0, 'expire': 7200, 'msg': 'ok', 'tenant_access_token': 't-g1046ghVNG4AX3E3TKFSVVZCBAG7EJZKPMCWUQ4V'}\n"
     ]
    }
   ],
   "source": [
    "res = get_access_token(APP_ID, APP_SECRET)\n",
    "access_token = res[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d96fd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lark] [2025-06-16 17:57:05,323] [DEBUG] GET https://open.feishu.cn/open-apis/docs/v1/content 200, headers: {\"User-Agent\": \"oapi-sdk-python/v1.4.18\", \"Authorization\": \"Bearer t-g1046ghVNG4AX3E3TKFSVVZCBAG7EJZKPMCWUQ4V\"}, params: [[\"doc_token\", \"QIvPdelxLonVSQxP8fHcRJ1BnCh\"], [\"doc_type\", \"docx\"], [\"content_type\", \"markdown\"]], body: None\n"
     ]
    }
   ],
   "source": [
    "sop_content = get_feishu_doc_content(RATING_SOP_DOC_TOKEN, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830e8082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# hugging face论文打分标准（动态维护）\\n\\n### AI 论文打分标准（总分 100 分）\\n\\n#### 一、创新性（权重 30 分）\\n\\n核心考量：是否提出突破性想法或对领域有显著推动作用。\\n\\n1. 问题创新性（10 分）\\n\\n    - ❶ 提出全新研究问题，填补领域空白（10 分）；\\n\\n    - ❷ 重新定义已有问题，提供新研究视角（7\\\\-9 分）；\\n\\n    - ❸ 延续经典问题，仅做边缘优化（4\\\\-6 分）；\\n\\n    - ❹ 重复已有研究问题（0\\\\-3 分）。\\n\\n2. 方法创新性（15 分）\\n\\n    - ❶ 提出全新理论框架或算法（13\\\\-15 分）；\\n\\n    - ❷ 对主流方法进行跨领域迁移或重大改进（9\\\\-12 分）；\\n\\n    - ❸ 组合现有方法形成新方案（5\\\\-8 分）；\\n\\n    - ❹ 仅调整参数或应用场景（0\\\\-4 分）。\\n\\n3. 理论创新性（5 分）\\n\\n    - ❶ 建立新理论体系或数学证明（5 分）；\\n\\n    - ❷ 完善现有理论的边界条件（3\\\\-4 分）；\\n\\n    - ❸ 缺乏理论支撑（0\\\\-2 分）。 \\n\\n#### 二、落地价值（权重 25 分，修改后聚焦大模型集成便利性）\\n\\n核心考量：新技术能否以低门槛、高效率集成至现有大模型，实现功能加成与便捷调用。\\n\\n##### 1\\\\. 大模型兼容性设计（10 分）\\n\\n- ❶ 架构级兼容（8\\\\-10 分）\\n技术方案严格遵循主流大模型架构规范（如 Transformer、MoE），支持跨框架（PyTorch/TensorFlow）无缝对接，无需修改大模型核心代码即可嵌入。\\n\\n- ❷ 接口级适配（5\\\\-7 分）\\n提供标准化输入输出接口（如 JSON/ProtoBuf），适配大模型常见调用协议（RESTful/gRPC），但需少量代码调整（\\\\&lt;5% 核心逻辑修改）。\\n\\n- ❸ 局部兼容性（2\\\\-4 分）\\n仅支持特定大模型版本或需改造大模型底层模块（如数据预处理层），集成需修改 10%\\\\-30% 代码。\\n\\n- ❹ 架构冲突（0\\\\-1 分）\\n与主流大模型架构不兼容（如依赖专用硬件 / 非标准算子），需重构大模型核心组件才能集成。\\n\\n##### 2\\\\. 集成成本与复杂度（8 分）\\n\\n- ❶ 轻量化集成（6\\\\-8 分）\\n无需额外依赖（如独立训练数据 / 专用加速库），集成文档完整，可通过 pip 安装或 Docker 镜像一键部署，算力开销增加≤10%。\\n\\n- ❷ 中度适配（3\\\\-5 分）\\n需适配大模型训练 / 推理流程（如调整优化器参数、修改日志模块），依赖 1\\\\-2 个第三方库，算力开销增加 10%\\\\-30%。\\n\\n- ❸ 重度改造（0\\\\-2 分）\\n需重新训练大模型基座（如冻结部分层后微调），或依赖定制化硬件 / 数据集，算力开销增加 \\\\&gt; 30%，集成周期 \\\\&gt; 2 周。\\n\\n##### 3\\\\. 调用便捷性与标准化（5 分）\\n\\n- ❶ 即插即用（4\\\\-5 分）\\n提供开箱即用的 SDK / 工具包，支持大模型主流调用方式（API 调用、本地函数调用），调用示例代码≤10 行，支持动态参数配置。\\n\\n- ❷ 流程化调用（2\\\\-3 分）\\n需按指定步骤初始化（如加载配置文件、注册自定义算子），调用需遵循特定格式，文档覆盖常见错误处理。\\n\\n- ❸ 手动适配（0\\\\-1 分）\\n仅提供底层代码片段，需用户自行封装调用接口，无标准化文档，依赖人工调试。\\n\\n##### 4\\\\. 功能加成普适性（2 分）\\n\\n- ❶ 通用化加成（2 分）\\n技术可适配多类型大模型（如文本、多模态、代码模型），支持跨任务场景（对话 / 生成 / 推理）无缝迁移，无需针对特定模型定制。\\n\\n- ❷ 特定场景加成（1 分）\\n仅适用于单一类型大模型（如仅适配 GPT 架构）或特定任务（如仅提升代码生成能力）。\\n\\n- ❸ 无普适性（0 分）\\n技术与特定大模型强绑定（如硬编码模型参数），无法迁移至其他大模型。\\n\\n#### 三、技术深度（权重 25 分）\\n\\n核心考量：方法的理论严谨性与技术复杂度。\\n\\n1. 理论推导严谨性（10 分）\\n\\n    - ❶ 数学推导完整，有严格理论证明（9\\\\-10 分）；\\n\\n    - ❷ 推导逻辑清晰，但存在部分假设（6\\\\-8 分）；\\n\\n    - ❸ 仅给出公式，缺乏推导过程（3\\\\-5 分）；\\n\\n    - ❹ 理论基础薄弱（0\\\\-2 分）。\\n\\n2. 技术复杂度（10 分）\\n\\n    - ❶ 解决高维、非凸等复杂问题，涉及跨学科技术融合（8\\\\-10 分）；\\n\\n    - ❷ 处理中等复杂度问题，技术路线有一定挑战性（5\\\\-7 分）；\\n\\n    - ❸ 仅在简单场景下应用基础技术（0\\\\-4 分）。\\n\\n3. 技术前瞻性（5 分）\\n\\n    - ❶ 探索前沿方向（如通用人工智能、神经符号融合）（4\\\\-5 分）；\\n\\n    - ❷ 跟进主流方向，有一定技术延伸（2\\\\-3 分）；\\n\\n    - ❸ 重复成熟技术（0\\\\-1 分）。\\n\\n#### 四、实验设计与结果（权重 20 分）\\n\\n核心考量：研究结论的可信度与说服力。\\n\\n1. 实验设计合理性（8 分）\\n\\n    - ❶ 使用多组权威数据集，覆盖不同场景（6\\\\-8 分）；\\n\\n    - ❷ 仅使用单一数据集或自采数据（3\\\\-5 分）；\\n\\n    - ❸ 实验设置不明确，无法复现（0\\\\-2 分）。\\n\\n2. 对比基线全面性（7 分）\\n\\n    - ❶ 对比当前 SOTA 方法及经典算法（6\\\\-7 分）；\\n\\n    - ❷ 仅对比部分主流方法（3\\\\-5 分）；\\n\\n    - ❸ 缺乏对比实验（0\\\\-2 分）。\\n\\n3. 结果分析深度（5 分）\\n\\n    - ❶ 包含消融实验、可视化分析及误差归因（4\\\\-5 分）；\\n\\n    - ❷ 仅展示基础性能指标（2\\\\-3 分）；\\n\\n    - ❸ 结果不显著或缺乏统计验证（0\\\\-1 分）。\\n\\n### 附加说明\\n\\n1. 核心评价逻辑：\\n调整后聚焦 “技术集成性价比”，即新技术能否以低代码修改成本、低算力增量、高标准化程度融入现有大模型生态，而非传统落地价值的行业应用场景。\\n\\n2. 特殊场景处理：\\n\\n    - 若论文提出全新大模型架构（而非加成技术），可将 “落地价值” 权重中的 10 分转移至 “创新性 \\\\- 方法创新性”，避免评分偏差。\\n\\n    - 对于大模型训练优化类技术（如高效训练算法），“集成成本” 评分可侧重数据并行 / 模型并行的兼容性，“调用便捷性” 可关注分布式训练框架适配（如 DeepSpeed/Megatron）。\\n\\n3. 负面扣分项（新增）：\\n\\n    - 若技术依赖大模型厂商私有接口（如仅适配某公司 API），“大模型兼容性设计” 直接扣 5 分；\\n\\n    - 若集成后导致大模型推理延迟增加 \\\\&gt; 50% 且无性能优化方案，“集成成本与复杂度” 扣 3\\\\-5 分。\\n\\n\\n\\n## Few Shot Eg:\\n\\n1. \\\\&\\\\#34;Attention Is All You Need\\\\&\\\\#34;,\\n\\n\\\\&\\\\#34;score\\\\&\\\\#34;: \\\\+\\\\+\\\\+=,\\n\\n\\\\&\\\\#34;reason\\\\&\\\\#34;: \\\\&\\\\#34;提出Transformer架构，开创了自然语言处理新纪元“\\n\\n2. \\\\&\\\\#34;BERT: Pre\\\\-training of Deep Bidirectional Transformers for Language Understanding\\\\&\\\\#34;,\\n\\n\\\\&\\\\#34;score\\\\&\\\\#34;: \\\\+\\\\+\\\\+=,\\n\\n\\\\&\\\\#34;reason\\\\&\\\\#34;: \\\\&\\\\#34;预训练语言模型的里程碑，显著提升了NLP任务性能\\\\&\\\\#34;\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sop_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee34049",
   "metadata": {},
   "source": [
    "## 让 Bot 为单篇论文打分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818acf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_link = \"https://arxiv.org/pdf/2502.14282\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f127010d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- standard request -----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Non-streaming:\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m----- standard request -----\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBOT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# bot-20250611193357-46rqp 为您当前的智能体的ID，注意此处与Chat API存在差异。差异对比详见 SDK使用指南\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_rating_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msop_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaper_link\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(completion.choices[\u001b[32m0\u001b[39m].message.content)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(completion, \u001b[33m\"\u001b[39m\u001b[33mreferences\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\openai\\_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\tasks\\ai-rating-paper\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1234\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1230\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1231\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1232\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1233\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1107\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1109\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 请确保您已将 API Key 存储在环境变量 ARK_API_KEY 中\n",
    "# 初始化Openai客户端，从环境变量中读取您的API Key\n",
    "client = OpenAI(\n",
    "    # 此为默认路径，您可根据业务所在地域进行配置\n",
    "    base_url=\"https://ark.cn-beijing.volces.com/api/v3/bots\",\n",
    "    # 从环境变量中获取您的 API Key\n",
    "    api_key=ARK_API_KEY,\n",
    ")\n",
    "\n",
    "# Non-streaming:\n",
    "print(\"----- standard request -----\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=BOT_ID,  # bot-20250611193357-46rqp 为您当前的智能体的ID，注意此处与Chat API存在差异。差异对比详见 SDK使用指南\n",
    "    messages=get_rating_prompt(sop_content, paper_link),\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n",
    "if hasattr(completion, \"references\"):\n",
    "    print(completion.references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d2aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_ret = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b67c6a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(ai_ret.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e078622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"link\"] = {\n",
    "    \"link\": paper_link,\n",
    "    \"text\": paper_link\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1241cf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'本文针对PC场景下复杂交互环境（密集GUI元素、跨应用工作流）和长序列任务的挑战，提出了PC-Agent框架。核心贡献包括：1）主动感知模块（APM），结合 accessibility tree提取交互元素信息、OCR与意图理解 agent 实现文本精准定位，提升细粒度感知能力；2）分层多代理协作架构，将决策过程分解为指令-子任务-动作三层，通过Manager Agent分解指令与管理子任务依赖、Progress Agent跟踪进度、Decision Agent生成动作，降低决策复杂度；3）反射机制（Reflection Agent），通过 bottom-up 错误反馈调整决策，减少执行误差；4）构建PC-Eval benchmark，包含25个复杂指令（覆盖8个应用），评估代理处理实际PC任务的能力。实验结果显示，PC-Agent在PC-Eval上的任务成功率（56%）较SOTA方法Agent-S（24%）提升32%，子任务成功率（76%）也显著优于对比模型。然而，框架依赖闭源大模型（如GPT-4o），效率与隐私问题有待解决；当前聚焦 productivity 场景，需扩展至社交、娱乐等更多领域。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db1f22be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将这篇论文的分析结果存储到本地\n",
    "with open(\"hello.json\", \"w\") as f:\n",
    "    json.dump([res], f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41032",
   "metadata": {},
   "source": [
    "## 存储单篇文档到飞书"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83696bfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_access_token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m user_access_token = \u001b[43mget_access_token\u001b[49m(APP_ID, APP_SECRET)[\u001b[33m\"\u001b[39m\u001b[33maccess_token\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'get_access_token' is not defined"
     ]
    }
   ],
   "source": [
    "user_access_token = get_access_token(APP_ID, APP_SECRET)[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7ac674",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hello.json\", 'r') as f:\n",
    "    res = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ee3dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 79,\n",
       " 'summary': '本文针对PC场景下复杂交互环境（密集GUI元素、跨应用工作流）和长序列任务的挑战，提出了PC-Agent框架。核心贡献包括：1）主动感知模块（APM），结合 accessibility tree提取交互元素信息、OCR与意图理解 agent 实现文本精准定位，提升细粒度感知能力；2）分层多代理协作架构，将决策过程分解为指令-子任务-动作三层，通过Manager Agent分解指令与管理子任务依赖、Progress Agent跟踪进度、Decision Agent生成动作，降低决策复杂度；3）反射机制（Reflection Agent），通过 bottom-up 错误反馈调整决策，减少执行误差；4）构建PC-Eval benchmark，包含25个复杂指令（覆盖8个应用），评估代理处理实际PC任务的能力。实验结果显示，PC-Agent在PC-Eval上的任务成功率（56%）较SOTA方法Agent-S（24%）提升32%，子任务成功率（76%）也显著优于对比模型。然而，框架依赖闭源大模型（如GPT-4o），效率与隐私问题有待解决；当前聚焦 productivity 场景，需扩展至社交、娱乐等更多领域。',\n",
       " 'link': {'link': 'https://arxiv.org/pdf/2502.14282',\n",
       "  'text': 'https://arxiv.org/pdf/2502.14282'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777399ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lark] [2025-06-13 17:34:33,304] [DEBUG] POST https://open.feishu.cn/open-apis/bitable/v1/apps/IFFJbfKixaVjiOsGK08cNv9xnye/tables/tbl59A6AKiOlxCav/records/batch_create 200, headers: {\"User-Agent\": \"oapi-sdk-python/v1.4.17\", \"Content-Type\": \"application/json; charset=utf-8\", \"Authorization\": \"Bearer t-g1046dgAKLGRK56PGC5HETAODSRO4HGRQP3IPHQ5\"}, params: [], body: {\"records\": [{\"fields\": {\"score\": 79, \"summary\": \"本文针对PC场景下复杂交互环境（密集GUI元素、跨应用工作流）和长序列任务的挑战，提出了PC-Agent框架。核心贡献包括：1）主动感知模块（APM），结合 accessibility tree提取交互元素信息、OCR与意图理解 agent 实现文本精准定位，提升细粒度感知能力；2）分层多代理协作架构，将决策过程分解为指令-子任务-动作三层，通过Manager Agent分解指令与管理子任务依赖、Progress Agent跟踪进度、Decision Agent生成动作，降低决策复杂度；3）反射机制（Reflection Agent），通过 bottom-up 错误反馈调整决策，减少执行误差；4）构建PC-Eval benchmark，包含25个复杂指令（覆盖8个应用），评估代理处理实际PC任务的能力。实验结果显示，PC-Agent在PC-Eval上的任务成功率（56%）较SOTA方法Agent-S（24%）提升32%，子任务成功率（76%）也显著优于对比模型。然而，框架依赖闭源大模型（如GPT-4o），效率与隐私问题有待解决；当前聚焦 productivity 场景，需扩展至社交、娱乐等更多领域。\", \"link\": {\"link\": \"https://arxiv.org/pdf/2502.14282\", \"text\": \"https://arxiv.org/pdf/2502.14282\"}}}]}\n",
      "[Lark] [2025-06-13 17:34:33,307] [INFO] {\n",
      "    \"records\": [\n",
      "        {\n",
      "            \"fields\": {\n",
      "                \"link\": {\n",
      "                    \"link\": \"https://arxiv.org/pdf/2502.14282\",\n",
      "                    \"text\": \"https://arxiv.org/pdf/2502.14282\"\n",
      "                },\n",
      "                \"score\": 79,\n",
      "                \"summary\": \"本文针对PC场景下复杂交互环境（密集GUI元素、跨应用工作流）和长序列任务的挑战，提出了PC-Agent框架。核心贡献包括：1）主动感知模块（APM），结合 accessibility tree提取交互元素信息、OCR与意图理解 agent 实现文本精准定位，提升细粒度感知能力；2）分层多代理协作架构，将决策过程分解为指令-子任务-动作三层，通过Manager Agent分解指令与管理子任务依赖、Progress Agent跟踪进度、Decision Agent生成动作，降低决策复杂度；3）反射机制（Reflection Agent），通过 bottom-up 错误反馈调整决策，减少执行误差；4）构建PC-Eval benchmark，包含25个复杂指令（覆盖8个应用），评估代理处理实际PC任务的能力。实验结果显示，PC-Agent在PC-Eval上的任务成功率（56%）较SOTA方法Agent-S（24%）提升32%，子任务成功率（76%）也显著优于对比模型。然而，框架依赖闭源大模型（如GPT-4o），效率与隐私问题有待解决；当前聚焦 productivity 场景，需扩展至社交、娱乐等更多领域。\"\n",
      "            },\n",
      "            \"record_id\": \"recuNZyZpR8usX\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "add_records(TABLE_APP_TOKEN, TABLE_ID, user_access_token, [res])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dd11f",
   "metadata": {},
   "source": [
    "## 总结多篇文档到飞书"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7079a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_links = [\n",
    "    \"https://arxiv.org/pdf/2502.14282\",\n",
    "    \"https://arxiv.org/pdf/2501.00663\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9d908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app_access_token': 't-g1046dgAKLGRK56PGC5HETAODSRO4HGRQP3IPHQ5', 'code': 0, 'expire': 3394, 'msg': 'ok', 'tenant_access_token': 't-g1046dgAKLGRK56PGC5HETAODSRO4HGRQP3IPHQ5'}\n",
      "[Lark] [2025-06-13 17:39:57,835] [DEBUG] GET https://open.feishu.cn/open-apis/docs/v1/content 200, headers: {\"User-Agent\": \"oapi-sdk-python/v1.4.17\", \"Authorization\": \"Bearer t-g1046dgAKLGRK56PGC5HETAODSRO4HGRQP3IPHQ5\"}, params: [[\"doc_token\", \"QIvPdelxLonVSQxP8fHcRJ1BnCh\"], [\"doc_type\", \"docx\"], [\"content_type\", \"markdown\"]], body: None\n"
     ]
    }
   ],
   "source": [
    "res = get_access_token(APP_ID, APP_SECRET)\n",
    "access_token = res[\"access_token\"]\n",
    "sop_content = get_feishu_doc_content(RATING_SOP_DOC_TOKEN, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f08f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app_access_token': 't-g1046dgAKLGRK56PGC5HETAODSRO4HGRQP3IPHQ5', 'code': 0, 'expire': 7183, 'msg': 'ok', 'tenant_access_token': 't-g1046dgAKLGRK56PGC5HETAODSRO4HGRQP3IPHQ5'}\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for link in paper_links:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b05a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lark] [2025-06-19 13:41:55,069] [DEBUG] POST https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal 200, headers: {\"User-Agent\": \"oapi-sdk-python/v1.4.18\"}, params: [], body: {\"app_id\": \"cli_a8c24cc4de61d00c\", \"app_secret\": \"I1iUQBmAbdlD1DtLvoh88m2EqP4G2fMH\"}\n",
      "[Lark] [2025-06-19 13:41:55,323] [DEBUG] GET https://open.feishu.cn/open-apis/sheets/v3/spreadsheets/PZFesLg6qhkYAotcOEuc3N7VnUc/sheets/query 200, headers: {\"User-Agent\": \"oapi-sdk-python/v1.4.18\", \"Authorization\": \"Bearer t-g1046jdFSR732754FJCWSSFE2AK2KJDINAUEJEVA\"}, params: [], body: None\n",
      "[Lark] [2025-06-19 13:41:55,330] [INFO] {\n",
      "    \"sheets\": [\n",
      "        {\n",
      "            \"sheet_id\": \"296f58\",\n",
      "            \"title\": \"Top Seed\",\n",
      "            \"index\": 0,\n",
      "            \"hidden\": false,\n",
      "            \"grid_properties\": {\n",
      "                \"frozen_row_count\": 0,\n",
      "                \"frozen_column_count\": 0,\n",
      "                \"row_count\": 200,\n",
      "                \"column_count\": 19\n",
      "            },\n",
      "            \"resource_type\": \"sheet\",\n",
      "            \"merges\": [\n",
      "                {\n",
      "                    \"start_row_index\": 22,\n",
      "                    \"end_row_index\": 27,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 22,\n",
      "                    \"end_row_index\": 27,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 18,\n",
      "                    \"end_row_index\": 21,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 18,\n",
      "                    \"end_row_index\": 21,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 13,\n",
      "                    \"end_row_index\": 17,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 9,\n",
      "                    \"end_row_index\": 12,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 9,\n",
      "                    \"end_row_index\": 17,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 1,\n",
      "                    \"end_row_index\": 2,\n",
      "                    \"start_column_index\": 2,\n",
      "                    \"end_column_index\": 2\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 1,\n",
      "                    \"end_row_index\": 8,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 1,\n",
      "                    \"end_row_index\": 8,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"sheet_id\": \"FWUrtz\",\n",
      "            \"title\": \"Seed Edge\",\n",
      "            \"index\": 1,\n",
      "            \"hidden\": false,\n",
      "            \"grid_properties\": {\n",
      "                \"frozen_row_count\": 0,\n",
      "                \"frozen_column_count\": 0,\n",
      "                \"row_count\": 200,\n",
      "                \"column_count\": 20\n",
      "            },\n",
      "            \"resource_type\": \"sheet\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "[Lark] [2025-06-19 13:41:55,529] [DEBUG] GET https://open.feishu.cn/open-apis/sheets/v3/spreadsheets/PZFesLg6qhkYAotcOEuc3N7VnUc/sheets/query 200, headers: {\"User-Agent\": \"oapi-sdk-python/v1.4.18\", \"Authorization\": \"Bearer t-g1046jdFSR732754FJCWSSFE2AK2KJDINAUEJEVA\"}, params: [], body: None\n",
      "[Lark] [2025-06-19 13:41:55,532] [INFO] {\n",
      "    \"sheets\": [\n",
      "        {\n",
      "            \"sheet_id\": \"296f58\",\n",
      "            \"title\": \"Top Seed\",\n",
      "            \"index\": 0,\n",
      "            \"hidden\": false,\n",
      "            \"grid_properties\": {\n",
      "                \"frozen_row_count\": 0,\n",
      "                \"frozen_column_count\": 0,\n",
      "                \"row_count\": 200,\n",
      "                \"column_count\": 19\n",
      "            },\n",
      "            \"resource_type\": \"sheet\",\n",
      "            \"merges\": [\n",
      "                {\n",
      "                    \"start_row_index\": 22,\n",
      "                    \"end_row_index\": 27,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 22,\n",
      "                    \"end_row_index\": 27,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 18,\n",
      "                    \"end_row_index\": 21,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 18,\n",
      "                    \"end_row_index\": 21,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 13,\n",
      "                    \"end_row_index\": 17,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 9,\n",
      "                    \"end_row_index\": 12,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 9,\n",
      "                    \"end_row_index\": 17,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 1,\n",
      "                    \"end_row_index\": 2,\n",
      "                    \"start_column_index\": 2,\n",
      "                    \"end_column_index\": 2\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 1,\n",
      "                    \"end_row_index\": 8,\n",
      "                    \"start_column_index\": 1,\n",
      "                    \"end_column_index\": 1\n",
      "                },\n",
      "                {\n",
      "                    \"start_row_index\": 1,\n",
      "                    \"end_row_index\": 8,\n",
      "                    \"start_column_index\": 0,\n",
      "                    \"end_column_index\": 0\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"sheet_id\": \"FWUrtz\",\n",
      "            \"title\": \"Seed Edge\",\n",
      "            \"index\": 1,\n",
      "            \"hidden\": false,\n",
      "            \"grid_properties\": {\n",
      "                \"frozen_row_count\": 0,\n",
      "                \"frozen_column_count\": 0,\n",
      "                \"row_count\": 200,\n",
      "                \"column_count\": 20\n",
      "            },\n",
      "            \"resource_type\": \"sheet\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import lark_oapi as lark\n",
    "from lark_oapi.api.sheets.v3 import *\n",
    "\n",
    "\n",
    "# SDK 使用说明: https://open.feishu.cn/document/uAjLw4CM/ukTMukTMukTM/server-side-sdk/python--sdk/preparations-before-development\n",
    "# 以下示例代码默认根据文档示例值填充，如果存在代码问题，请在 API 调试台填上相关必要参数后再复制代码使用\n",
    "# 复制该 Demo 后, 需要将 \"YOUR_APP_ID\", \"YOUR_APP_SECRET\" 替换为自己应用的 APP_ID, APP_SECRET.\n",
    "def main():\n",
    "    # 创建client\n",
    "    client = lark.Client.builder() \\\n",
    "        .app_id(\"cli_a8c24cc4de61d00c\") \\\n",
    "        .app_secret(\"I1iUQBmAbdlD1DtLvoh88m2EqP4G2fMH\") \\\n",
    "        .log_level(lark.LogLevel.DEBUG) \\\n",
    "        .build()\n",
    "\n",
    "    # 构造请求对象\n",
    "    request: QuerySpreadsheetSheetRequest = QuerySpreadsheetSheetRequest.builder() \\\n",
    "        .spreadsheet_token(\"PZFesLg6qhkYAotcOEuc3N7VnUc\") \\\n",
    "        .build()\n",
    "\n",
    "    # 发起请求\n",
    "    response: QuerySpreadsheetSheetResponse = client.sheets.v3.spreadsheet_sheet.query(request)\n",
    "\n",
    "    # 处理失败返回\n",
    "    if not response.success():\n",
    "        lark.logger.error(\n",
    "            f\"client.sheets.v3.spreadsheet_sheet.query failed, code: {response.code}, msg: {response.msg}, log_id: {response.get_log_id()}, resp: \\n{json.dumps(json.loads(response.raw.content), indent=4, ensure_ascii=False)}\")\n",
    "        return\n",
    "\n",
    "    # 处理业务结果\n",
    "    lark.logger.info(lark.JSON.marshal(response.data, indent=4))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "print(type(main()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db420e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app_access_token': 't-g1046jg7A6NWEI37ZBUWPS6LA4PZQHS4B4FHR7UO', 'code': 0, 'expire': 6922, 'msg': 'ok', 'tenant_access_token': 't-g1046jg7A6NWEI37ZBUWPS6LA4PZQHS4B4FHR7UO'}\n",
      "[Lark] [2025-06-19 16:11:40,951] [DEBUG] GET https://open.feishu.cn/open-apis/docs/v1/content 200, headers: {\"User-Agent\": \"oapi-sdk-python/v1.4.18\", \"Authorization\": \"Bearer t-g1046jg7A6NWEI37ZBUWPS6LA4PZQHS4B4FHR7UO\"}, params: [[\"doc_token\", \"A2gHdk98No36a9xIyRqcORGCnxh\"], [\"doc_type\", \"docx\"], [\"content_type\", \"markdown\"]], body: None\n",
      "# 岗位tag prompt\n",
      "\n",
      "招聘总共分两个项目（Top Seed以及Seed Edge；两项目的推荐可以有重叠，eg\\. 某候选人既适合Top Seed项目LLM分支Posttrain部门，又适合Seed Edge项目Continual Learning topic）：\n",
      "\n",
      "\n",
      "## ——Top Seed：\n",
      "\n",
      "#### LLM：\n",
      "\n",
      "A\\. Pretrain\\(预训练\\)：\n",
      "\n",
      "负责人：\n",
      "\n",
      "@李成刚：\n",
      "\n",
      "参考招聘关键词：pretrain，与训练\n",
      "\n",
      "人才画像/岗位画像：探索下一代 Pretrain 新范式，基于 Agent 和 Active Learning 的模型自我训练与演化；探索大规模合成数据 Pretrain，突破人类数据瓶颈与边界；探索多模态联合 Pretrain，以及更好的 Modeling 方法，提升智能的上限；研发高能力超小模型，研究如何用 1B 激活实现高推理能力，及支撑的Data 和 Modeling 新方法；研究Al for Math，长期愿景是实现 AI 数学家，能自动或辅助数学家，去解决真正困难和有价值的数学命题，比如黎曼猜想；通过让 NL和FL有效结合，研究上限更高的下一代 Prover 新范式，同时关注代码和数据等专项的训练\n",
      "\n",
      "@金霄然：\n",
      "\n",
      "参考招聘关键词：数学，math\n",
      "\n",
      "人才画像/岗位画像：探索下一代 Pretrain 新范式，基于 Agent 和 Active Learning 的模型自我训练与演化；探索大规模合成数据 Pretrain，突破人类数据瓶颈与边界；探索多模态联合 Pretrain，以及更好的 Modeling 方法，提升智能的上限；研发高能力超小模型，研究如何用 1B 激活实现高推理能力，及支撑的Data 和 Modeling 新方法；研究Al for Math，长期愿景是实现 AI 数学家，能自动或辅助数学家，去解决真正困难和有价值的数学命题，比如黎曼猜想；通过让 NL和FL有效结合，研究上限更高的下一代 Prover 新范式，同时关注代码和数据等专项的训练（可根据负责人需求调整）\n",
      "\n",
      "B\\. Posttrain（后训练）：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "C\\. Horizon Reach:\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "D\\. 模型架构：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "\n",
      "\n",
      "#### 机器学习系统：\n",
      "\n",
      "A\\. 大模型推理cuda算子:\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "B\\. 大模型训练:\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "C\\. 大模型训练调度和存储:\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "D\\. 大模型平台:\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 视觉理解多模态：\n",
      "\n",
      "A\\. 视频理解：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "B\\. 视觉推理：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "C\\. 视觉知识：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "D\\. 空间理解：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "E\\. 结构化智能：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "F\\. Canvas交互：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 视觉生成多模态：\n",
      "\n",
      "A\\. 图片生成：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "B\\. 视频生成：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "C\\. 3D生成：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 语音多模态：\n",
      "\n",
      "A\\. 语音理解：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "B\\. 语音生成：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "C\\. 音乐生成：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "## ——Seed Edge:\n",
      "\n",
      "#### Continual Learning:\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 内在学习能力：memory\\&amp;推理：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 动态算力分配Infra：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 多模态感知能力：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 连接外部的能力：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### Text Diffusion：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "#### 模型架构：\n",
      "\n",
      "负责人：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "（待填）：\n",
      "\n",
      "参考招聘关键词：\n",
      "\n",
      "人才画像/岗位画像：\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import lark_oapi as lark\n",
    "import requests\n",
    "from lark_oapi.api.bitable.v1 import *\n",
    "from lark_oapi.api.docs.v1 import *\n",
    "\n",
    "\n",
    "def get_feishu_doc_content(doc_token: str, access_token: str) -> str:\n",
    "    \"\"\"获取飞书文档内容\n",
    "\n",
    "    Args:\n",
    "        doc_token (str): 文档的 token\n",
    "        access_token (str): 访问令牌\n",
    "\n",
    "    Returns:\n",
    "        str: 文档内容\n",
    "    \"\"\"\n",
    "    # 创建client\n",
    "    client = (\n",
    "        lark.Client.builder()\n",
    "        .enable_set_token(True)\n",
    "        .log_level(lark.LogLevel.DEBUG)\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    # 构造请求对象\n",
    "    request: GetContentRequest = (\n",
    "        GetContentRequest.builder()\n",
    "        .doc_token(doc_token)\n",
    "        .doc_type(\"docx\")\n",
    "        .content_type(\"markdown\")\n",
    "        .build()\n",
    "    )\n",
    "\n",
    "    # 发起请求\n",
    "    option = lark.RequestOption.builder().user_access_token(access_token).build()\n",
    "    response: GetContentResponse = client.docs.v1.content.get(request, option)\n",
    "\n",
    "    # 处理失败返回\n",
    "    if not response.success():\n",
    "        error_msg = f\"client.docs.v1.content.get failed, code: {response.code}, msg: {response.msg}, log_id: {response.get_log_id()}, resp: \\n{json.dumps(json.loads(response.raw.content), indent=4, ensure_ascii=False)}\"\n",
    "        raise Exception(error_msg)\n",
    "\n",
    "    # 返回文档内容\n",
    "    return response.data.content\n",
    "\n",
    "\n",
    "def get_access_token(app_id, app_secret):\n",
    "    \"\"\"\n",
    "    获取自定义应用的app_access_token\n",
    "    :param app_id: 应用的唯一标识符\n",
    "    :param app_secret: 应用的密钥\n",
    "    :return: 包含app_access_token和过期时间的字典，失败时返回None\n",
    "    \"\"\"\n",
    "    # 定义API请求的URL\n",
    "    url = \"https://open.feishu.cn/open-apis/auth/v3/app_access_token/internal\"\n",
    "\n",
    "    # 设置请求头\n",
    "    headers = {\"Content-Type\": \"application/json; charset=utf-8\"}\n",
    "\n",
    "    # 构建请求体\n",
    "    payload = {\"app_id\": app_id, \"app_secret\": app_secret}\n",
    "\n",
    "    try:\n",
    "        # 发送POST请求\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "        # 解析响应内容\n",
    "        result = response.json()\n",
    "\n",
    "        # 检查请求是否成功（code为0表示成功）\n",
    "        if result.get(\"code\") == 0:\n",
    "            # 提取app_access_token和过期时间\n",
    "            access_token = result.get(\"app_access_token\")\n",
    "            expire = result.get(\"expire\")\n",
    "            print(result)\n",
    "\n",
    "            # 返回包含访问令牌和过期时间的字典\n",
    "            return {\n",
    "                \"access_token\": access_token,\n",
    "                \"expire\": expire,\n",
    "                \"timestamp\": int(time.time()),  # 添加获取时间戳\n",
    "            }\n",
    "        else:\n",
    "            # 打印错误信息\n",
    "            print(\n",
    "                f\"获取access_token失败，错误码：{result.get('code')}，错误信息：{result.get('msg')}\"\n",
    "            )\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # 处理请求异常\n",
    "        print(f\"请求异常：{e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        # 处理响应解析异常\n",
    "        print(f\"响应解析异常：{e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "access_token = get_access_token(\"cli_a8c24cc4de61d00c\",\"I1iUQBmAbdlD1DtLvoh88m2EqP4G2fMH\")[\"access_token\"]\n",
    "content = get_feishu_doc_content(\"A2gHdk98No36a9xIyRqcORGCnxh\",access_token)\n",
    "print(content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c15acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请求成功！\n",
      "<!doctype html>\n",
      "<html class=\"\">\n",
      "\t<head>\n",
      "\t\t<meta charset=\"utf-8\" />\n",
      "\t\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0, user-scalable=no\" />\n",
      "\t\t<meta name=\"description\" content=\"Your daily dose of AI research from AK\" />\n",
      "\t\t<meta property=\"fb:app_id\" content=\"1321688464574422\" />\n",
      "\t\t<meta name=\"twitter:card\" content=\"summary_large_image\" />\n",
      "\t\t<meta name=\"twitter:site\" content=\"@huggingface\" />\n",
      "\t\t<meta name=\"twitter:image\" content=\"https://huggingface.co/front/thumbnails/papers.png\" />\n",
      "\t\t<meta property=\"og:title\" content=\"Daily Papers - Hugging Face\" />\n",
      "\t\t<meta property=\"og:type\" content=\"website\" />\n",
      "\t\t<meta property=\"og:url\" content=\"https://huggingface.co/papers/date/2025-06-18\" />\n",
      "\t\t<meta property=\"og:image\" content=\"https://huggingface.co/front/thumbnails/papers.png\" />\n",
      "\n",
      "\t\t<link rel=\"stylesheet\" href=\"/front/build/kube-3562a0d/style.css\" />\n",
      "\n",
      "\t\t<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" />\n",
      "\t\t<link\n",
      "\t\t\thref=\"https://fonts.googleapis.com/css2?family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;1,200;1,300;1,400;1,600;1,700&display=swap\"\n",
      "\t\t\trel=\"stylesheet\"\n",
      "\t\t/>\n",
      "\t\t<link\n",
      "\t\t\thref=\"https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;600;700&display=swap\"\n",
      "\t\t\trel=\"stylesheet\"\n",
      "\t\t/>\n",
      "\n",
      "\t\t<link\n",
      "\t\t\trel=\"preload\"\n",
      "\t\t\thref=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css\"\n",
      "\t\t\tas=\"style\"\n",
      "\t\t\tonload=\"this.onload=null;this.rel='stylesheet'\"\n",
      "\t\t/>\n",
      "\t\t<noscript>\n",
      "\t\t\t<link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css\" />\n",
      "\t\t</noscript>\n",
      "\n",
      "\t\t<script>const guestTheme = document.cookie.match(/theme=(\\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches));</script>  \n",
      "\n",
      "\t\t<title>Daily Papers - Hugging Face</title>\n",
      "\n",
      "\t\t<script\n",
      "\t\t\tdefer\n",
      "\t\t\tdata-domain=\"huggingface.co\"\n",
      "\t\t\tevent-loggedIn=\"false\"\n",
      "\t\t\tsrc=\"/js/script.pageview-props.js\"\n",
      "\t\t></script>\n",
      "\t\t<script>\n",
      "\t\t\twindow.plausible =\n",
      "\t\t\t\twindow.plausible ||\n",
      "\t\t\t\tfunction () {\n",
      "\t\t\t\t\t(window.plausible.q = window.plausible.q || []).push(arguments);\n",
      "\t\t\t\t};\n",
      "\t\t</script>\n",
      "\t\t<script>\n",
      "\t\t\twindow.hubConfig = {\"features\":{\"signupDisabled\":false},\"sshGitUrl\":\"git@hf.co\",\"moonHttpUrl\":\"https:\\/\\/huggingface.co\",\"captchaApiKey\":\"bd5f2066-93dc-4bdd-a64b-a24646ca3859\",\"captchaDisabledOnSignup\":true,\"datasetViewerPublicUrl\":\"https:\\/\\/datasets-server.huggingface.co\",\"stripePublicKey\":\"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc\",\"environment\":\"production\",\"userAgent\":\"HuggingFace (production)\",\"spacesIframeDomain\":\"hf.space\",\"spacesApiUrl\":\"https:\\/\\/api.hf.space\",\"docSearchKey\":\"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6\",\"logoDev\":{\"apiUrl\":\"https:\\/\\/img.logo.dev\\/\",\"apiKey\":\"pk_UHS2HZOeRnaSOdDp7jbd5w\"}};\n",
      "\t\t</script>\n",
      "\t\t<script type=\"text/javascript\" src=\"https://de5282c3ca0c.edge.sdk.awswaf.com/de5282c3ca0c/526cf06acb0d/challenge.js\" defer></script> \n",
      "\t</head>\n",
      "\t<body class=\"flex flex-col min-h-dvh bg-white dark:bg-gray-950 text-black DailyPapersPage\">\n",
      "\t\t<div class=\"flex min-h-dvh flex-col\"><div class=\"SVELTE_HYDRATER contents\" data-target=\"SystemThemeMonitor\" data-props=\"{&quot;isLoggedIn&quot;:false}\"></div>\n",
      "\n",
      "\t<div class=\"SVELTE_HYDRATER contents\" data-target=\"MainHeader\" data-props=\"{&quot;classNames&quot;:&quot;&quot;,&quot;isWide&quot;:false,&quot;isZh&quot;:true,&quot;isPro&quot;:false}\"><header class=\"border-b border-gray-100 \"><div class=\"w-full px-4 container flex h-16 items-center\"><div class=\"flex flex-1 items-center\"><a class=\"mr-5 flex flex-none items-center lg:mr-6\" href=\"/\"><img alt=\"Hugging Face's logo\" class=\"w-7 md:mr-2\" src=\"/front/assets/huggingface_logo-noborder.svg\">\n",
      "\t\t\t\t<span class=\"hidden whitespace-nowrap text-lg font-bold md:block\">Hugging Face</span></a>\n",
      "\t\t\t<div class=\"relative flex-1 lg:max-w-sm mr-2 sm:mr-4 md:mr-3 xl:mr-6\"><input autocomplete=\"off\" class=\"w-full dark:bg-gray-950 pl-8 form-input-alt h-9 pr-3 focus:shadow-xl \" name=\"\" placeholder=\"Search models, datasets, users...\"   spellcheck=\"false\" type=\"text\" value=\"\">\n",
      "\t<svg class=\"absolute left-2.5 text-gray-400 top-1/2 transform -translate-y-1/2\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M30 28.59L22.45 21A11 11 0 1 0 21 22.45L28.59 30zM5 14a9 9 0 1 1 9 9a9 9 0 0 1-9-9z\" fill=\"currentColor\"></path></svg>\n",
      "\t</div>\n",
      "\t\t\t<div class=\"flex flex-none items-center justify-center p-0.5 place-self-stretch lg:hidden\"><button class=\"relative z-40 flex h-6 w-8 items-center justify-center\" type=\"button\"><svg width=\"1em\" height=\"1em\" viewBox=\"0 0 10 10\" class=\"text-xl\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" preserveAspectRatio=\"xMidYMid meet\" fill=\"currentColor\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M1.65039 2.9999C1.65039 2.8066 1.80709 2.6499 2.00039 2.6499H8.00039C8.19369 2.6499 8.35039 2.8066 8.35039 2.9999C8.35039 3.1932 8.19369 3.3499 8.00039 3.3499H2.00039C1.80709 3.3499 1.65039 3.1932 1.65039 2.9999ZM1.65039 4.9999C1.65039 4.8066 1.80709 4.6499 2.00039 4.6499H8.00039C8.19369 4.6499 8.35039 4.8066 8.35039 4.9999C8.35039 5.1932 8.19369 5.3499 8.00039 5.3499H2.00039C1.80709 5.3499 1.65039 5.1932 1.65039 4.9999ZM2.00039 6.6499C1.80709 6.6499 1.65039 6.8066 1.65039 6.9999C1.65039 7.1932 1.80709 7.3499 2.00039 7.3499H8.00039C8.19369 7.3499 8.35039 7.1932 8.35039 6.9999C8.35039 6.8066 8.19369 6.6499 8.00039 6.6499H2.00039Z\"></path></svg>\n",
      "\t\t</button>\n",
      "\n",
      "\t</div></div>\n",
      "\t\t<nav aria-label=\"Main\" class=\"ml-auto hidden lg:block\"><ul class=\"flex items-center gap-x-1 2xl:gap-x-2\"><li class=\"hover:text-indigo-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/models\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-indigo-500\" style=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path class=\"uim-quaternary\" d=\"M20.23 7.24L12 12L3.77 7.24a1.98 1.98 0 0 1 .7-.71L11 2.76c.62-.35 1.38-.35 2 0l6.53 3.77c.29.173.531.418.7.71z\" opacity=\".25\" fill=\"currentColor\"></path><path class=\"uim-tertiary\" d=\"M12 12v9.5a2.09 2.09 0 0 1-.91-.21L4.5 17.48a2.003 2.003 0 0 1-1-1.73v-7.5a2.06 2.06 0 0 1 .27-1.01L12 12z\" opacity=\".5\" fill=\"currentColor\"></path><path class=\"uim-primary\" d=\"M20.5 8.25v7.5a2.003 2.003 0 0 1-1 1.73l-6.62 3.82c-.275.13-.576.198-.88.2V12l8.23-4.76c.175.308.268.656.27 1.01z\" fill=\"currentColor\"></path></svg>\n",
      "\t\t\t\t\t\tModels</a>\n",
      "\t\t\t\t</li><li class=\"hover:text-red-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/datasets\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-red-500\" style=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 25 25\"><ellipse cx=\"12.5\" cy=\"5\" fill=\"currentColor\" fill-opacity=\"0.25\" rx=\"7.5\" ry=\"2\"></ellipse><path d=\"M12.5 15C16.6421 15 20 14.1046 20 13V20C20 21.1046 16.6421 22 12.5 22C8.35786 22 5 21.1046 5 20V13C5 14.1046 8.35786 15 12.5 15Z\" fill=\"currentColor\" opacity=\"0.5\"></path><path d=\"M12.5 7C16.6421 7 20 6.10457 20 5V11.5C20 12.6046 16.6421 13.5 12.5 13.5C8.35786 13.5 5 12.6046 5 11.5V5C5 6.10457 8.35786 7 12.5 7Z\" fill=\"currentColor\" opacity=\"0.5\"></path><path d=\"M5.23628 12C5.08204 12.1598 5 12.8273 5 13C5 14.1046 8.35786 15 12.5 15C16.6421 15 20 14.1046 20 13C20 12.8273 19.918 12.1598 19.7637 12C18.9311 12.8626 15.9947 13.5 12.5 13.5C9.0053 13.5 6.06886 12.8626 5.23628 12Z\" fill=\"currentColor\"></path></svg>\n",
      "\t\t\t\t\t\tDatasets</a>\n",
      "\t\t\t\t</li><li class=\"hover:text-blue-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/spaces\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-blue-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" viewBox=\"0 0 25 25\"><path opacity=\".5\" d=\"M6.016 14.674v4.31h4.31v-4.31h-4.31ZM14.674 14.674v4.31h4.31v-4.31h-4.31ZM6.016 6.016v4.31h4.31v-4.31h-4.31Z\" fill=\"currentColor\"></path><path opacity=\".75\" fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M3 4.914C3 3.857 3.857 3 4.914 3h6.514c.884 0 1.628.6 1.848 1.414a5.171 5.171 0 0 1 7.31 7.31c.815.22 1.414.964 1.414 1.848v6.514A1.914 1.914 0 0 1 20.086 22H4.914A1.914 1.914 0 0 1 3 20.086V4.914Zm3.016 1.102v4.31h4.31v-4.31h-4.31Zm0 12.968v-4.31h4.31v4.31h-4.31Zm8.658 0v-4.31h4.31v4.31h-4.31Zm0-10.813a2.155 2.155 0 1 1 4.31 0 2.155 2.155 0 0 1-4.31 0Z\" fill=\"currentColor\"></path><path opacity=\".25\" d=\"M16.829 6.016a2.155 2.155 0 1 0 0 4.31 2.155 2.155 0 0 0 0-4.31Z\" fill=\"currentColor\"></path></svg>\n",
      "\t\t\t\t\t\tSpaces</a>\n",
      "\t\t\t\t</li><li class=\"max-xl:hidden relative\"><div class=\"relative \">\n",
      "\t<button class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 hover:text-yellow-700 dark:hover:text-gray-100 \" type=\"button\">\n",
      "\t\t<svg class=\"mr-1.5 mr-1.5 text-gray-400 text-yellow-500! group-hover:text-yellow-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M20.6081 3C21.7684 3 22.8053 3.49196 23.5284 4.38415C23.9756 4.93678 24.4428 5.82749 24.4808 7.16133C24.9674 7.01707 25.4353 6.93643 25.8725 6.93643C26.9833 6.93643 27.9865 7.37587 28.696 8.17411C29.6075 9.19872 30.0124 10.4579 29.8361 11.7177C29.7523 12.3177 29.5581 12.8555 29.2678 13.3534C29.8798 13.8646 30.3306 14.5763 30.5485 15.4322C30.719 16.1032 30.8939 17.5006 29.9808 18.9403C30.0389 19.0342 30.0934 19.1319 30.1442 19.2318C30.6932 20.3074 30.7283 21.5229 30.2439 22.6548C29.5093 24.3704 27.6841 25.7219 24.1397 27.1727C21.9347 28.0753 19.9174 28.6523 19.8994 28.6575C16.9842 29.4379 14.3477 29.8345 12.0653 29.8345C7.87017 29.8345 4.8668 28.508 3.13831 25.8921C0.356375 21.6797 0.754104 17.8269 4.35369 14.1131C6.34591 12.058 7.67023 9.02782 7.94613 8.36275C8.50224 6.39343 9.97271 4.20438 12.4172 4.20438H12.4179C12.6236 4.20438 12.8314 4.2214 13.0364 4.25468C14.107 4.42854 15.0428 5.06476 15.7115 6.02205C16.4331 5.09583 17.134 4.359 17.7682 3.94323C18.7242 3.31737 19.6794 3 20.6081 3ZM20.6081 5.95917C20.2427 5.95917 19.7963 6.1197 19.3039 6.44225C17.7754 7.44319 14.8258 12.6772 13.7458 14.7131C13.3839 15.3952 12.7655 15.6837 12.2086 15.6837C11.1036 15.6837 10.2408 14.5497 12.1076 13.1085C14.9146 10.9402 13.9299 7.39584 12.5898 7.1776C12.5311 7.16799 12.4731 7.16355 12.4172 7.16355C11.1989 7.16355 10.6615 9.33114 10.6615 9.33114C10.6615 9.33114 9.0863 13.4148 6.38031 16.206C3.67434 18.998 3.5346 21.2388 5.50675 24.2246C6.85185 26.2606 9.42666 26.8753 12.0653 26.8753C14.8021 26.8753 17.6077 26.2139 19.1799 25.793C19.2574 25.7723 28.8193 22.984 27.6081 20.6107C27.4046 20.212 27.0693 20.0522 26.6471 20.0522C24.9416 20.0522 21.8393 22.6726 20.5057 22.6726C20.2076 22.6726 19.9976 22.5416 19.9116 22.222C19.3433 20.1173 28.552 19.2325 27.7758 16.1839C27.639 15.6445 27.2677 15.4256 26.746 15.4263C24.4923 15.4263 19.4358 19.5181 18.3759 19.5181C18.2949 19.5181 18.2368 19.4937 18.2053 19.4419C17.6743 18.557 17.9653 17.9394 21.7082 15.6009C25.4511 13.2617 28.0783 11.8545 26.5841 10.1752C26.4121 9.98141 26.1684 9.8956 25.8725 9.8956C23.6001 9.89634 18.2311 14.9403 18.2311 14.9403C18.2311 14.9403 16.7821 16.496 15.9057 16.496C15.7043 16.496 15.533 16.4139 15.4169 16.2112C14.7956 15.1296 21.1879 10.1286 21.5484 8.06535C21.7928 6.66715 21.3771 5.95917 20.6081 5.95917Z\" fill=\"#FF9D00\"></path><path d=\"M5.50686 24.2246C3.53472 21.2387 3.67446 18.9979 6.38043 16.206C9.08641 13.4147 10.6615 9.33111 10.6615 9.33111C10.6615 9.33111 11.2499 6.95933 12.59 7.17757C13.93 7.39581 14.9139 10.9401 12.1069 13.1084C9.29997 15.276 12.6659 16.7489 13.7459 14.713C14.8258 12.6772 17.7747 7.44316 19.304 6.44221C20.8326 5.44128 21.9089 6.00204 21.5484 8.06532C21.188 10.1286 14.795 15.1295 15.4171 16.2118C16.0391 17.2934 18.2312 14.9402 18.2312 14.9402C18.2312 14.9402 25.0907 8.49588 26.5842 10.1752C28.0776 11.8545 25.4512 13.2616 21.7082 15.6008C17.9646 17.9393 17.6744 18.557 18.2054 19.4418C18.7372 20.3266 26.9998 13.1351 27.7759 16.1838C28.5513 19.2324 19.3434 20.1173 19.9117 22.2219C20.48 24.3274 26.3979 18.2382 27.6082 20.6107C28.8193 22.9839 19.2574 25.7722 19.18 25.7929C16.0914 26.62 8.24723 28.3726 5.50686 24.2246Z\" fill=\"#FFD21E\"></path></svg>\n",
      "\t\t\tCommunity\n",
      "\t\t</button>\n",
      "\t\n",
      "\t\n",
      "\t</div>\n",
      "\t\t\t\t</li><li class=\"hover:text-yellow-700\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/docs\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" class=\"mr-1.5 text-gray-400 group-hover:text-yellow-500\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 16 16\"><path d=\"m2.28 3.7-.3.16a.67.67 0 0 0-.34.58v8.73l.01.04.02.07.01.04.03.06.02.04.02.03.04.06.05.05.04.04.06.04.06.04.08.04.08.02h.05l.07.02h.11l.04-.01.07-.02.03-.01.07-.03.22-.12a5.33 5.33 0 0 1 5.15.1.67.67 0 0 0 .66 0 5.33 5.33 0 0 1 5.33 0 .67.67 0 0 0 1-.58V4.36a.67.67 0 0 0-.34-.5l-.3-.17v7.78a.63.63 0 0 1-.87.59 4.9 4.9 0 0 0-4.35.35l-.65.39a.29.29 0 0 1-.15.04.29.29 0 0 1-.16-.04l-.65-.4a4.9 4.9 0 0 0-4.34-.34.63.63 0 0 1-.87-.59V3.7Z\" fill=\"currentColor\" class=\"dark:opacity-40\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M8 3.1a5.99 5.99 0 0 0-5.3-.43.66.66 0 0 0-.42.62v8.18c0 .45.46.76.87.59a4.9 4.9 0 0 1 4.34.35l.65.39c.05.03.1.04.16.04.05 0 .1-.01.15-.04l.65-.4a4.9 4.9 0 0 1 4.35-.34.63.63 0 0 0 .86-.59V3.3a.67.67 0 0 0-.41-.62 5.99 5.99 0 0 0-5.3.43l-.3.17L8 3.1Zm.73 1.87a.43.43 0 1 0-.86 0v5.48a.43.43 0 0 0 .86 0V4.97Z\" fill=\"currentColor\" class=\"opacity-40 dark:opacity-100\"></path><path d=\"M8.73 4.97a.43.43 0 1 0-.86 0v5.48a.43.43 0 1 0 .86 0V4.96Z\" fill=\"currentColor\" class=\"dark:opacity-40\"></path></svg>\n",
      "\t\t\t\t\t\tDocs</a>\n",
      "\t\t\t\t</li><li class=\"hover:text-black dark:hover:text-white max-2xl:hidden\"><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/enterprise\"><svg class=\"mr-1.5 text-gray-400 group-hover:text-black dark:group-hover:text-white\" xmlns=\"http://www.w3.org/2000/svg\" fill=\"none\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 33 27\"><path fill=\"currentColor\" fill-rule=\"evenodd\" d=\"M13.5.7a8.7 8.7 0 0 0-7.7 5.7L1 20.6c-1 3.1.9 5.7 4.1 5.7h15c3.3 0 6.8-2.6 7.8-5.7l4.6-14.2c1-3.1-.8-5.7-4-5.7h-15Zm1.1 5.7L9.8 20.3h9.8l1-3.1h-5.8l.8-2.5h4.8l1.1-3h-4.8l.8-2.3H23l1-3h-9.5Z\" clip-rule=\"evenodd\"></path></svg>\n",
      "\t\t\t\t\t\tEnterprise</a>\n",
      "\t\t\t\t</li>\n",
      "\n",
      "\t\t<li><a class=\"group flex items-center px-2 py-0.5 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/pricing\">Pricing\n",
      "\t\t\t</a></li>\n",
      "\n",
      "\t\t<li><div class=\"relative group\">\n",
      "\t<button class=\"px-2 py-0.5 hover:text-gray-500 dark:hover:text-gray-600 flex items-center \" type=\"button\">\n",
      "\t\t<svg class=\" text-gray-500 w-5 group-hover:text-gray-400 dark:text-gray-300 dark:group-hover:text-gray-100\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" viewBox=\"0 0 32 18\" preserveAspectRatio=\"xMidYMid meet\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.4504 3.30221C14.4504 2.836 14.8284 2.45807 15.2946 2.45807H28.4933C28.9595 2.45807 29.3374 2.836 29.3374 3.30221C29.3374 3.76842 28.9595 4.14635 28.4933 4.14635H15.2946C14.8284 4.14635 14.4504 3.76842 14.4504 3.30221Z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.4504 9.00002C14.4504 8.53382 14.8284 8.15588 15.2946 8.15588H28.4933C28.9595 8.15588 29.3374 8.53382 29.3374 9.00002C29.3374 9.46623 28.9595 9.84417 28.4933 9.84417H15.2946C14.8284 9.84417 14.4504 9.46623 14.4504 9.00002Z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M14.4504 14.6978C14.4504 14.2316 14.8284 13.8537 15.2946 13.8537H28.4933C28.9595 13.8537 29.3374 14.2316 29.3374 14.6978C29.3374 15.164 28.9595 15.542 28.4933 15.542H15.2946C14.8284 15.542 14.4504 15.164 14.4504 14.6978Z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M1.94549 6.87377C2.27514 6.54411 2.80962 6.54411 3.13928 6.87377L6.23458 9.96907L9.32988 6.87377C9.65954 6.54411 10.194 6.54411 10.5237 6.87377C10.8533 7.20343 10.8533 7.73791 10.5237 8.06756L6.23458 12.3567L1.94549 8.06756C1.61583 7.73791 1.61583 7.20343 1.94549 6.87377Z\" fill=\"currentColor\"></path></svg>\n",
      "\t\t\t\n",
      "\t\t</button>\n",
      "\t\n",
      "\t\n",
      "\t</div></li>\n",
      "\t\t<li><hr class=\"h-5 w-0.5 border-none bg-gray-100 dark:bg-gray-800\"></li>\n",
      "\t\t<li><a class=\"block cursor-pointer whitespace-nowrap px-2 py-0.5 hover:text-gray-500 dark:text-gray-300 dark:hover:text-gray-100\" href=\"/login\">Log In\n",
      "\t\t\t\t</a></li>\n",
      "\t\t\t<li><a class=\"whitespace-nowrap rounded-full border border-transparent bg-gray-900 px-3 py-1 leading-none text-white hover:border-black hover:bg-white hover:text-black\" href=\"/join\">Sign Up\n",
      "\t\t\t\t\t</a></li></ul></nav></div></header></div>\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t<div class=\"SVELTE_HYDRATER contents\" data-target=\"SSOBanner\" data-props=\"{}\"></div>\n",
      "\t\n",
      "\n",
      "\n",
      "\t\n",
      "\n",
      "\t<main class=\"flex flex-1 flex-col\"><div class=\"SVELTE_HYDRATER contents\" data-target=\"DailyPapersBannerSubscribe\" data-props=\"{&quot;isLoggedIn&quot;:false}\"><div class=\"-mt-px flex h-9 w-full justify-center text-gray-600\"><svg class=\"hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 110 41\"><path fill=\"currentColor\" d=\"M110 0H0c39.1 0 44 9.6 49 19.5C54.6 30 60 41 108 41h2V0Z\"></path></svg>\n",
      "\t<div class=\"flex items-center justify-center gap-3 bg-gray-100/80 text-sm dark:bg-gray-800/40 max-sm:flex-1\"><div class=\"rounded-md bg-blue-500/20 px-1 text-xs font-semibold uppercase text-blue-600\">new</div>\n",
      "\t\t\t<p class=\"hidden sm:inline\">Get trending papers in your email inbox once a day!</p>\n",
      "\t\t\t<p class=\"inline sm:hidden\">Get trending papers in your email inbox!</p>\n",
      "\t\t\t<a href=\"/login?next=%2Fpapers\" class=\"btn px-2! text-sm leading-none\">Subscribe</a></div>\n",
      "\t<svg class=\"hidden h-9 flex-none text-gray-100/80 dark:text-gray-800/40 sm:block\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 110 41\"><path fill=\"currentColor\" d=\"M0 0h110C70.9 0 66 9.6 61 19.5 55.4 30 50 41 2 41H0V0Z\"></path></svg></div></div>\n",
      "\t<div class=\"SVELTE_HYDRATER contents\" data-target=\"DailyPapers\" data-props=\"{&quot;canSubmit&quot;:false,&quot;dateString&quot;:&quot;2025-06-18&quot;,&quot;dailyPapers&quot;:[{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14028&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685240aa0164cd131671056a&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63a0c0803c8841cfe2cd1f15&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xueqing Peng&quot;,&quot;user&quot;:&quot;Xueqing&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xueqing Peng&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T16:15:27.517Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671056b&quot;,&quot;name&quot;:&quot;Lingfei Qian&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671056c&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;65d76cc5b9b7b8bf88faa916&quot;,&quot;avatarUrl&quot;:&quot;/avatars/d95232cd0c307efab6197ade1a66190b.svg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yan Wang&quot;,&quot;user&quot;:&quot;YanAdjeNole&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yan Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T16:15:31.114Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671056d&quot;,&quot;name&quot;:&quot;Ruoyu Xiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671056e&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;65bd14e8ce846f8aa94db1d1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/76eaad15bf32eba75271f3dc315527c2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yueru He&quot;,&quot;user&quot;:&quot;Yueru1&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yueru He&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T16:15:33.127Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671056f&quot;,&quot;name&quot;:&quot;Yang Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710570&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;678ab76d27bb31ad067cbffd&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/678ab76d27bb31ad067cbffd/l_5mRG6_BmqmADKs2Kb3W.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mingyang Jiang&quot;,&quot;user&quot;:&quot;0oJ1mmyo0&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Mingyang Jiang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T16:15:29.378Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710571&quot;,&quot;name&quot;:&quot;Jeff Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710572&quot;,&quot;name&quot;:&quot;Huan He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710573&quot;,&quot;name&quot;:&quot;Yi Han&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710574&quot;,&quot;name&quot;:&quot;Yun Feng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710575&quot;,&quot;name&quot;:&quot;Yuechen Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710576&quot;,&quot;name&quot;:&quot;Yupeng Cao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710577&quot;,&quot;name&quot;:&quot;Haohang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710578&quot;,&quot;name&quot;:&quot;Yangyang Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710579&quot;,&quot;name&quot;:&quot;Xiaoyu Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671057a&quot;,&quot;name&quot;:&quot;Penglei Gao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671057b&quot;,&quot;name&quot;:&quot;Shengyuan Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671057c&quot;,&quot;name&quot;:&quot;Keyi Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671057d&quot;,&quot;name&quot;:&quot;Shanshan Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671057e&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62f662bcc58915315c4eccea&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yilun Zhao&quot;,&quot;user&quot;:&quot;yilunzhao&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yilun Zhao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:58:59.111Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671057f&quot;,&quot;name&quot;:&quot;Zhiwei Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710580&quot;,&quot;name&quot;:&quot;Peng Lu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710581&quot;,&quot;name&quot;:&quot;Jerry Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710582&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62bb1e0f3ff437e49a3088e5&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/XuQKiRxyq9mncX5QrVLy7.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Suyuchen Wang&quot;,&quot;user&quot;:&quot;sheryc&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Suyuchen Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T16:15:35.878Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710583&quot;,&quot;name&quot;:&quot;Triantafillos Papadopoulos&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710584&quot;,&quot;name&quot;:&quot;Polydoros Giannouris&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710585&quot;,&quot;name&quot;:&quot;Efstathia Soufleri&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710586&quot;,&quot;name&quot;:&quot;Nuo Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710587&quot;,&quot;name&quot;:&quot;Guojun Xiong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710588&quot;,&quot;name&quot;:&quot;Zhiyang Deng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710589&quot;,&quot;name&quot;:&quot;Yijia Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671058a&quot;,&quot;name&quot;:&quot;Mingquan Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671058b&quot;,&quot;name&quot;:&quot;Meikang Qiu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671058c&quot;,&quot;name&quot;:&quot;Kaleb E Smith&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671058d&quot;,&quot;name&quot;:&quot;Arman Cohan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671058e&quot;,&quot;name&quot;:&quot;Xiao-Yang Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd131671058f&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63b58ed5889aa6707f0bb0f4&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63b58ed5889aa6707f0bb0f4/znl74_aMswlV8VtHrfj3G.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Jimin Huang&quot;,&quot;user&quot;:&quot;jiminHuang&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jimin Huang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-19T09:10:29.859Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710590&quot;,&quot;name&quot;:&quot;Alejandro Lopez-Lira&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710591&quot;,&quot;name&quot;:&quot;Xi Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710592&quot;,&quot;name&quot;:&quot;Junichi Tsujii&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710593&quot;,&quot;name&quot;:&quot;Jian-Yun Nie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710594&quot;,&quot;name&quot;:&quot;Sophia Ananiadou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685240aa0164cd1316710595&quot;,&quot;name&quot;:&quot;Qianqian Xie&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-16T22:01:49.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T13:36:29.873Z&quot;,&quot;title&quot;:&quot;MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\\n  for Financial LLM Evaluation&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;63a0c0803c8841cfe2cd1f15&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xueqing Peng&quot;,&quot;user&quot;:&quot;Xueqing&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Recent advances in large language models (LLMs) have accelerated progress in\\nfinancial NLP and applications, yet existing benchmarks remain limited to\\nmonolingual and unimodal settings, often over-relying on simple tasks and\\nfailing to reflect the complexity of real-world financial communication. We\\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\\nto the global financial domain, evaluating LLMs across modalities (text,\\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\\nextract and reason over information from visual-text financial documents.\\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\\na compact, balanced benchmark rather than simple aggregation existing datasets.\\nExtensive evaluation of 22 state-of-the-art models reveals that even the\\nstrongest models, despite their general multimodal and multilingual\\ncapabilities, struggle dramatically when faced with complex cross-lingual and\\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\\nfoster transparent, reproducible, and inclusive progress in financial studies\\nand applications.&quot;,&quot;upvotes&quot;:74,&quot;discussionId&quot;:&quot;685240ab0164cd1316710596&quot;,&quot;ai_summary&quot;:&quot;MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.&quot;,&quot;ai_keywords&quot;:[&quot;LLMs&quot;,&quot;financial NLP&quot;,&quot;multilingual&quot;,&quot;multimodal&quot;,&quot;benchmark&quot;,&quot;domain-specific tasks&quot;,&quot;PolyFiQA-Easy&quot;,&quot;PolyFiQA-Expert&quot;,&quot;EnglishOCR&quot;,&quot;SpanishOCR&quot;,&quot;dynamic selection mechanism&quot;,&quot;difficulty-aware&quot;,&quot;OCR-embedded&quot;,&quot;financial QA&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T18:01:49.000Z&quot;,&quot;title&quot;:&quot;MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\\n  for Financial LLM Evaluation&quot;,&quot;summary&quot;:&quot;Recent advances in large language models (LLMs) have accelerated progress in\\nfinancial NLP and applications, yet existing benchmarks remain limited to\\nmonolingual and unimodal settings, often over-relying on simple tasks and\\nfailing to reflect the complexity of real-world financial communication. We\\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\\nto the global financial domain, evaluating LLMs across modalities (text,\\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\\nextract and reason over information from visual-text financial documents.\\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\\na compact, balanced benchmark rather than simple aggregation existing datasets.\\nExtensive evaluation of 22 state-of-the-art models reveals that even the\\nstrongest models, despite their general multimodal and multilingual\\ncapabilities, struggle dramatically when faced with complex cross-lingual and\\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\\nfoster transparent, reproducible, and inclusive progress in financial studies\\nand applications.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14028.png&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;63a0c0803c8841cfe2cd1f15&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg&quot;,&quot;fullname&quot;:&quot;Xueqing Peng&quot;,&quot;name&quot;:&quot;Xueqing&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:9},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.12928&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6851dd060164cd13167103d7&quot;,&quot;name&quot;:&quot;King Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103d8&quot;,&quot;name&quot;:&quot;Hanhao Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103d9&quot;,&quot;name&quot;:&quot;Siwei Wu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103da&quot;,&quot;name&quot;:&quot;Tianshun Xing&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103db&quot;,&quot;name&quot;:&quot;Dehua Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103dc&quot;,&quot;name&quot;:&quot;Xiangru Tang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103dd&quot;,&quot;name&quot;:&quot;Minghao Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103de&quot;,&quot;name&quot;:&quot;Jian Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103df&quot;,&quot;name&quot;:&quot;Jiaheng Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103e0&quot;,&quot;name&quot;:&quot;Yuchen Eleanor Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103e1&quot;,&quot;name&quot;:&quot;Changwang Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103e2&quot;,&quot;name&quot;:&quot;Chenghua Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103e3&quot;,&quot;name&quot;:&quot;Jun Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103e4&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;638efcf4c67af472d316d424&quot;,&quot;avatarUrl&quot;:&quot;/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ge Zhang&quot;,&quot;user&quot;:&quot;zhangysk&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ge Zhang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:16:44.662Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6851dd060164cd13167103e5&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;628c8598ef14f971b698107f&quot;,&quot;avatarUrl&quot;:&quot;/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhou&quot;,&quot;user&quot;:&quot;Wangchunshu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wangchunshu Zhou&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:16:42.693Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-15T17:59:47.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T04:21:02.464Z&quot;,&quot;title&quot;:&quot;Scaling Test-time Compute for LLM Agents&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;638efcf4c67af472d316d424&quot;,&quot;avatarUrl&quot;:&quot;/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Ge Zhang&quot;,&quot;user&quot;:&quot;zhangysk&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Scaling test time compute has shown remarkable success in improving the\\nreasoning abilities of large language models (LLMs). In this work, we conduct\\nthe first systematic exploration of applying test-time scaling methods to\\nlanguage agents and investigate the extent to which it improves their\\neffectiveness. Specifically, we explore different test-time scaling strategies,\\nincluding: (1) parallel sampling algorithms; (2) sequential revision\\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\\nrollouts.We carefully analyze and ablate the impact of different design\\nstrategies on applying test-time scaling on language agents, and have follow\\nfindings: 1. Scaling test time compute could improve the performance of agents.\\n2. Knowing when to reflect is important for agents. 3. Among different\\nverification and result merging approaches, the list-wise method performs best.\\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\\nperformance.&quot;,&quot;upvotes&quot;:45,&quot;discussionId&quot;:&quot;6851dd060164cd13167103e6&quot;,&quot;ai_summary&quot;:&quot;Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.&quot;,&quot;ai_keywords&quot;:[&quot;parallel sampling algorithms&quot;,&quot;sequential revision strategies&quot;,&quot;verifiers&quot;,&quot;merging methods&quot;,&quot;diversified rollouts&quot;,&quot;test-time scaling&quot;,&quot;large language models&quot;]},&quot;publishedAt&quot;:&quot;2025-06-15T13:59:47.000Z&quot;,&quot;title&quot;:&quot;Scaling Test-time Compute for LLM Agents&quot;,&quot;summary&quot;:&quot;Scaling test time compute has shown remarkable success in improving the\\nreasoning abilities of large language models (LLMs). In this work, we conduct\\nthe first systematic exploration of applying test-time scaling methods to\\nlanguage agents and investigate the extent to which it improves their\\neffectiveness. Specifically, we explore different test-time scaling strategies,\\nincluding: (1) parallel sampling algorithms; (2) sequential revision\\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\\nrollouts.We carefully analyze and ablate the impact of different design\\nstrategies on applying test-time scaling on language agents, and have follow\\nfindings: 1. Scaling test time compute could improve the performance of agents.\\n2. Knowing when to reflect is important for agents. 3. Among different\\nverification and result merging approaches, the list-wise method performs best.\\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\\nperformance.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12928.png&quot;,&quot;numComments&quot;:3,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;638efcf4c67af472d316d424&quot;,&quot;avatarUrl&quot;:&quot;/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg&quot;,&quot;fullname&quot;:&quot;Ge Zhang&quot;,&quot;name&quot;:&quot;zhangysk&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:49},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.12285&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6852b1597eb90a35d35de603&quot;,&quot;name&quot;:&quot;Yinghao Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b1597eb90a35d35de604&quot;,&quot;name&quot;:&quot;Siyou Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b1597eb90a35d35de605&quot;,&quot;name&quot;:&quot;Juntao Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b1597eb90a35d35de606&quot;,&quot;name&quot;:&quot;Emmanouil Benetos&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b1597eb90a35d35de607&quot;,&quot;name&quot;:&quot;Akira Maezawa&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-14T00:18:44.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T13:02:05.187Z&quot;,&quot;title&quot;:&quot;CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\\n  Following&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6410665d5364a661bee22524&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Yinghao Ma&quot;,&quot;user&quot;:&quot;nicolaus625&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Recent advances in audio-text large language models (LLMs) have opened new\\npossibilities for music understanding and generation. However, existing\\nbenchmarks are limited in scope, often relying on simplified tasks or\\nmulti-choice evaluations that fail to reflect the complexity of real-world\\nmusic analysis. We reinterpret a broad range of traditional MIR annotations as\\ninstruction-following formats and introduce CMI-Bench, a comprehensive music\\ninstruction following benchmark designed to evaluate audio-text LLMs on a\\ndiverse set of music information retrieval (MIR) tasks. These include genre\\nclassification, emotion regression, emotion tagging, instrument classification,\\npitch estimation, key detection, lyrics transcription, melody extraction, vocal\\ntechnique recognition, instrument performance technique detection, music\\ntagging, music captioning, and (down)beat tracking: reflecting core challenges\\nin MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized\\nevaluation metrics consistent with previous state-of-the-art MIR models,\\nensuring direct comparability with supervised approaches. We provide an\\nevaluation toolkit supporting all open-source audio-textual LLMs, including\\nLTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant\\nperformance gaps between LLMs and supervised models, along with their culture,\\nchronological and gender bias, highlighting the potential and limitations of\\ncurrent models in addressing MIR tasks. CMI-Bench establishes a unified\\nfoundation for evaluating music instruction following, driving progress in\\nmusic-aware LLMs.&quot;,&quot;upvotes&quot;:43,&quot;discussionId&quot;:&quot;6852b15a7eb90a35d35de608&quot;,&quot;githubRepo&quot;:&quot;https://github.com/nicolaus625/CMI-bench&quot;,&quot;ai_summary&quot;:&quot;CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.&quot;,&quot;ai_keywords&quot;:[&quot;audio-text large language models&quot;,&quot;LLMs&quot;,&quot;music information retrieval&quot;,&quot;MIR&quot;,&quot;genre classification&quot;,&quot;emotion regression&quot;,&quot;instrument classification&quot;,&quot;pitch estimation&quot;,&quot;key detection&quot;,&quot;lyrics transcription&quot;,&quot;melody extraction&quot;,&quot;vocal technique recognition&quot;,&quot;instrument performance technique detection&quot;,&quot;music tagging&quot;,&quot;music captioning&quot;,&quot;beat tracking&quot;,&quot;evaluation metrics&quot;,&quot;cultural bias&quot;,&quot;chronological bias&quot;,&quot;gender bias&quot;]},&quot;publishedAt&quot;:&quot;2025-06-13T20:18:44.000Z&quot;,&quot;title&quot;:&quot;CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\\n  Following&quot;,&quot;summary&quot;:&quot;Recent advances in audio-text large language models (LLMs) have opened new\\npossibilities for music understanding and generation. However, existing\\nbenchmarks are limited in scope, often relying on simplified tasks or\\nmulti-choice evaluations that fail to reflect the complexity of real-world\\nmusic analysis. We reinterpret a broad range of traditional MIR annotations as\\ninstruction-following formats and introduce CMI-Bench, a comprehensive music\\ninstruction following benchmark designed to evaluate audio-text LLMs on a\\ndiverse set of music information retrieval (MIR) tasks. These include genre\\nclassification, emotion regression, emotion tagging, instrument classification,\\npitch estimation, key detection, lyrics transcription, melody extraction, vocal\\ntechnique recognition, instrument performance technique detection, music\\ntagging, music captioning, and (down)beat tracking: reflecting core challenges\\nin MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized\\nevaluation metrics consistent with previous state-of-the-art MIR models,\\nensuring direct comparability with supervised approaches. We provide an\\nevaluation toolkit supporting all open-source audio-textual LLMs, including\\nLTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant\\nperformance gaps between LLMs and supervised models, along with their culture,\\nchronological and gender bias, highlighting the potential and limitations of\\ncurrent models in addressing MIR tasks. CMI-Bench establishes a unified\\nfoundation for evaluating music instruction following, driving progress in\\nmusic-aware LLMs.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12285.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6410665d5364a661bee22524&quot;,&quot;avatarUrl&quot;:&quot;/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg&quot;,&quot;fullname&quot;:&quot;Yinghao Ma&quot;,&quot;name&quot;:&quot;nicolaus625&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:9},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14429&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68521a9a0164cd131671045c&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64f033ef82c6eea604c4da8b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/51b93fea7fd68b4274ee03701245dcca.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Liu Xiaoran&quot;,&quot;user&quot;:&quot;LiuXR&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xiaoran Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:16:22.707Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521a9a0164cd131671045d&quot;,&quot;name&quot;:&quot;Zhigeng Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521a9a0164cd131671045e&quot;,&quot;name&quot;:&quot;Zengfeng Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521a9a0164cd131671045f&quot;,&quot;name&quot;:&quot;Qipeng Guo&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521a9a0164cd1316710460&quot;,&quot;name&quot;:&quot;Ziwei He&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521a9a0164cd1316710461&quot;,&quot;name&quot;:&quot;Xipeng Qiu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-17T11:45:37.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T00:18:23.135Z&quot;,&quot;title&quot;:&quot;LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64f033ef82c6eea604c4da8b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/51b93fea7fd68b4274ee03701245dcca.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Liu Xiaoran&quot;,&quot;user&quot;:&quot;LiuXR&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Language Diffusion Models, or diffusion LLMs, have emerged as a\\nsignificant focus in NLP research, with substantial effort directed toward\\nunderstanding their scalability and downstream task performance. However, their\\nlong-context capabilities remain unexplored, lacking systematic analysis or\\nmethods for context extension. In this work, we present the first systematic\\ninvestigation comparing the long-context performance of diffusion LLMs and\\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\\n\\\\textit{stable perplexity} during direct context extrapolation.\\nFurthermore, where auto-regressive models fail outright during the\\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\\ndiscover diffusion LLMs exhibit a distinct \\\\textit{local perception}\\nphenomenon, enabling successful retrieval from recent context segments. We\\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\\nscaling theory. Building on these observations, we propose LongLLaDA, a\\ntraining-free method that integrates LLaDA with the NTK-based RoPE\\nextrapolation. Our results validate that established extrapolation scaling laws\\nremain effective for extending the context windows of diffusion LLMs.\\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\\nauto-regressive LLMs and others where they fall short. Consequently, this study\\nestablishes the first context extrapolation method for diffusion LLMs while\\nproviding essential theoretical insights and empirical benchmarks critical for\\nadvancing future research on long-context diffusion LLMs.&quot;,&quot;upvotes&quot;:35,&quot;discussionId&quot;:&quot;68521a9a0164cd1316710462&quot;,&quot;ai_summary&quot;:&quot;This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion LLMs&quot;,&quot;auto-regressive LLMs&quot;,&quot;stable perplexity&quot;,&quot;local perception&quot;,&quot;Rotary Position Embedding (RoPE) scaling theory&quot;,&quot;LongLLaDA&quot;,&quot;NTK-based RoPE extrapolation&quot;,&quot;context extrapolation scaling laws&quot;,&quot;long-context tasks&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T07:45:37.000Z&quot;,&quot;title&quot;:&quot;LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs&quot;,&quot;summary&quot;:&quot;Large Language Diffusion Models, or diffusion LLMs, have emerged as a\\nsignificant focus in NLP research, with substantial effort directed toward\\nunderstanding their scalability and downstream task performance. However, their\\nlong-context capabilities remain unexplored, lacking systematic analysis or\\nmethods for context extension. In this work, we present the first systematic\\ninvestigation comparing the long-context performance of diffusion LLMs and\\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\\n\\\\textit{stable perplexity} during direct context extrapolation.\\nFurthermore, where auto-regressive models fail outright during the\\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\\ndiscover diffusion LLMs exhibit a distinct \\\\textit{local perception}\\nphenomenon, enabling successful retrieval from recent context segments. We\\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\\nscaling theory. Building on these observations, we propose LongLLaDA, a\\ntraining-free method that integrates LLaDA with the NTK-based RoPE\\nextrapolation. Our results validate that established extrapolation scaling laws\\nremain effective for extending the context windows of diffusion LLMs.\\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\\nauto-regressive LLMs and others where they fall short. Consequently, this study\\nestablishes the first context extrapolation method for diffusion LLMs while\\nproviding essential theoretical insights and empirical benchmarks critical for\\nadvancing future research on long-context diffusion LLMs.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14429.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64f033ef82c6eea604c4da8b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/51b93fea7fd68b4274ee03701245dcca.svg&quot;,&quot;fullname&quot;:&quot;Liu Xiaoran&quot;,&quot;name&quot;:&quot;LiuXR&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14245&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68521c2a0164cd131671046b&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;669f940b3b09946711e20c52&quot;,&quot;avatarUrl&quot;:&quot;/avatars/27044caec57d8d68d700208fae78b6c6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XumengWen&quot;,&quot;user&quot;:&quot;XumengWen&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xumeng Wen&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:15:48.128Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd131671046c&quot;,&quot;name&quot;:&quot;Zihan Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd131671046d&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64a7a2bad001860e0c34f7f2&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2433104071e4ae1c3e2d755d81d7964b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shun Zheng&quot;,&quot;user&quot;:&quot;shun-zheng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Shun Zheng&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T02:37:06.379Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd131671046e&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;67d7c0e0cb3e80af0d13660a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hCmEo__IWO_R_Ps8Q52Os.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;zhijianxu&quot;,&quot;user&quot;:&quot;VEWOXIC&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhijian Xu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:15:46.044Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd131671046f&quot;,&quot;name&quot;:&quot;Shengyu Ye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd1316710470&quot;,&quot;name&quot;:&quot;Zhirong Wu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd1316710471&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6560763e152b659e623865ae&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xiao Liang&quot;,&quot;user&quot;:&quot;MasterVito&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Xiao Liang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:16:18.590Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd1316710472&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;604714a0c82d59b7347b55ae&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/604714a0c82d59b7347b55ae/WZFKDDUPi8JS0BK8t7mIv.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;YangWang92&quot;,&quot;user&quot;:&quot;yangwang92&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yang Wang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:16:20.452Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd1316710473&quot;,&quot;name&quot;:&quot;Junjie Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd1316710474&quot;,&quot;name&quot;:&quot;Ziming Miao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd1316710475&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;683843c8a9bdf98f92657042&quot;,&quot;avatarUrl&quot;:&quot;/avatars/4983876f615fa830cfc8f2cb5a00fa08.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jiang Bian&quot;,&quot;user&quot;:&quot;jiangbian22&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jiang Bian&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-19T09:10:31.848Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521c2a0164cd1316710476&quot;,&quot;name&quot;:&quot;Mao Yang&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png&quot;],&quot;publishedAt&quot;:&quot;2025-06-17T07:06:56.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T01:24:22.712Z&quot;,&quot;title&quot;:&quot;Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\\n  Correct Reasoning in Base LLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64a7a2bad001860e0c34f7f2&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2433104071e4ae1c3e2d755d81d7964b.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shun Zheng&quot;,&quot;user&quot;:&quot;shun-zheng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\\npromising paradigm for advancing the reasoning capabilities of Large Language\\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\\nmodels often underperform their base models on the Pass@K metric for\\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\\nresolve this contradiction by identifying the source of the problem: the\\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\\nfinal answers that probably arise from inaccurate or incomplete chains of\\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\\nCoT-Pass@K, which mandates that both the reasoning path and the final\\nanswer be correct. We provide a new theoretical foundation that formalizes how\\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\\nobserve that RLVR can incentivize the generalization of correct reasoning for\\nall values of K. Furthermore, by analyzing the training dynamics, we find\\nthat this enhanced reasoning capability emerges early in the training process\\nand smoothly generalizes. Our work provides a clear perspective on the role of\\nRLVR, offers a more reliable method for its evaluation, and confirms its\\npotential to genuinely advance machine reasoning.&quot;,&quot;upvotes&quot;:27,&quot;discussionId&quot;:&quot;68521c2a0164cd1316710477&quot;,&quot;ai_summary&quot;:&quot;RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.&quot;,&quot;ai_keywords&quot;:[&quot;Reinforcement Learning with Verifiable Rewards&quot;,&quot;LLMS&quot;,&quot;Pass@K&quot;,&quot;chains of thought&quot;,&quot;CoT-Pass@K&quot;,&quot;logical integrity&quot;,&quot;machine reasoning&quot;,&quot;training dynamics&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T03:06:56.000Z&quot;,&quot;title&quot;:&quot;Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\\n  Correct Reasoning in Base LLMs&quot;,&quot;summary&quot;:&quot;Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\\npromising paradigm for advancing the reasoning capabilities of Large Language\\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\\nmodels often underperform their base models on the Pass@K metric for\\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\\nresolve this contradiction by identifying the source of the problem: the\\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\\nfinal answers that probably arise from inaccurate or incomplete chains of\\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\\nCoT-Pass@K, which mandates that both the reasoning path and the final\\nanswer be correct. We provide a new theoretical foundation that formalizes how\\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\\nobserve that RLVR can incentivize the generalization of correct reasoning for\\nall values of K. Furthermore, by analyzing the training dynamics, we find\\nthat this enhanced reasoning capability emerges early in the training process\\nand smoothly generalizes. Our work provides a clear perspective on the role of\\nRLVR, offers a more reliable method for its evaluation, and confirms its\\npotential to genuinely advance machine reasoning.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14245.png&quot;,&quot;numComments&quot;:5,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64a7a2bad001860e0c34f7f2&quot;,&quot;avatarUrl&quot;:&quot;/avatars/2433104071e4ae1c3e2d755d81d7964b.svg&quot;,&quot;fullname&quot;:&quot;Shun Zheng&quot;,&quot;name&quot;:&quot;shun-zheng&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14234&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68522d7f0164cd13167104ee&quot;,&quot;name&quot;:&quot;Md Tanzib Hosain&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522d7f0164cd13167104ef&quot;,&quot;name&quot;:&quot;Salman Rahman&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522d7f0164cd13167104f0&quot;,&quot;name&quot;:&quot;Md Kishor Morol&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522d7f0164cd13167104f1&quot;,&quot;name&quot;:&quot;Md Rizwan Parvez&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-17T06:47:19.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T01:39:00.207Z&quot;,&quot;title&quot;:&quot;Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\\n  Like an Olympiad Team&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;65ae1c4468139e3c42973fe4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b065a857dd763410caadea37a2dc01c4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Md Rizwan Parvez&quot;,&quot;user&quot;:&quot;mparvez&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Despite impressive progress on complex reasoning, current large language\\nmodels (LLMs) typically operate in isolation - treating each problem as an\\nindependent attempt, without accumulating or integrating experiential\\nknowledge. In contrast, expert problem solvers - such as Olympiad or\\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\\nmentorship from coaches, developing intuition from past problems, leveraging\\nknowledge of tool usage and library functionality, adapting strategies based on\\nthe expertise and experiences of peers, continuously refining their reasoning\\nthrough trial and error, and learning from other related problems even during\\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\\nframework that equips a black-box LLM with a persistent, evolving memory of\\nholistic experience. Xolver integrates diverse experience modalities, including\\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\\nevaluation, and iterative refinement. By learning from relevant strategies,\\ncode fragments, and abstract reasoning patterns at inference time, Xolver\\navoids generating solutions from scratch - marking a transition from isolated\\ninference toward experience-aware language agents. Built on both open-weight\\nand proprietary models, Xolver consistently outperforms specialized reasoning\\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\\nhighlighting holistic experience learning as a key step toward generalist\\nagents capable of expert-level reasoning. Code and data are available at\\nhttps://kagnlp.github.io/xolver.github.io/.&quot;,&quot;upvotes&quot;:25,&quot;discussionId&quot;:&quot;68522d7f0164cd13167104f2&quot;,&quot;ai_summary&quot;:&quot;Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;LLMs&quot;,&quot;multi-agent reasoning framework&quot;,&quot;persistent memory&quot;,&quot;experience-aware language agents&quot;,&quot;external and self-retrieval&quot;,&quot;tool use&quot;,&quot;collaborative interactions&quot;,&quot;agent-driven evaluation&quot;,&quot;iterative refinement&quot;,&quot;GSM8K&quot;,&quot;AIME'24&quot;,&quot;AIME'25&quot;,&quot;Math-500&quot;,&quot;LiveCodeBench-V5&quot;,&quot;generalist agents&quot;,&quot;expert-level reasoning&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T02:47:19.000Z&quot;,&quot;title&quot;:&quot;Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\\n  Like an Olympiad Team&quot;,&quot;summary&quot;:&quot;Despite impressive progress on complex reasoning, current large language\\nmodels (LLMs) typically operate in isolation - treating each problem as an\\nindependent attempt, without accumulating or integrating experiential\\nknowledge. In contrast, expert problem solvers - such as Olympiad or\\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\\nmentorship from coaches, developing intuition from past problems, leveraging\\nknowledge of tool usage and library functionality, adapting strategies based on\\nthe expertise and experiences of peers, continuously refining their reasoning\\nthrough trial and error, and learning from other related problems even during\\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\\nframework that equips a black-box LLM with a persistent, evolving memory of\\nholistic experience. Xolver integrates diverse experience modalities, including\\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\\nevaluation, and iterative refinement. By learning from relevant strategies,\\ncode fragments, and abstract reasoning patterns at inference time, Xolver\\navoids generating solutions from scratch - marking a transition from isolated\\ninference toward experience-aware language agents. Built on both open-weight\\nand proprietary models, Xolver consistently outperforms specialized reasoning\\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\\nhighlighting holistic experience learning as a key step toward generalist\\nagents capable of expert-level reasoning. Code and data are available at\\nhttps://kagnlp.github.io/xolver.github.io/.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14234.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;65ae1c4468139e3c42973fe4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b065a857dd763410caadea37a2dc01c4.svg&quot;,&quot;fullname&quot;:&quot;Md Rizwan Parvez&quot;,&quot;name&quot;:&quot;mparvez&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13363&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685234100164cd1316710508&quot;,&quot;name&quot;:&quot;Lijun Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234100164cd1316710509&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;66864a38369b09d564ba2ce4&quot;,&quot;avatarUrl&quot;:&quot;/avatars/866699ffee8a1d7cf2c9bebe0d3d58fe.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;lry&quot;,&quot;user&quot;:&quot;lryyyy&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ruiyang Li&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:59:03.729Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234100164cd131671050a&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;633e570be7d5ce7bfe037a53&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhaocheng Liu&quot;,&quot;user&quot;:&quot;zhaocheng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhaocheng Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:59:00.988Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234100164cd131671050b&quot;,&quot;name&quot;:&quot;Chenglin Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234100164cd131671050c&quot;,&quot;name&quot;:&quot;Chong Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234100164cd131671050d&quot;,&quot;name&quot;:&quot;Jiehan Cheng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234100164cd131671050e&quot;,&quot;name&quot;:&quot;Qiang Ju&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234100164cd131671050f&quot;,&quot;name&quot;:&quot;Jian Xie&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt&quot;],&quot;publishedAt&quot;:&quot;2025-06-16T11:10:25.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T02:12:29.534Z&quot;,&quot;title&quot;:&quot;Efficient Medical VIE via Reinforcement Learning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;633e570be7d5ce7bfe037a53&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhaocheng Liu&quot;,&quot;user&quot;:&quot;zhaocheng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Visual Information Extraction (VIE) converts unstructured document images\\ninto structured formats like JSON, critical for medical applications such as\\nreport analysis and online consultations. Traditional methods rely on OCR and\\nlanguage models, while end-to-end multimodal models offer direct JSON\\ngeneration. However, domain-specific schemas and high annotation costs limit\\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\\nfield coverage, and innovative sampling strategies to enhance reasoning\\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\\nprecision, and recall. While our models excel on tasks similar to medical\\ndatasets, performance drops on dissimilar tasks, highlighting the need for\\ndomain-specific optimization. Case studies further demonstrate the value of\\nreasoning during training and inference for VIE.&quot;,&quot;upvotes&quot;:24,&quot;discussionId&quot;:&quot;685234100164cd1316710510&quot;,&quot;ai_summary&quot;:&quot;An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.&quot;,&quot;ai_keywords&quot;:[&quot;Reinforcement Learning with Verifiable Rewards (RLVR)&quot;,&quot;JSON generation&quot;,&quot;multimodal models&quot;,&quot;dataset diversity&quot;,&quot;precision-recall reward mechanism&quot;,&quot;hallucinations&quot;,&quot;field coverage&quot;,&quot;sampling strategies&quot;,&quot;fine-tuning&quot;,&quot;Qwen2.5-VL-7B&quot;,&quot;F1 score&quot;,&quot;case studies&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T07:10:25.000Z&quot;,&quot;title&quot;:&quot;Efficient Medical VIE via Reinforcement Learning&quot;,&quot;summary&quot;:&quot;Visual Information Extraction (VIE) converts unstructured document images\\ninto structured formats like JSON, critical for medical applications such as\\nreport analysis and online consultations. Traditional methods rely on OCR and\\nlanguage models, while end-to-end multimodal models offer direct JSON\\ngeneration. However, domain-specific schemas and high annotation costs limit\\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\\nfield coverage, and innovative sampling strategies to enhance reasoning\\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\\nprecision, and recall. While our models excel on tasks similar to medical\\ndatasets, performance drops on dissimilar tasks, highlighting the need for\\ndomain-specific optimization. Case studies further demonstrate the value of\\nreasoning during training and inference for VIE.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13363.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;633e570be7d5ce7bfe037a53&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg&quot;,&quot;fullname&quot;:&quot;Zhaocheng Liu&quot;,&quot;name&quot;:&quot;zhaocheng&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13642&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685129448a68fee7f6ba4c04&quot;,&quot;name&quot;:&quot;Shaolei Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685129448a68fee7f6ba4c05&quot;,&quot;name&quot;:&quot;Shoutao Guo&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685129448a68fee7f6ba4c06&quot;,&quot;name&quot;:&quot;Qingkai Fang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685129448a68fee7f6ba4c07&quot;,&quot;name&quot;:&quot;Yan Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685129448a68fee7f6ba4c08&quot;,&quot;name&quot;:&quot;Yang Feng&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-06-16T16:06:45.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T00:16:02.465Z&quot;,&quot;title&quot;:&quot;Stream-Omni: Simultaneous Multimodal Interactions with Large\\n  Language-Vision-Speech Model&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64803e5dc57f629056c601f1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Shaolei Zhang&quot;,&quot;user&quot;:&quot;zhangshaolei&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\\nexploration of integrating text, vision, and speech modalities to support more\\nflexible multimodal interaction. Existing LMMs typically concatenate\\nrepresentation of modalities along the sequence dimension and feed them into a\\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\\nstraightforward for modality integration, it often relies heavily on\\nlarge-scale data to learn modality alignments. In this paper, we aim to model\\nthe relationships between modalities more purposefully, thereby achieving more\\nefficient and flexible modality alignments. To this end, we propose\\nStream-Omni, a large language-vision-speech model with efficient modality\\nalignments, which can simultaneously support interactions under various\\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\\nvision and speech to the text based on their relationships. For vision that is\\nsemantically complementary to text, Stream-Omni uses sequence-dimension\\nconcatenation to achieve vision-text alignment. For speech that is semantically\\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\\nmodality alignments with less data (especially speech), enabling the transfer\\nof text capabilities to other modalities. Experiments on various benchmarks\\ndemonstrate that Stream-Omni achieves strong performance on visual\\nunderstanding, speech interaction, and vision-grounded speech interaction\\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\\nprovide intermediate text outputs (such as ASR transcriptions and model\\nresponses) during speech interaction, offering users a comprehensive multimodal\\nexperience.&quot;,&quot;upvotes&quot;:21,&quot;discussionId&quot;:&quot;685129458a68fee7f6ba4c09&quot;,&quot;projectPage&quot;:&quot;https://github.com/ictnlp/Stream-Omni&quot;,&quot;githubRepo&quot;:&quot;https://github.com/ictnlp/Stream-Omni&quot;,&quot;ai_summary&quot;:&quot;Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.&quot;,&quot;ai_keywords&quot;:[&quot;GPT-4o-like&quot;,&quot;large multimodal models&quot;,&quot;LLM backbone&quot;,&quot;modality alignments&quot;,&quot;sequence-dimension concatenation&quot;,&quot;CTC-based layer-dimension mapping&quot;,&quot;visual understanding&quot;,&quot;speech interaction&quot;,&quot;vision-grounded speech interaction&quot;,&quot;ASR transcriptions&quot;,&quot;model responses&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T12:06:45.000Z&quot;,&quot;title&quot;:&quot;Stream-Omni: Simultaneous Multimodal Interactions with Large\\n  Language-Vision-Speech Model&quot;,&quot;summary&quot;:&quot;The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\\nexploration of integrating text, vision, and speech modalities to support more\\nflexible multimodal interaction. Existing LMMs typically concatenate\\nrepresentation of modalities along the sequence dimension and feed them into a\\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\\nstraightforward for modality integration, it often relies heavily on\\nlarge-scale data to learn modality alignments. In this paper, we aim to model\\nthe relationships between modalities more purposefully, thereby achieving more\\nefficient and flexible modality alignments. To this end, we propose\\nStream-Omni, a large language-vision-speech model with efficient modality\\nalignments, which can simultaneously support interactions under various\\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\\nvision and speech to the text based on their relationships. For vision that is\\nsemantically complementary to text, Stream-Omni uses sequence-dimension\\nconcatenation to achieve vision-text alignment. For speech that is semantically\\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\\nmodality alignments with less data (especially speech), enabling the transfer\\nof text capabilities to other modalities. Experiments on various benchmarks\\ndemonstrate that Stream-Omni achieves strong performance on visual\\nunderstanding, speech interaction, and vision-grounded speech interaction\\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\\nprovide intermediate text outputs (such as ASR transcriptions and model\\nresponses) during speech interaction, offering users a comprehensive multimodal\\nexperience.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13642.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64803e5dc57f629056c601f1&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg&quot;,&quot;fullname&quot;:&quot;Shaolei Zhang&quot;,&quot;name&quot;:&quot;zhangshaolei&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14758&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685239610164cd1316710553&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;649e6761f9134a06ed1e0cea&quot;,&quot;avatarUrl&quot;:&quot;/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Daixuan Cheng&quot;,&quot;user&quot;:&quot;daixuancheng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Daixuan Cheng&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T06:57:21.864Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685239610164cd1316710554&quot;,&quot;name&quot;:&quot;Shaohan Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685239610164cd1316710555&quot;,&quot;name&quot;:&quot;Xuekai Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685239610164cd1316710556&quot;,&quot;name&quot;:&quot;Bo Dai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685239610164cd1316710557&quot;,&quot;name&quot;:&quot;Wayne Xin Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685239610164cd1316710558&quot;,&quot;name&quot;:&quot;Zhenliang Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685239610164cd1316710559&quot;,&quot;name&quot;:&quot;Furu Wei&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png&quot;],&quot;publishedAt&quot;:&quot;2025-06-17T17:54:03.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T02:33:04.259Z&quot;,&quot;title&quot;:&quot;Reasoning with Exploration: An Entropy Perspective&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;649e6761f9134a06ed1e0cea&quot;,&quot;avatarUrl&quot;:&quot;/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Daixuan Cheng&quot;,&quot;user&quot;:&quot;daixuancheng&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Balancing exploration and exploitation is a central goal in reinforcement\\nlearning (RL). Despite recent advances in enhancing language model (LM)\\nreasoning, most methods lean toward exploitation, and increasingly encounter\\nperformance plateaus. In this work, we revisit entropy -- a signal of\\nexploration in RL -- and examine its relationship to exploratory reasoning in\\nLMs. Through empirical analysis, we uncover strong positive correlations\\nbetween high-entropy regions and three types of exploratory reasoning actions:\\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\\nactions such as self-verification and correction, and (3) rare behaviors\\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\\nmodification to standard RL with only one line of code: augmenting the\\nadvantage function with an entropy-based term. Unlike traditional\\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\\nwe encourage exploration by promoting longer and deeper reasoning chains.\\nNotably, our method achieves significant gains on the Pass@K metric -- an\\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\\nextremely large K values, pushing the boundaries of LM reasoning.&quot;,&quot;upvotes&quot;:19,&quot;discussionId&quot;:&quot;685239610164cd131671055a&quot;,&quot;ai_summary&quot;:&quot;Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.&quot;,&quot;ai_keywords&quot;:[&quot;reinforcement learning&quot;,&quot;entropy&quot;,&quot;exploratory reasoning&quot;,&quot;pivotal tokens&quot;,&quot;reflective actions&quot;,&quot;rare behaviors&quot;,&quot;advantage function&quot;,&quot;Pass@K&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T13:54:03.000Z&quot;,&quot;title&quot;:&quot;Reasoning with Exploration: An Entropy Perspective&quot;,&quot;summary&quot;:&quot;Balancing exploration and exploitation is a central goal in reinforcement\\nlearning (RL). Despite recent advances in enhancing language model (LM)\\nreasoning, most methods lean toward exploitation, and increasingly encounter\\nperformance plateaus. In this work, we revisit entropy -- a signal of\\nexploration in RL -- and examine its relationship to exploratory reasoning in\\nLMs. Through empirical analysis, we uncover strong positive correlations\\nbetween high-entropy regions and three types of exploratory reasoning actions:\\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\\nactions such as self-verification and correction, and (3) rare behaviors\\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\\nmodification to standard RL with only one line of code: augmenting the\\nadvantage function with an entropy-based term. Unlike traditional\\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\\nwe encourage exploration by promoting longer and deeper reasoning chains.\\nNotably, our method achieves significant gains on the Pass@K metric -- an\\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\\nextremely large K values, pushing the boundaries of LM reasoning.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14758.png&quot;,&quot;numComments&quot;:4,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;649e6761f9134a06ed1e0cea&quot;,&quot;avatarUrl&quot;:&quot;/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg&quot;,&quot;fullname&quot;:&quot;Daixuan Cheng&quot;,&quot;name&quot;:&quot;daixuancheng&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:9},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.09985&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;684b22c63b733ba333686eed&quot;,&quot;name&quot;:&quot;Mido Assran&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686eee&quot;,&quot;name&quot;:&quot;Adrien Bardes&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686eef&quot;,&quot;name&quot;:&quot;David Fan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef0&quot;,&quot;name&quot;:&quot;Quentin Garrido&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef1&quot;,&quot;name&quot;:&quot;Russell Howes&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef2&quot;,&quot;name&quot;:&quot;Mojtaba&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef3&quot;,&quot;name&quot;:&quot;Komeili&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef4&quot;,&quot;name&quot;:&quot;Matthew Muckley&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef5&quot;,&quot;name&quot;:&quot;Ammar Rizvi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef6&quot;,&quot;name&quot;:&quot;Claire Roberts&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef7&quot;,&quot;name&quot;:&quot;Koustuv Sinha&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef8&quot;,&quot;name&quot;:&quot;Artem Zholus&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686ef9&quot;,&quot;name&quot;:&quot;Sergio Arnaud&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686efa&quot;,&quot;name&quot;:&quot;Abha Gejji&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686efb&quot;,&quot;name&quot;:&quot;Ada Martin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686efc&quot;,&quot;name&quot;:&quot;Francois Robert Hogan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686efd&quot;,&quot;name&quot;:&quot;Daniel Dugas&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686efe&quot;,&quot;name&quot;:&quot;Piotr Bojanowski&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686eff&quot;,&quot;name&quot;:&quot;Vasil Khalidov&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f00&quot;,&quot;name&quot;:&quot;Patrick Labatut&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f01&quot;,&quot;name&quot;:&quot;Francisco Massa&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f02&quot;,&quot;name&quot;:&quot;Marc Szafraniec&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f03&quot;,&quot;name&quot;:&quot;Kapil Krishnakumar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f04&quot;,&quot;name&quot;:&quot;Yong Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f05&quot;,&quot;name&quot;:&quot;Xiaodong Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f06&quot;,&quot;name&quot;:&quot;Sarath Chandar&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f07&quot;,&quot;name&quot;:&quot;Franziska Meier&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f08&quot;,&quot;name&quot;:&quot;Yann LeCun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f09&quot;,&quot;name&quot;:&quot;Michael Rabbat&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;684b22c63b733ba333686f0a&quot;,&quot;name&quot;:&quot;Nicolas Ballas&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-11T17:57:09.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T12:40:43.816Z&quot;,&quot;title&quot;:&quot;V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\\n  and Planning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;622b93067b9143726fbedc37&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1647023505150-622b93067b9143726fbedc37.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Koustuv Sinha&quot;,&quot;user&quot;:&quot;koustuvs&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;A major challenge for modern AI is to learn to understand the world and learn\\nto act largely by observation. This paper explores a self-supervised approach\\nthat combines internet-scale video data with a small amount of interaction data\\n(robot trajectories), to develop models capable of understanding, predicting,\\nand planning in the physical world. We first pre-train an action-free\\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\\nv2) and state-of-the-art performance on human action anticipation (39.7\\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\\nAdditionally, after aligning V-JEPA 2 with a large language model, we\\ndemonstrate state-of-the-art performance on multiple video question-answering\\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\\nTempCompass). Finally, we show how self-supervised learning can be applied to\\nrobotic planning tasks by post-training a latent action-conditioned world\\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\\nlabs and enable picking and placing of objects using planning with image goals.\\nNotably, this is achieved without collecting any data from the robots in these\\nenvironments, and without any task-specific training or reward. This work\\ndemonstrates how self-supervised learning from web-scale data and a small\\namount of robot interaction data can yield a world model capable of planning in\\nthe physical world.&quot;,&quot;upvotes&quot;:16,&quot;discussionId&quot;:&quot;684b22c73b733ba333686f0b&quot;,&quot;ai_summary&quot;:&quot;A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.&quot;,&quot;ai_keywords&quot;:[&quot;self-supervised learning&quot;,&quot;joint-embedding-predictive architecture&quot;,&quot;V-JEPA 2&quot;,&quot;motion understanding&quot;,&quot;human action anticipation&quot;,&quot;video question-answering&quot;,&quot;latent action-conditioned world model&quot;,&quot;V-JEPA 2-AC&quot;,&quot;robotic planning&quot;,&quot;zero-shot deployment&quot;,&quot;Franka arms&quot;,&quot;image goals&quot;]},&quot;publishedAt&quot;:&quot;2025-06-11T13:57:09.000Z&quot;,&quot;title&quot;:&quot;V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\\n  and Planning&quot;,&quot;summary&quot;:&quot;A major challenge for modern AI is to learn to understand the world and learn\\nto act largely by observation. This paper explores a self-supervised approach\\nthat combines internet-scale video data with a small amount of interaction data\\n(robot trajectories), to develop models capable of understanding, predicting,\\nand planning in the physical world. We first pre-train an action-free\\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\\nv2) and state-of-the-art performance on human action anticipation (39.7\\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\\nAdditionally, after aligning V-JEPA 2 with a large language model, we\\ndemonstrate state-of-the-art performance on multiple video question-answering\\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\\nTempCompass). Finally, we show how self-supervised learning can be applied to\\nrobotic planning tasks by post-training a latent action-conditioned world\\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\\nlabs and enable picking and placing of objects using planning with image goals.\\nNotably, this is achieved without collecting any data from the robots in these\\nenvironments, and without any task-specific training or reward. This work\\ndemonstrates how self-supervised learning from web-scale data and a small\\namount of robot interaction data can yield a world model capable of planning in\\nthe physical world.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09985.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;622b93067b9143726fbedc37&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1647023505150-622b93067b9143726fbedc37.jpeg&quot;,&quot;fullname&quot;:&quot;Koustuv Sinha&quot;,&quot;name&quot;:&quot;koustuvs&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.12278&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685234030164cd1316710502&quot;,&quot;name&quot;:&quot;Zheyuan Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234030164cd1316710503&quot;,&quot;name&quot;:&quot;Zexi Kuang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234030164cd1316710504&quot;,&quot;name&quot;:&quot;Xue Xia&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685234030164cd1316710505&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62f662bcc58915315c4eccea&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yilun Zhao&quot;,&quot;user&quot;:&quot;yilunzhao&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Yilun Zhao&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:59:08.920Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-13T23:56:17.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T02:08:35.444Z&quot;,&quot;title&quot;:&quot;Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;62f662bcc58915315c4eccea&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Yilun Zhao&quot;,&quot;user&quot;:&quot;yilunzhao&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\\nsets probe diverse input scenarios and cover a wide range of potential failure\\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\\ntest input that reveals a specific incorrect code implementation. We provide a\\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\\nin generating effective test cases for algorithm problems.&quot;,&quot;upvotes&quot;:15,&quot;discussionId&quot;:&quot;685234030164cd1316710506&quot;,&quot;githubRepo&quot;:&quot;https://github.com/FlowRays/TestCase-Eval&quot;,&quot;ai_summary&quot;:&quot;TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.&quot;,&quot;ai_keywords&quot;:[&quot;test-case generation&quot;,&quot;Fault Coverage&quot;,&quot;Fault Exposure&quot;,&quot;LLMs&quot;,&quot;algorithm problems&quot;,&quot;human-crafted solutions&quot;,&quot;Codeforces&quot;,&quot;test sets&quot;,&quot;failure modes&quot;,&quot;incorrect code implementation&quot;]},&quot;publishedAt&quot;:&quot;2025-06-13T19:56:17.000Z&quot;,&quot;title&quot;:&quot;Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure&quot;,&quot;summary&quot;:&quot;We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\\nsets probe diverse input scenarios and cover a wide range of potential failure\\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\\ntest input that reveals a specific incorrect code implementation. We provide a\\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\\nin generating effective test cases for algorithm problems.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12278.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;62f662bcc58915315c4eccea&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg&quot;,&quot;fullname&quot;:&quot;Yilun Zhao&quot;,&quot;name&quot;:&quot;yilunzhao&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:13},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14603&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68521f8a0164cd1316710481&quot;,&quot;name&quot;:&quot;Amirmojtaba Sabour&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521f8a0164cd1316710482&quot;,&quot;name&quot;:&quot;Sanja Fidler&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521f8a0164cd1316710483&quot;,&quot;name&quot;:&quot;Karsten Kreis&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg&quot;],&quot;publishedAt&quot;:&quot;2025-06-17T15:06:07.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T00:42:20.168Z&quot;,&quot;title&quot;:&quot;Align Your Flow: Scaling Continuous-Time Flow Map Distillation&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;656015f28138827138c70858&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Amirmojtaba Sabour&quot;,&quot;user&quot;:&quot;amsabour&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Diffusion- and flow-based models have emerged as state-of-the-art generative\\nmodeling approaches, but they require many sampling steps. Consistency models\\ncan distill these models into efficient one-step generators; however, unlike\\nflow- and diffusion-based methods, their performance inevitably degrades when\\nincreasing the number of steps, which we show both analytically and\\nempirically. Flow maps generalize these approaches by connecting any two noise\\nlevels in a single step and remain effective across all step counts. In this\\npaper, we introduce two new continuous-time objectives for training flow maps,\\nalong with additional novel training techniques, generalizing existing\\nconsistency and flow matching objectives. We further demonstrate that\\nautoguidance can improve performance, using a low-quality model for guidance\\nduring distillation, and an additional boost can be achieved by adversarial\\nfinetuning, with minimal loss in sample diversity. We extensively validate our\\nflow map models, called Align Your Flow, on challenging image generation\\nbenchmarks and achieve state-of-the-art few-step generation performance on both\\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\\nwe show text-to-image flow map models that outperform all existing\\nnon-adversarially trained few-step samplers in text-conditioned synthesis.&quot;,&quot;upvotes&quot;:14,&quot;discussionId&quot;:&quot;68521f8a0164cd1316710484&quot;,&quot;projectPage&quot;:&quot;https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/&quot;,&quot;ai_summary&quot;:&quot;Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion models&quot;,&quot;flow-based models&quot;,&quot;consistency models&quot;,&quot;flow maps&quot;,&quot;noise levels&quot;,&quot;autoguidance&quot;,&quot;adversarial finetuning&quot;,&quot;Align Your Flow&quot;,&quot;ImageNet&quot;,&quot;text-to-image synthesis&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T11:06:07.000Z&quot;,&quot;title&quot;:&quot;Align Your Flow: Scaling Continuous-Time Flow Map Distillation&quot;,&quot;summary&quot;:&quot;Diffusion- and flow-based models have emerged as state-of-the-art generative\\nmodeling approaches, but they require many sampling steps. Consistency models\\ncan distill these models into efficient one-step generators; however, unlike\\nflow- and diffusion-based methods, their performance inevitably degrades when\\nincreasing the number of steps, which we show both analytically and\\nempirically. Flow maps generalize these approaches by connecting any two noise\\nlevels in a single step and remain effective across all step counts. In this\\npaper, we introduce two new continuous-time objectives for training flow maps,\\nalong with additional novel training techniques, generalizing existing\\nconsistency and flow matching objectives. We further demonstrate that\\nautoguidance can improve performance, using a low-quality model for guidance\\nduring distillation, and an additional boost can be achieved by adversarial\\nfinetuning, with minimal loss in sample diversity. We extensively validate our\\nflow map models, called Align Your Flow, on challenging image generation\\nbenchmarks and achieve state-of-the-art few-step generation performance on both\\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\\nwe show text-to-image flow map models that outperform all existing\\nnon-adversarially trained few-step samplers in text-conditioned synthesis.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14603.png&quot;,&quot;numComments&quot;:4,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;656015f28138827138c70858&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg&quot;,&quot;fullname&quot;:&quot;Amirmojtaba Sabour&quot;,&quot;name&quot;:&quot;amsabour&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.12860&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685226a40164cd13167104bd&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64eb333e6878d90b031fa5c5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a0d875b49d1c56be88f34854647306da.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wanlong Liu&quot;,&quot;user&quot;:&quot;lwl-uestc&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wanlong Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:59:15.053Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104be&quot;,&quot;name&quot;:&quot;Junxiao Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104bf&quot;,&quot;name&quot;:&quot;Fei Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104c0&quot;,&quot;name&quot;:&quot;Yukang Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104c1&quot;,&quot;name&quot;:&quot;Ke Ji&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104c2&quot;,&quot;name&quot;:&quot;Wenyu Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104c3&quot;,&quot;name&quot;:&quot;Yan Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104c4&quot;,&quot;name&quot;:&quot;Yasheng Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104c5&quot;,&quot;name&quot;:&quot;Lifeng Shang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685226a40164cd13167104c6&quot;,&quot;name&quot;:&quot;Benyou Wang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-15T14:21:28.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T02:06:38.533Z&quot;,&quot;title&quot;:&quot;QFFT, Question-Free Fine-Tuning for Adaptive Reasoning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64eb333e6878d90b031fa5c5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a0d875b49d1c56be88f34854647306da.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wanlong Liu&quot;,&quot;user&quot;:&quot;lwl-uestc&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\\nimproved performance on complex tasks, but they suffer from overthinking, which\\ngenerates redundant reasoning steps, especially for simple questions. This\\npaper revisits the reasoning patterns of Long and Short CoT models, observing\\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\\nCoT patterns excel in challenging scenarios where the Short CoT patterns\\nstruggle. To enable models to leverage both patterns, we propose Question-Free\\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\\nduring training and learns exclusively from Long CoT responses. This approach\\nenables the model to adaptively employ both reasoning patterns: it prioritizes\\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\\nExperiments on various mathematical datasets demonstrate that QFFT reduces\\naverage response length by more than 50\\\\%, while achieving performance\\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\\nscenarios.&quot;,&quot;upvotes&quot;:13,&quot;discussionId&quot;:&quot;685226a40164cd13167104c7&quot;,&quot;githubRepo&quot;:&quot;https://github.com/LWL-cpu/Question-Free-Fine-Tuning&quot;,&quot;ai_summary&quot;:&quot;Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.&quot;,&quot;ai_keywords&quot;:[&quot;Long Chain-of-Thought&quot;,&quot;Short Chain-of-Thought&quot;,&quot;Question-Free Fine-Tuning&quot;,&quot;fine-tuning&quot;,&quot;Supervised Fine-Tuning&quot;]},&quot;publishedAt&quot;:&quot;2025-06-15T10:21:28.000Z&quot;,&quot;title&quot;:&quot;QFFT, Question-Free Fine-Tuning for Adaptive Reasoning&quot;,&quot;summary&quot;:&quot;Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\\nimproved performance on complex tasks, but they suffer from overthinking, which\\ngenerates redundant reasoning steps, especially for simple questions. This\\npaper revisits the reasoning patterns of Long and Short CoT models, observing\\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\\nCoT patterns excel in challenging scenarios where the Short CoT patterns\\nstruggle. To enable models to leverage both patterns, we propose Question-Free\\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\\nduring training and learns exclusively from Long CoT responses. This approach\\nenables the model to adaptively employ both reasoning patterns: it prioritizes\\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\\nExperiments on various mathematical datasets demonstrate that QFFT reduces\\naverage response length by more than 50\\\\%, while achieving performance\\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\\nscenarios.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12860.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64eb333e6878d90b031fa5c5&quot;,&quot;avatarUrl&quot;:&quot;/avatars/a0d875b49d1c56be88f34854647306da.svg&quot;,&quot;fullname&quot;:&quot;Wanlong Liu&quot;,&quot;name&quot;:&quot;lwl-uestc&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14606&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68521f240164cd131671047a&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;656864e12d73834278a8dea7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ahmed Heakl&quot;,&quot;user&quot;:&quot;ahmedheakl&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ahmed Heakl&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:15:43.091Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521f240164cd131671047b&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;62676a94dacab364889bb36c&quot;,&quot;avatarUrl&quot;:&quot;/avatars/0ead41b44957eb30564ea685ed22781a.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;SARIM HASHMI&quot;,&quot;user&quot;:&quot;Sarim-Hash&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Sarim Hashmi&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:59:21.334Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521f240164cd131671047c&quot;,&quot;name&quot;:&quot;Chaimaa Abi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521f240164cd131671047d&quot;,&quot;name&quot;:&quot;Celine Lee&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68521f240164cd131671047e&quot;,&quot;name&quot;:&quot;Abdulrahman Mahmoud&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png&quot;],&quot;publishedAt&quot;:&quot;2025-06-17T15:06:54.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T00:38:14.336Z&quot;,&quot;title&quot;:&quot;Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\\n  Transpilation with Testing Guarantees&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;656864e12d73834278a8dea7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ahmed Heakl&quot;,&quot;user&quot;:&quot;ahmedheakl&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The hardware ecosystem is rapidly evolving, with increasing interest in\\ntranslating low-level programs across different instruction set architectures\\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\\nlongevity of existing code. A particularly challenging class of this\\ntranspilation problem is translating between complex- (CISC) and reduced-\\n(RISC) hardware architectures, due to fundamental differences in instruction\\ncomplexity, memory models, and execution paradigms. In this work, we introduce\\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\\ntranslation power of pre-trained large language models (LLMs) with the rigor of\\nestablished software testing constructs. Our method generates candidate\\ntranslations using an LLM from one ISA to another, and embeds such translations\\nwithin a software-testing framework to build quantifiable confidence in the\\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\\n1.47x better energy efficiency, and 2.41x better memory usage for our\\ntranspiled code, demonstrating the effectiveness of GG for real-world\\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\\nand benchmarks to establish a common foundation for ISA-level code translation\\nresearch.&quot;,&quot;upvotes&quot;:10,&quot;discussionId&quot;:&quot;68521f240164cd131671047f&quot;,&quot;projectPage&quot;:&quot;https://ahmedheakl.github.io/Guaranteed-Guess/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/ahmedheakl/Guaranteed-Guess&quot;,&quot;ai_summary&quot;:&quot;A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.&quot;,&quot;ai_keywords&quot;:[&quot;pre-trained large language models&quot;,&quot;software testing constructs&quot;,&quot;ISA-centric transpilation&quot;,&quot;complex-instruction set computing (CISC)&quot;,&quot;reduced-instruction set computing (RISC)&quot;,&quot;instruction set architecture (ISA)&quot;,&quot;HumanEval&quot;,&quot;BringupBench&quot;,&quot;Rosetta 2 framework&quot;,&quot;functional/semantic correctness&quot;,&quot;real-world CISC-to-RISC translation&quot;,&quot;memory models&quot;,&quot;execution paradigms&quot;,&quot;transpilation&quot;,&quot;hardware ecosystem&quot;,&quot;low-level programs&quot;,&quot;code portability&quot;,&quot;longevity&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T11:06:54.000Z&quot;,&quot;title&quot;:&quot;Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\\n  Transpilation with Testing Guarantees&quot;,&quot;summary&quot;:&quot;The hardware ecosystem is rapidly evolving, with increasing interest in\\ntranslating low-level programs across different instruction set architectures\\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\\nlongevity of existing code. A particularly challenging class of this\\ntranspilation problem is translating between complex- (CISC) and reduced-\\n(RISC) hardware architectures, due to fundamental differences in instruction\\ncomplexity, memory models, and execution paradigms. In this work, we introduce\\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\\ntranslation power of pre-trained large language models (LLMs) with the rigor of\\nestablished software testing constructs. Our method generates candidate\\ntranslations using an LLM from one ISA to another, and embeds such translations\\nwithin a software-testing framework to build quantifiable confidence in the\\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\\n1.47x better energy efficiency, and 2.41x better memory usage for our\\ntranspiled code, demonstrating the effectiveness of GG for real-world\\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\\nand benchmarks to establish a common foundation for ISA-level code translation\\nresearch.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png&quot;,&quot;https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14606.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;656864e12d73834278a8dea7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg&quot;,&quot;fullname&quot;:&quot;Ahmed Heakl&quot;,&quot;name&quot;:&quot;ahmedheakl&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:41},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.10100&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68522b190164cd13167104d9&quot;,&quot;name&quot;:&quot;Yantai Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522b190164cd13167104da&quot;,&quot;name&quot;:&quot;Yuhao Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522b190164cd13167104db&quot;,&quot;name&quot;:&quot;Zichen Wen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522b190164cd13167104dc&quot;,&quot;name&quot;:&quot;Luo Zhongwei&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522b190164cd13167104dd&quot;,&quot;name&quot;:&quot;Chang Zou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522b190164cd13167104de&quot;,&quot;name&quot;:&quot;Zhipeng Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522b190164cd13167104df&quot;,&quot;name&quot;:&quot;Chuan Wen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68522b190164cd13167104e0&quot;,&quot;name&quot;:&quot;Linfeng Zhang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-11T18:34:57.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T01:30:42.708Z&quot;,&quot;title&quot;:&quot;EfficientVLA: Training-Free Acceleration and Compression for\\n  Vision-Language-Action Models&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;653b8c3e97a4d71d950e2f20&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b68880022e14556d0be58c69615db3be.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zichen Wen&quot;,&quot;user&quot;:&quot;zichenwen&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Vision-Language-Action (VLA) models, particularly diffusion-based\\narchitectures, demonstrate transformative potential for embodied intelligence\\nbut are severely hampered by high computational and memory demands stemming\\nfrom extensive inherent and inference-time redundancies. While existing\\nacceleration efforts often target isolated inefficiencies, such piecemeal\\nsolutions typically fail to holistically address the varied computational and\\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\\ndeployability. We introduce EfficientVLA, a structured and training-free\\ninference acceleration framework that systematically eliminates these barriers\\nby cohesively exploiting multifaceted redundancies. EfficientVLA\\nsynergistically integrates three targeted strategies: (1) pruning of\\nfunctionally inconsequential layers from the language module, guided by an\\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\\npathway through a task-aware strategy that selects a compact, diverse set of\\nvisual tokens, balancing task-criticality with informational coverage; and (3)\\nalleviating temporal computational redundancy within the iterative\\ndiffusion-based action head by strategically caching and reusing key\\nintermediate features. We apply our method to a standard VLA model CogACT,\\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\\nsuccess rate drop in the SIMPLER benchmark.&quot;,&quot;upvotes&quot;:8,&quot;discussionId&quot;:&quot;68522b190164cd13167104e1&quot;,&quot;ai_summary&quot;:&quot;EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion-based architectures&quot;,&quot;inference acceleration framework&quot;,&quot;pruning&quot;,&quot;inter-layer redundancies&quot;,&quot;visual tokens&quot;,&quot;task-aware strategy&quot;,&quot;iterative diffusion-based action head&quot;,&quot;caching&quot;,&quot;FLOPs&quot;,&quot;SIMPLER benchmark&quot;]},&quot;publishedAt&quot;:&quot;2025-06-11T14:34:57.000Z&quot;,&quot;title&quot;:&quot;EfficientVLA: Training-Free Acceleration and Compression for\\n  Vision-Language-Action Models&quot;,&quot;summary&quot;:&quot;Vision-Language-Action (VLA) models, particularly diffusion-based\\narchitectures, demonstrate transformative potential for embodied intelligence\\nbut are severely hampered by high computational and memory demands stemming\\nfrom extensive inherent and inference-time redundancies. While existing\\nacceleration efforts often target isolated inefficiencies, such piecemeal\\nsolutions typically fail to holistically address the varied computational and\\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\\ndeployability. We introduce EfficientVLA, a structured and training-free\\ninference acceleration framework that systematically eliminates these barriers\\nby cohesively exploiting multifaceted redundancies. EfficientVLA\\nsynergistically integrates three targeted strategies: (1) pruning of\\nfunctionally inconsequential layers from the language module, guided by an\\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\\npathway through a task-aware strategy that selects a compact, diverse set of\\nvisual tokens, balancing task-criticality with informational coverage; and (3)\\nalleviating temporal computational redundancy within the iterative\\ndiffusion-based action head by strategically caching and reusing key\\nintermediate features. We apply our method to a standard VLA model CogACT,\\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\\nsuccess rate drop in the SIMPLER benchmark.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10100.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;653b8c3e97a4d71d950e2f20&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b68880022e14556d0be58c69615db3be.svg&quot;,&quot;fullname&quot;:&quot;Zichen Wen&quot;,&quot;name&quot;:&quot;zichenwen&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13977&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68524ff90164cd13167105aa&quot;,&quot;name&quot;:&quot;Shiting Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105ab&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64b0a5037a475fba70a7260d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhen Fang&quot;,&quot;user&quot;:&quot;CostaliyA&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhen Fang&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:58:56.933Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105ac&quot;,&quot;name&quot;:&quot;Zehui Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105ad&quot;,&quot;name&quot;:&quot;Siyu Yuan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105ae&quot;,&quot;name&quot;:&quot;Junjie Ye&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105af&quot;,&quot;name&quot;:&quot;Yu Zeng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105b0&quot;,&quot;name&quot;:&quot;Lin Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105b1&quot;,&quot;name&quot;:&quot;Qi Mao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68524ff90164cd13167105b2&quot;,&quot;name&quot;:&quot;Feng Zhao&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-11T17:59:18.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T04:10:42.444Z&quot;,&quot;title&quot;:&quot;CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\\n  Models in Tool-Calling Error Scenarios&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64b0a5037a475fba70a7260d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Zhen Fang&quot;,&quot;user&quot;:&quot;CostaliyA&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The ability of large language models (LLMs) to utilize external tools has\\nenabled them to tackle an increasingly diverse range of tasks. However, as the\\ntasks become more complex and long-horizon, the intricate tool utilization\\nprocess may trigger various unexpected errors. Therefore, how to effectively\\nhandle such errors, including identifying, diagnosing, and recovering from\\nthem, has emerged as a key research direction for advancing tool learning. In\\nthis work, we first extensively analyze the types of errors encountered during\\nthe function-calling process on several competitive tool evaluation benchmarks.\\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\\nbenchmark specialized for tool learning. Building upon a novel evolutionary\\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\\nwith varying complexities, which better reflects real-world scenarios. We\\nconduct extensive experiments on CRITICTOOL, and validate the generalization\\nand effectiveness of our constructed benchmark strategy. We also provide an\\nin-depth analysis of the tool reflection ability on various LLMs, offering a\\nnew perspective on the field of tool learning in LLMs. The code is available at\\nhttps://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.&quot;,&quot;upvotes&quot;:8,&quot;discussionId&quot;:&quot;68524ff90164cd13167105b3&quot;,&quot;githubRepo&quot;:&quot;https://github.com/Shellorley0513/CriticTool&quot;,&quot;ai_summary&quot;:&quot;A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;tool learning&quot;,&quot;function-calling process&quot;,&quot;error identification&quot;,&quot;error diagnosis&quot;,&quot;error recovery&quot;,&quot;evolutionary strategy&quot;,&quot;dataset construction&quot;,&quot;tool reflection ability&quot;,&quot;critique evaluation benchmark&quot;]},&quot;publishedAt&quot;:&quot;2025-06-11T13:59:18.000Z&quot;,&quot;title&quot;:&quot;CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\\n  Models in Tool-Calling Error Scenarios&quot;,&quot;summary&quot;:&quot;The ability of large language models (LLMs) to utilize external tools has\\nenabled them to tackle an increasingly diverse range of tasks. However, as the\\ntasks become more complex and long-horizon, the intricate tool utilization\\nprocess may trigger various unexpected errors. Therefore, how to effectively\\nhandle such errors, including identifying, diagnosing, and recovering from\\nthem, has emerged as a key research direction for advancing tool learning. In\\nthis work, we first extensively analyze the types of errors encountered during\\nthe function-calling process on several competitive tool evaluation benchmarks.\\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\\nbenchmark specialized for tool learning. Building upon a novel evolutionary\\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\\nwith varying complexities, which better reflects real-world scenarios. We\\nconduct extensive experiments on CRITICTOOL, and validate the generalization\\nand effectiveness of our constructed benchmark strategy. We also provide an\\nin-depth analysis of the tool reflection ability on various LLMs, offering a\\nnew perspective on the field of tool learning in LLMs. The code is available at\\nhttps://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13977.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64b0a5037a475fba70a7260d&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg&quot;,&quot;fullname&quot;:&quot;Zhen Fang&quot;,&quot;name&quot;:&quot;CostaliyA&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14755&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685225250164cd13167104aa&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;669096da35cddb688a352ca8&quot;,&quot;avatarUrl&quot;:&quot;/avatars/5dd096cb7360682016d0fca909ab9744.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;zxiang&quot;,&quot;user&quot;:&quot;zx10086&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Zhengxiang Cheng&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T16:15:41.166Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685225250164cd13167104ab&quot;,&quot;name&quot;:&quot;Dongping Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685225250164cd13167104ac&quot;,&quot;name&quot;:&quot;Mingyang Fu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685225250164cd13167104ad&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;647f5af5b0e96764589f3b2a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tianyi Zhou&quot;,&quot;user&quot;:&quot;zhoutianyi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Tianyi Zhou&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:59:17.278Z&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-17T17:50:16.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T01:05:46.554Z&quot;,&quot;title&quot;:&quot;Optimizing Length Compression in Large Reasoning Models&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;647f5af5b0e96764589f3b2a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Tianyi Zhou&quot;,&quot;user&quot;:&quot;zhoutianyi&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Large Reasoning Models (LRMs) have achieved remarkable success, yet they\\noften suffer from producing unnecessary and verbose reasoning chains. We\\nidentify a core aspect of this issue as \\&quot;invalid thinking\\&quot; -- models tend to\\nrepeatedly double-check their work after having derived the correct answer. To\\naddress this specific inefficiency, we move beyond the general principles of\\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\\ncritical reasoning steps are preserved. Guided by these principles, we\\nintroduce LC-R1, a post-training method based on Group Relative Policy\\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\\noverall conciseness and a Compress Reward that is specifically designed to\\nremove the invalid portion of the thinking process. Extensive experiments on\\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\\nprioritizes high compression. Our analysis further validates the robustness of\\nLC-R1 and provides valuable insights for developing more powerful yet\\ncomputationally efficient LRMs. Our code is released at\\nhttps://github.com/zxiangx/LC-R1.&quot;,&quot;upvotes&quot;:6,&quot;discussionId&quot;:&quot;685225250164cd13167104ae&quot;,&quot;githubRepo&quot;:&quot;https://github.com/zxiangx/LC-R1&quot;,&quot;ai_summary&quot;:&quot;LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.&quot;,&quot;ai_keywords&quot;:[&quot;Large Reasoning Models&quot;,&quot;LRM&quot;,&quot;post-training method&quot;,&quot;Group Relative Policy Optimization&quot;,&quot;GRPO&quot;,&quot;Length Reward&quot;,&quot;Compress Reward&quot;,&quot;reasoning benchmarks&quot;,&quot;Pareto frontier&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T13:50:16.000Z&quot;,&quot;title&quot;:&quot;Optimizing Length Compression in Large Reasoning Models&quot;,&quot;summary&quot;:&quot;Large Reasoning Models (LRMs) have achieved remarkable success, yet they\\noften suffer from producing unnecessary and verbose reasoning chains. We\\nidentify a core aspect of this issue as \\&quot;invalid thinking\\&quot; -- models tend to\\nrepeatedly double-check their work after having derived the correct answer. To\\naddress this specific inefficiency, we move beyond the general principles of\\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\\ncritical reasoning steps are preserved. Guided by these principles, we\\nintroduce LC-R1, a post-training method based on Group Relative Policy\\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\\noverall conciseness and a Compress Reward that is specifically designed to\\nremove the invalid portion of the thinking process. Extensive experiments on\\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\\nprioritizes high compression. Our analysis further validates the robustness of\\nLC-R1 and provides valuable insights for developing more powerful yet\\ncomputationally efficient LRMs. Our code is released at\\nhttps://github.com/zxiangx/LC-R1.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14755.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;647f5af5b0e96764589f3b2a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg&quot;,&quot;fullname&quot;:&quot;Tianyi Zhou&quot;,&quot;name&quot;:&quot;zhoutianyi&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:14},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13651&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe2&quot;,&quot;name&quot;:&quot;Kaiyuan Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe3&quot;,&quot;name&quot;:&quot;Yixin Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe4&quot;,&quot;name&quot;:&quot;Yang Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe5&quot;,&quot;name&quot;:&quot;Xiaobo Hu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe6&quot;,&quot;name&quot;:&quot;Haotong Tian&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe7&quot;,&quot;name&quot;:&quot;Tianbao Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe8&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6505a02f9310ce8c400edc63&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bbf781594fc8c812316711aa8e2797aa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Fangfu Liu&quot;,&quot;user&quot;:&quot;Liuff23&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Fangfu Liu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T12:16:52.421Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fe9&quot;,&quot;name&quot;:&quot;Haoye Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fea&quot;,&quot;name&quot;:&quot;Hongzhang Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88feb&quot;,&quot;name&quot;:&quot;Yuan Gong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fec&quot;,&quot;name&quot;:&quot;Chen Sun&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fed&quot;,&quot;name&quot;:&quot;Han Hou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fee&quot;,&quot;name&quot;:&quot;Hui Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fef&quot;,&quot;name&quot;:&quot;James Pan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff0&quot;,&quot;name&quot;:&quot;Jianan Lou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff1&quot;,&quot;name&quot;:&quot;Jiayi Mao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff2&quot;,&quot;name&quot;:&quot;Jizheng Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff3&quot;,&quot;name&quot;:&quot;Jinpeng Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff4&quot;,&quot;name&quot;:&quot;Kangyi Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff5&quot;,&quot;name&quot;:&quot;Kenkun Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff6&quot;,&quot;name&quot;:&quot;Rui Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff7&quot;,&quot;name&quot;:&quot;Run Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff8&quot;,&quot;name&quot;:&quot;Tong Niu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ff9&quot;,&quot;name&quot;:&quot;Wenlong Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ffa&quot;,&quot;name&quot;:&quot;Wenqi Yan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ffb&quot;,&quot;name&quot;:&quot;Xuanzheng Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ffc&quot;,&quot;name&quot;:&quot;Yuchen Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ffd&quot;,&quot;name&quot;:&quot;Yi-Hsin Hung&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88ffe&quot;,&quot;name&quot;:&quot;Yuan Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce88fff&quot;,&quot;name&quot;:&quot;Zexuan Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce89000&quot;,&quot;name&quot;:&quot;Zihan Yin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce89001&quot;,&quot;name&quot;:&quot;Zijian Ma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6850cf555e07650ecce89002&quot;,&quot;name&quot;:&quot;Zhiwen Mo&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-16T16:16:14.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T04:38:47.336Z&quot;,&quot;title&quot;:&quot;xbench: Tracking Agents Productivity Scaling with Profession-Aligned\\n  Real-World Evaluations&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6505a02f9310ce8c400edc63&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bbf781594fc8c812316711aa8e2797aa.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Fangfu Liu&quot;,&quot;user&quot;:&quot;Liuff23&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce xbench, a dynamic, profession-aligned evaluation suite designed\\nto bridge the gap between AI agent capabilities and real-world productivity.\\nWhile existing benchmarks often focus on isolated technical skills, they may\\nnot accurately reflect the economic value agents deliver in professional\\nsettings. To address this, xbench targets commercially significant domains with\\nevaluation tasks defined by industry professionals. Our framework creates\\nmetrics that strongly correlate with productivity value, enables prediction of\\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\\nover time. As our initial implementations, we present two benchmarks:\\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\\nheadhunting business scenarios to evaluate agents' abilities in company\\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\\nagents' ability to match influencers with advertiser needs, evaluating their\\nperformance across 50 advertiser requirements using a curated pool of 836\\ncandidate influencers. We present initial evaluation results for leading\\ncontemporary agents, establishing a baseline for these professional domains.\\nOur continuously updated evalsets and evaluations are available at\\nhttps://xbench.org.&quot;,&quot;upvotes&quot;:6,&quot;discussionId&quot;:&quot;6850cf555e07650ecce89003&quot;,&quot;projectPage&quot;:&quot;https://xbench.org/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/xbench-ai/xbench-evals&quot;},&quot;publishedAt&quot;:&quot;2025-06-16T12:16:14.000Z&quot;,&quot;title&quot;:&quot;xbench: Tracking Agents Productivity Scaling with Profession-Aligned\\n  Real-World Evaluations&quot;,&quot;summary&quot;:&quot;We introduce xbench, a dynamic, profession-aligned evaluation suite designed\\nto bridge the gap between AI agent capabilities and real-world productivity.\\nWhile existing benchmarks often focus on isolated technical skills, they may\\nnot accurately reflect the economic value agents deliver in professional\\nsettings. To address this, xbench targets commercially significant domains with\\nevaluation tasks defined by industry professionals. Our framework creates\\nmetrics that strongly correlate with productivity value, enables prediction of\\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\\nover time. As our initial implementations, we present two benchmarks:\\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\\nheadhunting business scenarios to evaluate agents' abilities in company\\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\\nagents' ability to match influencers with advertiser needs, evaluating their\\nperformance across 50 advertiser requirements using a curated pool of 836\\ncandidate influencers. We present initial evaluation results for leading\\ncontemporary agents, establishing a baseline for these professional domains.\\nOur continuously updated evalsets and evaluations are available at\\nhttps://xbench.org.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13651.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6505a02f9310ce8c400edc63&quot;,&quot;avatarUrl&quot;:&quot;/avatars/bbf781594fc8c812316711aa8e2797aa.svg&quot;,&quot;fullname&quot;:&quot;Fangfu Liu&quot;,&quot;name&quot;:&quot;Liuff23&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:8},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.10038&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68523b4a0164cd131671055d&quot;,&quot;name&quot;:&quot;Giannis Daras&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68523b4a0164cd131671055e&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;67d204c05422de5644126f0b&quot;,&quot;avatarUrl&quot;:&quot;/avatars/8a9ac73d93785f48e63184d612b9fff1.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Adrian Rodriguez Munoz&quot;,&quot;user&quot;:&quot;adrianrm&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Adrian Rodriguez-Munoz&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:29:49.294Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68523b4a0164cd131671055f&quot;,&quot;name&quot;:&quot;Adam Klivans&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68523b4a0164cd1316710560&quot;,&quot;name&quot;:&quot;Antonio Torralba&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68523b4a0164cd1316710561&quot;,&quot;name&quot;:&quot;Constantinos Daskalakis&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-10T22:37:39.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T02:40:58.837Z&quot;,&quot;title&quot;:&quot;Ambient Diffusion Omni: Training Good Models with Bad Data&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;5f45f44b79c1ba4c353d1035&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Giannis Daras&quot;,&quot;user&quot;:&quot;giannisdaras&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We show how to use low-quality, synthetic, and out-of-distribution images to\\nimprove the quality of a diffusion model. Typically, diffusion models are\\ntrained on curated datasets that emerge from highly filtered data pools from\\nthe Web and other sources. We show that there is immense value in the\\nlower-quality images that are often discarded. We present Ambient Diffusion\\nOmni, a simple, principled framework to train diffusion models that can extract\\nsignal from all available images during training. Our framework exploits two\\nproperties of natural images -- spectral power law decay and locality. We first\\nvalidate our framework by successfully training diffusion models with images\\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\\nsignificant improvements in both image quality and diversity for text-to-image\\ngenerative modeling. The core insight is that noise dampens the initial skew\\nbetween the desired high-quality distribution and the mixed distribution we\\nactually observe. We provide rigorous theoretical justification for our\\napproach by analyzing the trade-off between learning from biased data versus\\nlimited unbiased data across diffusion times.&quot;,&quot;upvotes&quot;:6,&quot;discussionId&quot;:&quot;68523b4a0164cd1316710562&quot;,&quot;projectPage&quot;:&quot;https://giannisdaras.github.io/publication/ambient_omni&quot;,&quot;githubRepo&quot;:&quot;https://github.com/giannisdaras/ambient-omni&quot;,&quot;ai_summary&quot;:&quot;Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion models&quot;,&quot;synthetic images&quot;,&quot;out-of-distribution images&quot;,&quot;Ambient Diffusion Omni&quot;,&quot;spectral power law decay&quot;,&quot;locality&quot;,&quot;Gaussian blur&quot;,&quot;JPEG compression&quot;,&quot;motion blur&quot;,&quot;ImageNet FID&quot;,&quot;text-to-image generative modeling&quot;,&quot;noise dampening&quot;,&quot;biased data&quot;,&quot;limited unbiased data&quot;]},&quot;publishedAt&quot;:&quot;2025-06-10T18:37:39.000Z&quot;,&quot;title&quot;:&quot;Ambient Diffusion Omni: Training Good Models with Bad Data&quot;,&quot;summary&quot;:&quot;We show how to use low-quality, synthetic, and out-of-distribution images to\\nimprove the quality of a diffusion model. Typically, diffusion models are\\ntrained on curated datasets that emerge from highly filtered data pools from\\nthe Web and other sources. We show that there is immense value in the\\nlower-quality images that are often discarded. We present Ambient Diffusion\\nOmni, a simple, principled framework to train diffusion models that can extract\\nsignal from all available images during training. Our framework exploits two\\nproperties of natural images -- spectral power law decay and locality. We first\\nvalidate our framework by successfully training diffusion models with images\\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\\nsignificant improvements in both image quality and diversity for text-to-image\\ngenerative modeling. The core insight is that noise dampens the initial skew\\nbetween the desired high-quality distribution and the mixed distribution we\\nactually observe. We provide rigorous theoretical justification for our\\napproach by analyzing the trade-off between learning from biased data versus\\nlimited unbiased data across diffusion times.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10038.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;5f45f44b79c1ba4c353d1035&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg&quot;,&quot;fullname&quot;:&quot;Giannis Daras&quot;,&quot;name&quot;:&quot;giannisdaras&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:2},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14002&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685210eb0164cd131671043e&quot;,&quot;name&quot;:&quot;Siyu Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685210eb0164cd131671043f&quot;,&quot;name&quot;:&quot;Heejune Sheen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685210eb0164cd1316710440&quot;,&quot;name&quot;:&quot;Xuyuan Xiong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685210eb0164cd1316710441&quot;,&quot;name&quot;:&quot;Tianhao Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685210eb0164cd1316710442&quot;,&quot;name&quot;:&quot;Zhuoran Yang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-16T20:58:05.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T00:09:08.222Z&quot;,&quot;title&quot;:&quot;Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\\n  Autoencoders&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;683229900411a9d65cd410c0&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Siyu Chen&quot;,&quot;user&quot;:&quot;Siyuc&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We study the challenge of achieving theoretically grounded feature recovery\\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\\nModels. Existing SAE training algorithms often lack rigorous mathematical\\nguarantees and suffer from practical limitations such as hyperparameter\\nsensitivity and instability. To address these issues, we first propose a novel\\nstatistical framework for the feature recovery problem, which includes a new\\nnotion of feature identifiability by modeling polysemantic features as sparse\\nmixtures of underlying monosemantic concepts. Building on this framework, we\\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\\ntechnique that adaptively adjusts neural network bias parameters to ensure\\nappropriate activation sparsity. We theoretically prove that this\\nalgorithm correctly recovers all monosemantic features when input data is\\nsampled from our proposed statistical model. Furthermore, we develop an\\nimproved empirical variant, Group Bias Adaptation (GBA), and\\ndemonstrate its superior performance against benchmark methods when\\napplied to LLMs with up to 1.5 billion parameters. This work represents a\\nfoundational step in demystifying SAE training by providing the first SAE\\nalgorithm with theoretical recovery guarantees, thereby advancing the\\ndevelopment of more transparent and trustworthy AI systems through enhanced\\nmechanistic interpretability.&quot;,&quot;upvotes&quot;:5,&quot;discussionId&quot;:&quot;685210ec0164cd1316710443&quot;,&quot;ai_summary&quot;:&quot;A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.&quot;,&quot;ai_keywords&quot;:[&quot;Sparse Autoencoders&quot;,&quot;feature recovery&quot;,&quot;statistical framework&quot;,&quot;feature identifiability&quot;,&quot;polysemantic features&quot;,&quot;monosemantic concepts&quot;,&quot;bias adaptation&quot;,&quot;Group Bias Adaptation&quot;,&quot;Large Language Models&quot;,&quot;theoretical recovery guarantees&quot;,&quot;mechnistic interpretability&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T16:58:05.000Z&quot;,&quot;title&quot;:&quot;Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\\n  Autoencoders&quot;,&quot;summary&quot;:&quot;We study the challenge of achieving theoretically grounded feature recovery\\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\\nModels. Existing SAE training algorithms often lack rigorous mathematical\\nguarantees and suffer from practical limitations such as hyperparameter\\nsensitivity and instability. To address these issues, we first propose a novel\\nstatistical framework for the feature recovery problem, which includes a new\\nnotion of feature identifiability by modeling polysemantic features as sparse\\nmixtures of underlying monosemantic concepts. Building on this framework, we\\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\\ntechnique that adaptively adjusts neural network bias parameters to ensure\\nappropriate activation sparsity. We theoretically prove that this\\nalgorithm correctly recovers all monosemantic features when input data is\\nsampled from our proposed statistical model. Furthermore, we develop an\\nimproved empirical variant, Group Bias Adaptation (GBA), and\\ndemonstrate its superior performance against benchmark methods when\\napplied to LLMs with up to 1.5 billion parameters. This work represents a\\nfoundational step in demystifying SAE training by providing the first SAE\\nalgorithm with theoretical recovery guarantees, thereby advancing the\\ndevelopment of more transparent and trustworthy AI systems through enhanced\\nmechanistic interpretability.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14002.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;683229900411a9d65cd410c0&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png&quot;,&quot;fullname&quot;:&quot;Siyu Chen&quot;,&quot;name&quot;:&quot;Siyuc&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.05336&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68425a7ab63271ff41652734&quot;,&quot;name&quot;:&quot;Ghazi Shazan Ahmad&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68425a7ab63271ff41652735&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;656864e12d73834278a8dea7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ahmed Heakl&quot;,&quot;user&quot;:&quot;ahmedheakl&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Ahmed Heakl&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-07T05:48:53.401Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68425a7ab63271ff41652736&quot;,&quot;name&quot;:&quot;Hanan Gani&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68425a7ab63271ff41652737&quot;,&quot;name&quot;:&quot;Abdelrahman Shaker&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68425a7ab63271ff41652738&quot;,&quot;name&quot;:&quot;Zhiqiang Shen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68425a7ab63271ff41652739&quot;,&quot;name&quot;:&quot;Ranjay Krishna&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68425a7ab63271ff4165273a&quot;,&quot;name&quot;:&quot;Fahad Shahbaz Khan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68425a7ab63271ff4165273b&quot;,&quot;name&quot;:&quot;Salman Khan&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-06-05T17:59:29.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T01:30:20.599Z&quot;,&quot;title&quot;:&quot;VideoMolmo: Spatio-Temporal Grounding Meets Pointing&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;656864e12d73834278a8dea7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;Ahmed Heakl&quot;,&quot;user&quot;:&quot;ahmedheakl&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Spatio-temporal localization is vital for precise interactions across diverse\\ndomains, from biological research to autonomous navigation and interactive\\ninterfaces. Current video-based approaches, while proficient in tracking, lack\\nthe sophisticated reasoning capabilities of large language models, limiting\\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\\nconditioned on textual descriptions. Building upon the Molmo architecture,\\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\\ncondition each frame on preceding frames, ensuring temporal consistency.\\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\\nbidirectional point propagation, significantly enhancing coherence across video\\nsequences. This two-step decomposition, i.e., first using the LLM to generate\\nprecise pointing coordinates, then relying on a sequential mask-fusion module\\nto produce coherent segmentation, not only simplifies the task for the language\\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\\naccuracy and reasoning capability. Our code and models are publicly available\\nat https://github.com/mbzuai-oryx/VideoMolmo.&quot;,&quot;upvotes&quot;:5,&quot;discussionId&quot;:&quot;68425a80b63271ff416528f2&quot;,&quot;projectPage&quot;:&quot;https://mbzuai-oryx.github.io/VideoMolmo/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/mbzuai-oryx/VideoMolmo&quot;,&quot;ai_summary&quot;:&quot;VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.&quot;,&quot;ai_keywords&quot;:[&quot;Molmo&quot;,&quot;attention mechanism&quot;,&quot;temporal mask fusion&quot;,&quot;SAM2&quot;,&quot;bidirectional point propagation&quot;,&quot;VideoMolmo&quot;,&quot;LLM&quot;,&quot;sequential mask-fusion module&quot;,&quot;VPoS-Bench&quot;,&quot;Referring Video Object Segmentation&quot;,&quot;Reasoning VOS&quot;]},&quot;publishedAt&quot;:&quot;2025-06-05T13:59:29.000Z&quot;,&quot;title&quot;:&quot;VideoMolmo: Spatio-Temporal Grounding Meets Pointing&quot;,&quot;summary&quot;:&quot;Spatio-temporal localization is vital for precise interactions across diverse\\ndomains, from biological research to autonomous navigation and interactive\\ninterfaces. Current video-based approaches, while proficient in tracking, lack\\nthe sophisticated reasoning capabilities of large language models, limiting\\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\\nconditioned on textual descriptions. Building upon the Molmo architecture,\\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\\ncondition each frame on preceding frames, ensuring temporal consistency.\\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\\nbidirectional point propagation, significantly enhancing coherence across video\\nsequences. This two-step decomposition, i.e., first using the LLM to generate\\nprecise pointing coordinates, then relying on a sequential mask-fusion module\\nto produce coherent segmentation, not only simplifies the task for the language\\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\\naccuracy and reasoning capability. Our code and models are publicly available\\nat https://github.com/mbzuai-oryx/VideoMolmo.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05336.png&quot;,&quot;numComments&quot;:6,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;656864e12d73834278a8dea7&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg&quot;,&quot;fullname&quot;:&quot;Ahmed Heakl&quot;,&quot;name&quot;:&quot;ahmedheakl&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:41},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14761&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685264ca0164cd13167105f2&quot;,&quot;name&quot;:&quot;Mathurin Videau&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685264ca0164cd13167105f3&quot;,&quot;name&quot;:&quot;Badr Youbi Idrissi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685264ca0164cd13167105f4&quot;,&quot;name&quot;:&quot;Alessandro Leite&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685264ca0164cd13167105f5&quot;,&quot;name&quot;:&quot;Marc Schoenauer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685264ca0164cd13167105f6&quot;,&quot;name&quot;:&quot;Olivier Teytaud&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685264ca0164cd13167105f7&quot;,&quot;name&quot;:&quot;David Lopez-Paz&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-17T17:55:11.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T15:32:27.211Z&quot;,&quot;title&quot;:&quot;From Bytes to Ideas: Language Modeling with Autoregressive U-Nets&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;632df0e4860318fab98f8804&quot;,&quot;avatarUrl&quot;:&quot;/avatars/94b5810ec4a167d9e2b523c0626a23b4.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Badr Youbi Idrissi&quot;,&quot;user&quot;:&quot;cetosignis&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Tokenization imposes a fixed granularity on the input text, freezing how a\\nlanguage model operates on data and how far in the future it predicts. Byte\\nPair Encoding (BPE) and similar schemes split text once, build a static\\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\\nby introducing an autoregressive U-Net that learns to embed its own tokens as\\nit trains. The network reads raw bytes, pools them into words, then pairs of\\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\\ndeeper stages, the model must predict further into the future -- anticipating\\nthe next few words rather than the next byte -- so deeper stages focus on\\nbroader semantic patterns while earlier stages handle fine details. When\\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\\ntokenization now lives inside the model, the same system can handle\\ncharacter-level tasks and carry knowledge across low-resource languages.&quot;,&quot;upvotes&quot;:4,&quot;discussionId&quot;:&quot;685264ca0164cd13167105f8&quot;,&quot;ai_summary&quot;:&quot;An autoregressive U-Net learns to embed its own tokens during training, enabling a multi-scale view of text sequences and improved handling of character-level tasks and low-resource languages.&quot;,&quot;ai_keywords&quot;:[&quot;autoregressive U-Net&quot;,&quot;Byte Pair Encoding (BPE)&quot;,&quot;multi-scale view&quot;,&quot;semantic patterns&quot;,&quot;character-level tasks&quot;,&quot;low-resource languages&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T13:55:11.000Z&quot;,&quot;title&quot;:&quot;From Bytes to Ideas: Language Modeling with Autoregressive U-Nets&quot;,&quot;summary&quot;:&quot;Tokenization imposes a fixed granularity on the input text, freezing how a\\nlanguage model operates on data and how far in the future it predicts. Byte\\nPair Encoding (BPE) and similar schemes split text once, build a static\\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\\nby introducing an autoregressive U-Net that learns to embed its own tokens as\\nit trains. The network reads raw bytes, pools them into words, then pairs of\\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\\ndeeper stages, the model must predict further into the future -- anticipating\\nthe next few words rather than the next byte -- so deeper stages focus on\\nbroader semantic patterns while earlier stages handle fine details. When\\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\\ntokenization now lives inside the model, the same system can handle\\ncharacter-level tasks and carry knowledge across low-resource languages.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14761.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;632df0e4860318fab98f8804&quot;,&quot;avatarUrl&quot;:&quot;/avatars/94b5810ec4a167d9e2b523c0626a23b4.svg&quot;,&quot;fullname&quot;:&quot;Badr Youbi Idrissi&quot;,&quot;name&quot;:&quot;cetosignis&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:3},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.09033&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6852ac4a7eb90a35d35de5f1&quot;,&quot;name&quot;:&quot;Haozhen Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852ac4a7eb90a35d35de5f2&quot;,&quot;name&quot;:&quot;Tao Feng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852ac4a7eb90a35d35de5f3&quot;,&quot;name&quot;:&quot;Jiaxuan You&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-10T17:56:45.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T10:41:49.919Z&quot;,&quot;title&quot;:&quot;Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\\n  Reinforcement Learning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64f58f3468047192d6c7f335&quot;,&quot;avatarUrl&quot;:&quot;/avatars/88be16ee80da7d2eaa0feae878375001.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;XaiverZ&quot;,&quot;user&quot;:&quot;XaiverZ&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;The rapid emergence of diverse large language models (LLMs) has spurred the\\ndevelopment of LLM routers that assign user queries to the most suitable model.\\nHowever, existing LLM routers typically perform a single-round, one-to-one\\nmapping (i.e., assigning each query to a single model in isolation),\\nwhich limits their capability to tackle complex tasks that demand the\\ncomplementary strengths of multiple LLMs. In this paper, we present\\nRouter-R1, a reinforcement learning (RL)-based framework that\\nformulates multi-LLM routing and aggregation as a sequential decision process.\\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\\nreasoning ability to interleave \\&quot;think\\&quot; actions (internal deliberation) with\\n\\&quot;route\\&quot; actions (dynamic model invocation), and integrates each response into\\nits evolving context. To guide learning, we employ a lightweight rule-based\\nreward comprising format rewards, final outcome rewards, and a novel cost\\nreward for performance and cost trade-off optimization, opening a pathway\\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\\nonly on simple model descriptors such as pricing, latency, and example\\nperformance, enabling strong generalization to unseen model selection.\\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\\noutperforms over several strong baselines, achieving superior performance while\\nmaintaining robust generalization and cost management.Code is available at\\nhttps://github.com/ulab-uiuc/Router-R1.&quot;,&quot;upvotes&quot;:3,&quot;discussionId&quot;:&quot;6852ac4b7eb90a35d35de5f4&quot;,&quot;projectPage&quot;:&quot;https://ulab-uiuc.github.io/Router-R1/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/ulab-uiuc/Router-R1&quot;,&quot;ai_summary&quot;:&quot;Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.&quot;,&quot;ai_keywords&quot;:[&quot;LLMs&quot;,&quot;reinforcement learning&quot;,&quot;multi-LLM routing&quot;,&quot;aggregation&quot;,&quot;sequential decision process&quot;,&quot;think actions&quot;,&quot;route actions&quot;,&quot;context integration&quot;,&quot;reward shaping&quot;,&quot;format rewards&quot;,&quot;final outcome rewards&quot;,&quot;cost reward&quot;,&quot;performance optimization&quot;,&quot;cost management&quot;,&quot;generalization&quot;]},&quot;publishedAt&quot;:&quot;2025-06-10T13:56:45.000Z&quot;,&quot;title&quot;:&quot;Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\\n  Reinforcement Learning&quot;,&quot;summary&quot;:&quot;The rapid emergence of diverse large language models (LLMs) has spurred the\\ndevelopment of LLM routers that assign user queries to the most suitable model.\\nHowever, existing LLM routers typically perform a single-round, one-to-one\\nmapping (i.e., assigning each query to a single model in isolation),\\nwhich limits their capability to tackle complex tasks that demand the\\ncomplementary strengths of multiple LLMs. In this paper, we present\\nRouter-R1, a reinforcement learning (RL)-based framework that\\nformulates multi-LLM routing and aggregation as a sequential decision process.\\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\\nreasoning ability to interleave \\&quot;think\\&quot; actions (internal deliberation) with\\n\\&quot;route\\&quot; actions (dynamic model invocation), and integrates each response into\\nits evolving context. To guide learning, we employ a lightweight rule-based\\nreward comprising format rewards, final outcome rewards, and a novel cost\\nreward for performance and cost trade-off optimization, opening a pathway\\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\\nonly on simple model descriptors such as pricing, latency, and example\\nperformance, enabling strong generalization to unseen model selection.\\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\\noutperforms over several strong baselines, achieving superior performance while\\nmaintaining robust generalization and cost management.Code is available at\\nhttps://github.com/ulab-uiuc/Router-R1.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09033.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64f58f3468047192d6c7f335&quot;,&quot;avatarUrl&quot;:&quot;/avatars/88be16ee80da7d2eaa0feae878375001.svg&quot;,&quot;fullname&quot;:&quot;XaiverZ&quot;,&quot;name&quot;:&quot;XaiverZ&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14731&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685236ac0164cd1316710512&quot;,&quot;name&quot;:&quot;Ring Team&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710513&quot;,&quot;name&quot;:&quot;Bin Hu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710514&quot;,&quot;name&quot;:&quot;Cai Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710515&quot;,&quot;name&quot;:&quot;Deng Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710516&quot;,&quot;name&quot;:&quot;Ding Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710517&quot;,&quot;name&quot;:&quot;Dingnan Jin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710518&quot;,&quot;name&quot;:&quot;Feng Zhu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710519&quot;,&quot;name&quot;:&quot;Hao Dai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671051a&quot;,&quot;name&quot;:&quot;Hongzhi Luan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671051b&quot;,&quot;name&quot;:&quot;Jia Guo&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671051c&quot;,&quot;name&quot;:&quot;Jiaming Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671051d&quot;,&quot;name&quot;:&quot;Jiewei Wu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671051e&quot;,&quot;name&quot;:&quot;Jun Mei&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671051f&quot;,&quot;name&quot;:&quot;Jun Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710520&quot;,&quot;name&quot;:&quot;Junbo Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710521&quot;,&quot;name&quot;:&quot;Junwu Xiong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710522&quot;,&quot;name&quot;:&quot;Kaihong Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710523&quot;,&quot;name&quot;:&quot;Kuan Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710524&quot;,&quot;name&quot;:&quot;Lei Liang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710525&quot;,&quot;name&quot;:&quot;Liang Jiang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710526&quot;,&quot;name&quot;:&quot;Liangcheng Fu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710527&quot;,&quot;name&quot;:&quot;Longfei Zheng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710528&quot;,&quot;name&quot;:&quot;Qiang Gao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710529&quot;,&quot;name&quot;:&quot;Qing Cui&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671052a&quot;,&quot;name&quot;:&quot;Quan Wan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671052b&quot;,&quot;name&quot;:&quot;Shaomian Zheng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671052c&quot;,&quot;name&quot;:&quot;Shuaicheng Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671052d&quot;,&quot;name&quot;:&quot;Tongkai Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671052e&quot;,&quot;name&quot;:&quot;Wang Ren&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671052f&quot;,&quot;name&quot;:&quot;Xiaodong Yan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710530&quot;,&quot;name&quot;:&quot;Xiaopei Wan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710531&quot;,&quot;name&quot;:&quot;Xiaoyun Feng&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710532&quot;,&quot;name&quot;:&quot;Xin Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710533&quot;,&quot;name&quot;:&quot;Xinxing Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710534&quot;,&quot;name&quot;:&quot;Xinyu Kong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710535&quot;,&quot;name&quot;:&quot;Xuemin Yang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710536&quot;,&quot;name&quot;:&quot;Yang Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710537&quot;,&quot;name&quot;:&quot;Yingting Wu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710538&quot;,&quot;name&quot;:&quot;Yongkang Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd1316710539&quot;,&quot;name&quot;:&quot;Zhankai Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671053a&quot;,&quot;name&quot;:&quot;Zhenduo Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671053b&quot;,&quot;name&quot;:&quot;Zhenglei Zhou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671053c&quot;,&quot;name&quot;:&quot;Zhenyu Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671053d&quot;,&quot;name&quot;:&quot;Zhiqiang Zhang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671053e&quot;,&quot;name&quot;:&quot;Zihao Wang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685236ac0164cd131671053f&quot;,&quot;name&quot;:&quot;Zujie Wen&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-17T17:12:34.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T03:14:38.830Z&quot;,&quot;title&quot;:&quot;Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\\n  for LLMs&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\\noptimized via reinforcement learning (RL) to achieve efficient and robust\\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\\n16.8 billion parameter model with 2.75 billion activated parameters, our\\napproach matches the performance of state-of-the-art (SOTA) small-scale\\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\\nGPQA-Diamond) while activating only one-third of the parameters required by\\ncomparable models. To accomplish this, we introduce a joint training pipeline\\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\\ntraining. First, we identify optimization instability during RL training, and\\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\\nnovel approach that enhances training stability and improves computational\\nthroughput via algorithm-system co-design methodology. Second, we empirically\\ndemonstrate that selecting distillation checkpoints based on entropy loss for\\nRL training, rather than validation metrics, yields superior\\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\\ndevelop a two-stage training paradigm to harmonize multi-domain data\\nintegration, addressing domain conflicts that arise in training with mixed\\ndataset. We will release the model, dataset, and code.&quot;,&quot;upvotes&quot;:2,&quot;discussionId&quot;:&quot;685236ad0164cd1316710540&quot;,&quot;ai_summary&quot;:&quot;Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.&quot;,&quot;ai_keywords&quot;:[&quot;Mixture-of-Experts (MoE)&quot;,&quot;reinforcement learning (RL)&quot;,&quot;Ling-lite&quot;,&quot;AIME&quot;,&quot;LiveCodeBench&quot;,&quot;GPQA-Diamond&quot;,&quot;Constrained Contextual Computation Policy Optimization(C3PO)&quot;,&quot;entropy loss&quot;,&quot;two-stage training paradigm&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T13:12:34.000Z&quot;,&quot;title&quot;:&quot;Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\\n  for LLMs&quot;,&quot;summary&quot;:&quot;We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\\noptimized via reinforcement learning (RL) to achieve efficient and robust\\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\\n16.8 billion parameter model with 2.75 billion activated parameters, our\\napproach matches the performance of state-of-the-art (SOTA) small-scale\\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\\nGPQA-Diamond) while activating only one-third of the parameters required by\\ncomparable models. To accomplish this, we introduce a joint training pipeline\\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\\ntraining. First, we identify optimization instability during RL training, and\\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\\nnovel approach that enhances training stability and improves computational\\nthroughput via algorithm-system co-design methodology. Second, we empirically\\ndemonstrate that selecting distillation checkpoints based on entropy loss for\\nRL training, rather than validation metrics, yields superior\\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\\ndevelop a two-stage training paradigm to harmonize multi-domain data\\nintegration, addressing domain conflicts that arise in training with mixed\\ndataset. We will release the model, dataset, and code.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14731.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;fullname&quot;:&quot;AK&quot;,&quot;name&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:7139},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14702&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6852331e0164cd13167104fb&quot;,&quot;name&quot;:&quot;Daniel D'souza&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852331e0164cd13167104fc&quot;,&quot;name&quot;:&quot;Julia Kreutzer&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852331e0164cd13167104fd&quot;,&quot;name&quot;:&quot;Adrien Morisot&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852331e0164cd13167104fe&quot;,&quot;name&quot;:&quot;Ahmet Üstün&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852331e0164cd13167104ff&quot;,&quot;name&quot;:&quot;Sara Hooker&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png&quot;],&quot;publishedAt&quot;:&quot;2025-06-17T16:40:42.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T02:55:26.808Z&quot;,&quot;title&quot;:&quot;Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\\n  Markers&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6658011eaba105a066e37e1b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Daniel D'souza&quot;,&quot;user&quot;:&quot;dsouzadaniel&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;One of the most profound challenges of modern machine learning is performing\\nwell on the long-tail of rare and underrepresented features. Large\\ngeneral-purpose models are trained for many tasks, but work best on\\nhigh-frequency use cases. After training, it is hard to adapt a model to\\nperform well on specific use cases underrepresented in the training corpus.\\nRelying on prompt engineering or few-shot examples to maximize the output\\nquality on a particular test case can be frustrating, as models can be highly\\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\\nprompt for maintaining performance. In this work, we ask: \\&quot;Can we optimize our\\ntraining protocols to both improve controllability and performance on\\nunderrepresented use cases at inference time?\\&quot; We revisit the divide between\\ntraining and inference techniques to improve long-tail performance while\\nproviding users with a set of control levers the model is trained to be\\nresponsive to. We create a detailed taxonomy of data characteristics and task\\nprovenance to explicitly control generation attributes and implicitly condition\\ngenerations at inference time. We fine-tune a base model to infer these markers\\nautomatically, which makes them optional at inference time. This principled and\\nflexible approach yields pronounced improvements in performance, especially on\\nexamples from the long tail of the training distribution. While we observe an\\naverage lift of 5.7% win rates in open-ended generation quality with our\\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\\nabsolute improvements of 35.3% on length instruction following evaluations.&quot;,&quot;upvotes&quot;:2,&quot;discussionId&quot;:&quot;6852331e0164cd1316710500&quot;,&quot;ai_summary&quot;:&quot;A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.&quot;,&quot;ai_keywords&quot;:[&quot;prompt engineering&quot;,&quot;few-shot examples&quot;,&quot;controllability&quot;,&quot;performance&quot;,&quot;long-tail&quot;,&quot;training protocols&quot;,&quot;inference techniques&quot;,&quot;taxonomy&quot;,&quot;data characteristics&quot;,&quot;task provenance&quot;,&quot;fine-tuning&quot;,&quot;generation attributes&quot;,&quot;markers&quot;,&quot;underrepresented domains&quot;,&quot;CodeRepair&quot;,&quot;length instruction following&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T12:40:42.000Z&quot;,&quot;title&quot;:&quot;Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\\n  Markers&quot;,&quot;summary&quot;:&quot;One of the most profound challenges of modern machine learning is performing\\nwell on the long-tail of rare and underrepresented features. Large\\ngeneral-purpose models are trained for many tasks, but work best on\\nhigh-frequency use cases. After training, it is hard to adapt a model to\\nperform well on specific use cases underrepresented in the training corpus.\\nRelying on prompt engineering or few-shot examples to maximize the output\\nquality on a particular test case can be frustrating, as models can be highly\\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\\nprompt for maintaining performance. In this work, we ask: \\&quot;Can we optimize our\\ntraining protocols to both improve controllability and performance on\\nunderrepresented use cases at inference time?\\&quot; We revisit the divide between\\ntraining and inference techniques to improve long-tail performance while\\nproviding users with a set of control levers the model is trained to be\\nresponsive to. We create a detailed taxonomy of data characteristics and task\\nprovenance to explicitly control generation attributes and implicitly condition\\ngenerations at inference time. We fine-tune a base model to infer these markers\\nautomatically, which makes them optional at inference time. This principled and\\nflexible approach yields pronounced improvements in performance, especially on\\nexamples from the long tail of the training distribution. While we observe an\\naverage lift of 5.7% win rates in open-ended generation quality with our\\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\\nabsolute improvements of 35.3% on length instruction following evaluations.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14702.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6658011eaba105a066e37e1b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg&quot;,&quot;fullname&quot;:&quot;Daniel D'souza&quot;,&quot;name&quot;:&quot;dsouzadaniel&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13599&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685220810164cd131671048e&quot;,&quot;name&quot;:&quot;Yuwei Du&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685220810164cd131671048f&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6465d3bd63e7e09dd02e95c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jie Feng&quot;,&quot;user&quot;:&quot;JJ-TMT&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Jie Feng&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:59:19.122Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685220810164cd1316710490&quot;,&quot;name&quot;:&quot;Jian Yuan&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685220810164cd1316710491&quot;,&quot;name&quot;:&quot;Yong Li&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg&quot;],&quot;publishedAt&quot;:&quot;2025-06-16T15:24:07.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T01:27:33.225Z&quot;,&quot;title&quot;:&quot;CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\\n  Simulation&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6465d3bd63e7e09dd02e95c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Jie Feng&quot;,&quot;user&quot;:&quot;JJ-TMT&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Human mobility simulation plays a crucial role in various real-world\\napplications. Recently, to address the limitations of traditional data-driven\\napproaches, researchers have explored leveraging the commonsense knowledge and\\nreasoning capabilities of large language models (LLMs) to accelerate human\\nmobility simulation. However, these methods suffer from several critical\\nshortcomings, including inadequate modeling of urban spaces and poor\\nintegration with both individual mobility patterns and collective mobility\\ndistributions. To address these challenges, we propose CityGPT-Powered\\nAgentic framework for Mobility Simulation\\n(CAMS), an agentic framework that leverages the language based urban\\nfoundation model to simulate human mobility in urban space. CAMS\\ncomprises three core modules, including MobExtractor to extract template\\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\\nto generate anchor points considering collective knowledge and generate\\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\\ngenerate trajectories with real trajectory preference alignment via DPO.\\nExperiments on real-world datasets show that CAMS achieves superior\\nperformance without relying on externally provided geospatial information.\\nMoreover, by holistically modeling both individual mobility patterns and\\ncollective mobility constraints, CAMS generates more realistic and\\nplausible trajectories. In general, CAMS establishes a new paradigm\\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\\nmobility simulation.&quot;,&quot;upvotes&quot;:2,&quot;discussionId&quot;:&quot;685220820164cd1316710492&quot;,&quot;ai_summary&quot;:&quot;CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.&quot;,&quot;ai_keywords&quot;:[&quot;large language models&quot;,&quot;CityGPT&quot;,&quot;agentic framework&quot;,&quot;human mobility simulation&quot;,&quot;urban spaces&quot;,&quot;individual mobility patterns&quot;,&quot;collective mobility distributions&quot;,&quot;MobExtractor&quot;,&quot;GeoGenerator&quot;,&quot;TrajEnhancer&quot;,&quot;DPO&quot;,&quot;trajectory preference alignment&quot;,&quot;real-world datasets&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T11:24:07.000Z&quot;,&quot;title&quot;:&quot;CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\\n  Simulation&quot;,&quot;summary&quot;:&quot;Human mobility simulation plays a crucial role in various real-world\\napplications. Recently, to address the limitations of traditional data-driven\\napproaches, researchers have explored leveraging the commonsense knowledge and\\nreasoning capabilities of large language models (LLMs) to accelerate human\\nmobility simulation. However, these methods suffer from several critical\\nshortcomings, including inadequate modeling of urban spaces and poor\\nintegration with both individual mobility patterns and collective mobility\\ndistributions. To address these challenges, we propose CityGPT-Powered\\nAgentic framework for Mobility Simulation\\n(CAMS), an agentic framework that leverages the language based urban\\nfoundation model to simulate human mobility in urban space. CAMS\\ncomprises three core modules, including MobExtractor to extract template\\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\\nto generate anchor points considering collective knowledge and generate\\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\\ngenerate trajectories with real trajectory preference alignment via DPO.\\nExperiments on real-world datasets show that CAMS achieves superior\\nperformance without relying on externally provided geospatial information.\\nMoreover, by holistically modeling both individual mobility patterns and\\ncollective mobility constraints, CAMS generates more realistic and\\nplausible trajectories. In general, CAMS establishes a new paradigm\\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\\nmobility simulation.&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13599.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6465d3bd63e7e09dd02e95c3&quot;,&quot;avatarUrl&quot;:&quot;/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg&quot;,&quot;fullname&quot;:&quot;Jie Feng&quot;,&quot;name&quot;:&quot;JJ-TMT&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.12880&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685296b1a523e43c8c016658&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;635671cdef1d4c919152b8e8&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Matan BT&quot;,&quot;user&quot;:&quot;MatanBT&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Matan Ben-Tov&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:58:44.651Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685296b1a523e43c8c016659&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;610b729f9da682cd54ad9adf&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1628140189042-noauth.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Mor Geva&quot;,&quot;user&quot;:&quot;mega&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Mor Geva&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-19T09:10:27.494Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685296b1a523e43c8c01665a&quot;,&quot;name&quot;:&quot;Mahmood Sharif&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-15T15:20:37.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T10:20:54.475Z&quot;,&quot;title&quot;:&quot;Universal Jailbreak Suffixes Are Strong Attention Hijackers&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;635671cdef1d4c919152b8e8&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Matan BT&quot;,&quot;user&quot;:&quot;MatanBT&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We study suffix-based jailbreaksx2013a powerful family of attacks\\nagainst large language models (LLMs) that optimize adversarial suffixes to\\ncircumvent safety alignment. Focusing on the widely used foundational GCG\\nattack (Zou et al., 2023), we observe that suffixes vary in efficacy: some\\nmarkedly more universalx2013generalizing to many unseen harmful\\ninstructionsx2013than others. We first show that GCG's\\neffectiveness is driven by a shallow, critical mechanism, built on the\\ninformation flow from the adversarial suffix to the final chat template tokens\\nbefore generation. Quantifying the dominance of this mechanism during\\ngeneration, we find GCG irregularly and aggressively hijacks the\\ncontextualization process. Crucially, we tie hijacking to the universality\\nphenomenon, with more universal suffixes being stronger hijackers.\\nSubsequently, we show that these insights have practical implications: GCG\\nuniversality can be efficiently enhanced (up to times5 in some cases) at no\\nadditional computational cost, and can also be surgically mitigated, at least\\nhalving attack success with minimal utility loss. We release our code and data\\nat http://github.com/matanbt/interp-jailbreak.&quot;,&quot;upvotes&quot;:2,&quot;discussionId&quot;:&quot;685296b1a523e43c8c01665b&quot;,&quot;githubRepo&quot;:&quot;https://github.com/matanbt/interp-jailbreak&quot;,&quot;ai_summary&quot;:&quot;Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.&quot;,&quot;ai_keywords&quot;:[&quot;suffix-based jailbreaks&quot;,&quot;large language models&quot;,&quot;adversarial suffixes&quot;,&quot;safety alignment&quot;,&quot;GCG attack&quot;,&quot;information flow&quot;,&quot;contextualization process&quot;,&quot;suffix universality&quot;,&quot;hijacking&quot;,&quot;attack mitigation&quot;]},&quot;publishedAt&quot;:&quot;2025-06-15T11:20:37.000Z&quot;,&quot;title&quot;:&quot;Universal Jailbreak Suffixes Are Strong Attention Hijackers&quot;,&quot;summary&quot;:&quot;We study suffix-based jailbreaksx2013a powerful family of attacks\\nagainst large language models (LLMs) that optimize adversarial suffixes to\\ncircumvent safety alignment. Focusing on the widely used foundational GCG\\nattack (Zou et al., 2023), we observe that suffixes vary in efficacy: some\\nmarkedly more universalx2013generalizing to many unseen harmful\\ninstructionsx2013than others. We first show that GCG's\\neffectiveness is driven by a shallow, critical mechanism, built on the\\ninformation flow from the adversarial suffix to the final chat template tokens\\nbefore generation. Quantifying the dominance of this mechanism during\\ngeneration, we find GCG irregularly and aggressively hijacks the\\ncontextualization process. Crucially, we tie hijacking to the universality\\nphenomenon, with more universal suffixes being stronger hijackers.\\nSubsequently, we show that these insights have practical implications: GCG\\nuniversality can be efficiently enhanced (up to times5 in some cases) at no\\nadditional computational cost, and can also be surgically mitigated, at least\\nhalving attack success with minimal utility loss. We release our code and data\\nat http://github.com/matanbt/interp-jailbreak.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12880.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;635671cdef1d4c919152b8e8&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg&quot;,&quot;fullname&quot;:&quot;Matan BT&quot;,&quot;name&quot;:&quot;MatanBT&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.05426&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68527e8ce329b3b5e93f2997&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;6756970ce110734a48701a08&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wenhao Wu&quot;,&quot;user&quot;:&quot;Wenhao0&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Wenhao Wu&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:58:48.394Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68527e8ce329b3b5e93f2998&quot;,&quot;name&quot;:&quot;Fuhong Liu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68527e8ce329b3b5e93f2999&quot;,&quot;name&quot;:&quot;Haoru Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68527e8ce329b3b5e93f299a&quot;,&quot;name&quot;:&quot;Zican Hu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68527e8ce329b3b5e93f299b&quot;,&quot;name&quot;:&quot;Daoyi Dong&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68527e8ce329b3b5e93f299c&quot;,&quot;name&quot;:&quot;Chunlin Chen&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68527e8ce329b3b5e93f299d&quot;,&quot;name&quot;:&quot;Zhi Wang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-05T06:29:14.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T09:34:58.708Z&quot;,&quot;title&quot;:&quot;Mixture-of-Experts Meets In-Context Reinforcement Learning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6756970ce110734a48701a08&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Wenhao Wu&quot;,&quot;user&quot;:&quot;Wenhao0&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;In-context reinforcement learning (ICRL) has emerged as a promising paradigm\\nfor adapting RL agents to downstream tasks through prompt conditioning.\\nHowever, two notable challenges remain in fully harnessing in-context learning\\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\\nand the diverse, heterogeneous nature of decision tasks. To tackle these\\nchallenges, we propose T2MIR (Token- and Task-wise\\nMoE for In-context RL), an innovative framework that\\nintroduces architectural advances of mixture-of-experts (MoE) into\\ntransformer-based decision models. T2MIR substitutes the feedforward layer with\\ntwo parallel layers: a token-wise MoE that captures distinct semantics of input\\ntokens across multiple modalities, and a task-wise MoE that routes diverse\\ntasks to specialized experts for managing a broad task distribution with\\nalleviated gradient conflicts. To enhance task-wise routing, we introduce a\\ncontrastive learning method that maximizes the mutual information between the\\ntask and its router representation, enabling more precise capture of\\ntask-relevant information. The outputs of two MoE components are concatenated\\nand fed into the next layer. Comprehensive experiments show that T2MIR\\nsignificantly facilitates in-context learning capacity and outperforms various\\ntypes of baselines. We bring the potential and promise of MoE to ICRL, offering\\na simple and scalable architectural enhancement to advance ICRL one step closer\\ntoward achievements in language and vision communities. Our code is available\\nat https://github.com/NJU-RL/T2MIR.&quot;,&quot;upvotes&quot;:2,&quot;discussionId&quot;:&quot;68527e8de329b3b5e93f299e&quot;,&quot;ai_summary&quot;:&quot;T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.&quot;,&quot;ai_keywords&quot;:[&quot;Mixture-of-experts (MoE)&quot;,&quot;transformer-based decision models&quot;,&quot;token-wise MoE&quot;,&quot;task-wise MoE&quot;,&quot;contrastive learning&quot;,&quot;mutual information&quot;]},&quot;publishedAt&quot;:&quot;2025-06-05T02:29:14.000Z&quot;,&quot;title&quot;:&quot;Mixture-of-Experts Meets In-Context Reinforcement Learning&quot;,&quot;summary&quot;:&quot;In-context reinforcement learning (ICRL) has emerged as a promising paradigm\\nfor adapting RL agents to downstream tasks through prompt conditioning.\\nHowever, two notable challenges remain in fully harnessing in-context learning\\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\\nand the diverse, heterogeneous nature of decision tasks. To tackle these\\nchallenges, we propose T2MIR (Token- and Task-wise\\nMoE for In-context RL), an innovative framework that\\nintroduces architectural advances of mixture-of-experts (MoE) into\\ntransformer-based decision models. T2MIR substitutes the feedforward layer with\\ntwo parallel layers: a token-wise MoE that captures distinct semantics of input\\ntokens across multiple modalities, and a task-wise MoE that routes diverse\\ntasks to specialized experts for managing a broad task distribution with\\nalleviated gradient conflicts. To enhance task-wise routing, we introduce a\\ncontrastive learning method that maximizes the mutual information between the\\ntask and its router representation, enabling more precise capture of\\ntask-relevant information. The outputs of two MoE components are concatenated\\nand fed into the next layer. Comprehensive experiments show that T2MIR\\nsignificantly facilitates in-context learning capacity and outperforms various\\ntypes of baselines. We bring the potential and promise of MoE to ICRL, offering\\na simple and scalable architectural enhancement to advance ICRL one step closer\\ntoward achievements in language and vision communities. Our code is available\\nat https://github.com/NJU-RL/T2MIR.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05426.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6756970ce110734a48701a08&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg&quot;,&quot;fullname&quot;:&quot;Wenhao Wu&quot;,&quot;name&quot;:&quot;Wenhao0&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14205&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6853647b99bf39f9665c7941&quot;,&quot;name&quot;:&quot;Jingxu Xie&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6853647b99bf39f9665c7942&quot;,&quot;name&quot;:&quot;Dylan Xu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6853647b99bf39f9665c7943&quot;,&quot;name&quot;:&quot;Xuandong Zhao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6853647b99bf39f9665c7944&quot;,&quot;name&quot;:&quot;Dawn Song&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-17T05:46:52.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T23:47:08.172Z&quot;,&quot;title&quot;:&quot;AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;6275a465597c70eb8949fce5&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Xuandong Zhao&quot;,&quot;user&quot;:&quot;Xuandong&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;We introduce AgentSynth, a scalable and cost-efficient pipeline for\\nautomatically synthesizing high-quality tasks and trajectory datasets for\\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\\nconstructs subtasks that are simple during generation but significantly more\\nchallenging when composed into long-horizon tasks, enabling the creation of\\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\\ntask proposer guided by a persona, followed by an execution agent that\\ncompletes the task and logs the trajectory. This process is repeated\\niteratively to form a sequence of subtasks, which are then summarized by a\\nseparate agent into a composite task of controllable difficulty. A key strength\\nof AgentSynth is its ability to precisely modulate task complexity by varying\\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\\nagents suffer a steep performance drop, from 18% success at difficulty level 1\\nto just 4% at level 6, highlighting the benchmark's difficulty and\\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\\n\\\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\\ncode and data are publicly available at\\nhttps://github.com/sunblaze-ucb/AgentSynth&quot;,&quot;upvotes&quot;:1,&quot;discussionId&quot;:&quot;6853647c99bf39f9665c7945&quot;,&quot;ai_summary&quot;:&quot;AgentSynth synthesizes high-quality, diverse tasks and trajectory datasets for generalist computer-use agents using LLMs and an iterative subtask construction approach, enabling precise control over task complexity and offering significant cost savings compared to human annotations.&quot;,&quot;ai_keywords&quot;:[&quot;LLM-based task proposer&quot;,&quot;execution agent&quot;,&quot;composite task&quot;,&quot;subtasks&quot;,&quot;state-of-the-art LLM agents&quot;,&quot;performance drop&quot;,&quot;human annotations&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T01:46:52.000Z&quot;,&quot;title&quot;:&quot;AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents&quot;,&quot;summary&quot;:&quot;We introduce AgentSynth, a scalable and cost-efficient pipeline for\\nautomatically synthesizing high-quality tasks and trajectory datasets for\\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\\nconstructs subtasks that are simple during generation but significantly more\\nchallenging when composed into long-horizon tasks, enabling the creation of\\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\\ntask proposer guided by a persona, followed by an execution agent that\\ncompletes the task and logs the trajectory. This process is repeated\\niteratively to form a sequence of subtasks, which are then summarized by a\\nseparate agent into a composite task of controllable difficulty. A key strength\\nof AgentSynth is its ability to precisely modulate task complexity by varying\\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\\nagents suffer a steep performance drop, from 18% success at difficulty level 1\\nto just 4% at level 6, highlighting the benchmark's difficulty and\\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\\n\\\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\\ncode and data are publicly available at\\nhttps://github.com/sunblaze-ucb/AgentSynth&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14205.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;6275a465597c70eb8949fce5&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png&quot;,&quot;fullname&quot;:&quot;Xuandong Zhao&quot;,&quot;name&quot;:&quot;Xuandong&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13901&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;685252860164cd13167105c7&quot;,&quot;name&quot;:&quot;Abhilekh Borah&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105c8&quot;,&quot;name&quot;:&quot;Chhavi Sharma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105c9&quot;,&quot;name&quot;:&quot;Danush Khanna&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105ca&quot;,&quot;name&quot;:&quot;Utkarsh Bhatt&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105cb&quot;,&quot;name&quot;:&quot;Gurpreet Singh&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105cc&quot;,&quot;name&quot;:&quot;Hasnat Md Abdullah&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105cd&quot;,&quot;name&quot;:&quot;Raghav Kaushik Ravi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105ce&quot;,&quot;name&quot;:&quot;Vinija Jain&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105cf&quot;,&quot;name&quot;:&quot;Jyoti Patel&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105d0&quot;,&quot;name&quot;:&quot;Shubham Singh&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105d1&quot;,&quot;name&quot;:&quot;Vasu Sharma&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105d2&quot;,&quot;name&quot;:&quot;Arpita Vats&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105d3&quot;,&quot;name&quot;:&quot;Rahul Raja&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105d4&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;63a4754927f1f64ed7238dac&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Aman Chadha&quot;,&quot;user&quot;:&quot;amanchadha&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Aman Chadha&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T10:58:55.038Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;685252860164cd13167105d5&quot;,&quot;name&quot;:&quot;Amitava Das&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-16T18:22:28.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T04:17:43.427Z&quot;,&quot;title&quot;:&quot;Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\\n  Pooled Representations&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;63a4754927f1f64ed7238dac&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Aman Chadha&quot;,&quot;user&quot;:&quot;amanchadha&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Alignment is no longer a luxury, it is a necessity. As large language models\\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\\nlaw, their behavior must reliably reflect human-aligned values and safety\\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\\nstochasticity of generation, and alignment faking.\\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\\nby analyzing the separation of safe and unsafe activations in latent space. By\\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\\nformulations, AQI captures clustering quality to detect hidden misalignments\\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\\nearly warning signal for alignment faking, offering a robust, decoding\\ninvariant tool for behavior agnostic safety auditing.\\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\\nunder these challenging conditions. Empirical tests on LITMUS across different\\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\\ncorrelation with external judges and ability to reveal vulnerabilities missed\\nby refusal metrics. We make our implementation publicly available to foster\\nfuture research in this area.&quot;,&quot;upvotes&quot;:1,&quot;discussionId&quot;:&quot;685252870164cd13167105d6&quot;,&quot;ai_summary&quot;:&quot;A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.&quot;,&quot;ai_keywords&quot;:[&quot;Alignment Quality Index (AQI)&quot;,&quot;latent space&quot;,&quot;Davies-Bouldin Score (DBS)&quot;,&quot;Dunn Index (DI)&quot;,&quot;Xie-Beni Index (XBI)&quot;,&quot;Calinski-Harabasz Index (CHI)&quot;,&quot;LITMUS dataset&quot;,&quot;DPO&quot;,&quot;GRPO&quot;,&quot;RLHF&quot;,&quot;alignment faking&quot;,&quot;external judges&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T14:22:28.000Z&quot;,&quot;title&quot;:&quot;Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\\n  Pooled Representations&quot;,&quot;summary&quot;:&quot;Alignment is no longer a luxury, it is a necessity. As large language models\\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\\nlaw, their behavior must reliably reflect human-aligned values and safety\\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\\nstochasticity of generation, and alignment faking.\\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\\nby analyzing the separation of safe and unsafe activations in latent space. By\\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\\nformulations, AQI captures clustering quality to detect hidden misalignments\\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\\nearly warning signal for alignment faking, offering a robust, decoding\\ninvariant tool for behavior agnostic safety auditing.\\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\\nunder these challenging conditions. Empirical tests on LITMUS across different\\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\\ncorrelation with external judges and ability to reveal vulnerabilities missed\\nby refusal metrics. We make our implementation publicly available to foster\\nfuture research in this area.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13901.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;63a4754927f1f64ed7238dac&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg&quot;,&quot;fullname&quot;:&quot;Aman Chadha&quot;,&quot;name&quot;:&quot;amanchadha&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:6},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13387&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6852239f0164cd13167104a4&quot;,&quot;name&quot;:&quot;Beilei Cui&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852239f0164cd13167104a5&quot;,&quot;name&quot;:&quot;Yiming Huang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852239f0164cd13167104a6&quot;,&quot;name&quot;:&quot;Long Bai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852239f0164cd13167104a7&quot;,&quot;name&quot;:&quot;Hongliang Ren&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-16T11:50:00.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T00:57:15.579Z&quot;,&quot;title&quot;:&quot;TR2M: Transferring Monocular Relative Depth to Metric Depth with\\n  Language Descriptions and Scale-Oriented Contrast&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;68518fb45452a74491857c5b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Beilei Cui&quot;,&quot;user&quot;:&quot;BeileiCui&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;This work presents a generalizable framework to transfer relative depth to\\nmetric depth. Current monocular depth estimation methods are mainly divided\\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\\nestimate depth in metric scale but are often limited to a specific domain.\\nMRDEs generalize well across different domains, but with uncertain scales which\\nhinders downstream applications. To this end, we aim to build up a framework to\\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\\nmethods used language as input and estimated two factors for conducting\\nrescaling. Our approach, TR2M, utilizes both text description and image as\\ninputs and estimates two rescale maps to transfer relative depth to metric\\ndepth at pixel level. Features from two modalities are fused with a\\ncross-modality attention module to better capture scale information. A strategy\\nis designed to construct and filter confident pseudo metric depth for more\\ncomprehensive supervision. We also develop scale-oriented contrastive learning\\nto utilize depth distribution as guidance to enforce the model learning about\\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\\nsmall number of trainable parameters to train on datasets in various domains\\nand experiments not only demonstrate TR2M's great performance in seen datasets\\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\\nshow the huge potential in pixel-wise transferring relative depth to metric\\ndepth with language assistance. (Code is available at:\\nhttps://github.com/BeileiCui/TR2M)&quot;,&quot;upvotes&quot;:1,&quot;discussionId&quot;:&quot;6852239f0164cd13167104a8&quot;,&quot;ai_summary&quot;:&quot;A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.&quot;,&quot;ai_keywords&quot;:[&quot;relative depth estimation&quot;,&quot;metric depth estimation&quot;,&quot;cross-modality attention&quot;,&quot;contrastive learning&quot;,&quot;rescale maps&quot;,&quot;pseudo metric depth&quot;,&quot;intrinsically aligned scale distribution&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T07:50:00.000Z&quot;,&quot;title&quot;:&quot;TR2M: Transferring Monocular Relative Depth to Metric Depth with\\n  Language Descriptions and Scale-Oriented Contrast&quot;,&quot;summary&quot;:&quot;This work presents a generalizable framework to transfer relative depth to\\nmetric depth. Current monocular depth estimation methods are mainly divided\\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\\nestimate depth in metric scale but are often limited to a specific domain.\\nMRDEs generalize well across different domains, but with uncertain scales which\\nhinders downstream applications. To this end, we aim to build up a framework to\\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\\nmethods used language as input and estimated two factors for conducting\\nrescaling. Our approach, TR2M, utilizes both text description and image as\\ninputs and estimates two rescale maps to transfer relative depth to metric\\ndepth at pixel level. Features from two modalities are fused with a\\ncross-modality attention module to better capture scale information. A strategy\\nis designed to construct and filter confident pseudo metric depth for more\\ncomprehensive supervision. We also develop scale-oriented contrastive learning\\nto utilize depth distribution as guidance to enforce the model learning about\\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\\nsmall number of trainable parameters to train on datasets in various domains\\nand experiments not only demonstrate TR2M's great performance in seen datasets\\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\\nshow the huge potential in pixel-wise transferring relative depth to metric\\ndepth with language assistance. (Code is available at:\\nhttps://github.com/BeileiCui/TR2M)&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13387.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;68518fb45452a74491857c5b&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png&quot;,&quot;fullname&quot;:&quot;Beilei Cui&quot;,&quot;name&quot;:&quot;BeileiCui&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.14629&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6852b60f10fbcda7a650e024&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;66ec02212eb421192b62c303&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aw0TJjO3ohRlEYWlCzxQY.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Md. Adnanul Islam&quot;,&quot;user&quot;:&quot;Adnanul&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Md. Adnanul Islam&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T19:47:56.745Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b60f10fbcda7a650e025&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;667dcb41bc9abbfa3408382a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Md. Faiyaz Abdullah Sayeedi&quot;,&quot;user&quot;:&quot;FaiyazAbdullah114708&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Md. Faiyaz Abdullah Sayeedi&quot;,&quot;status&quot;:&quot;claimed_verified&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T16:15:25.634Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b60f10fbcda7a650e026&quot;,&quot;name&quot;:&quot;Md. Asaduzzaman Shuvo&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b60f10fbcda7a650e027&quot;,&quot;name&quot;:&quot;Muhammad Ziaur Rahman&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b60f10fbcda7a650e028&quot;,&quot;name&quot;:&quot;Shahanur Rahman Bappy&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b60f10fbcda7a650e029&quot;,&quot;name&quot;:&quot;Raiyan Rahman&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852b60f10fbcda7a650e02a&quot;,&quot;name&quot;:&quot;Swakkhar Shatabda&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-17T15:24:30.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T11:28:31.203Z&quot;,&quot;title&quot;:&quot;VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\\n  Mosquito Breeding Site Detection and Reasoning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;667dcb41bc9abbfa3408382a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Md. Faiyaz Abdullah Sayeedi&quot;,&quot;user&quot;:&quot;FaiyazAbdullah114708&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Mosquito-borne diseases pose a major global health risk, requiring early\\ndetection and proactive control of breeding sites to prevent outbreaks. In this\\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\\nand textual data to support automated detection, segmentation, and reasoning\\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\\nimages for object detection, 142 images for water surface segmentation, and\\nnatural language reasoning texts linked to each image. The YOLOv9s model\\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\\n\\&quot;Prevention is Better than Cure\\&quot;, showcasing how AI-based detection can\\nproactively address mosquito-borne disease risks. The dataset and\\nimplementation code are publicly available at GitHub:\\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito&quot;,&quot;upvotes&quot;:0,&quot;discussionId&quot;:&quot;6852b60f10fbcda7a650e02b&quot;,&quot;projectPage&quot;:&quot;https://data.mendeley.com/datasets/rtsfh7jh7p/2&quot;,&quot;githubRepo&quot;:&quot;https://github.com/adnanul-islam-jisun/VisText-Mosquito&quot;,&quot;ai_summary&quot;:&quot;VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.&quot;,&quot;ai_keywords&quot;:[&quot;YOLOv9s&quot;,&quot;YOLOv11n-Seg&quot;,&quot;BLIP model&quot;,&quot;object detection&quot;,&quot;water surface segmentation&quot;,&quot;reasoning generation&quot;,&quot;BLEU score&quot;,&quot;BERTScore&quot;,&quot;ROUGE-L&quot;]},&quot;publishedAt&quot;:&quot;2025-06-17T11:24:30.000Z&quot;,&quot;title&quot;:&quot;VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\\n  Mosquito Breeding Site Detection and Reasoning&quot;,&quot;summary&quot;:&quot;Mosquito-borne diseases pose a major global health risk, requiring early\\ndetection and proactive control of breeding sites to prevent outbreaks. In this\\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\\nand textual data to support automated detection, segmentation, and reasoning\\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\\nimages for object detection, 142 images for water surface segmentation, and\\nnatural language reasoning texts linked to each image. The YOLOv9s model\\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\\n\\&quot;Prevention is Better than Cure\\&quot;, showcasing how AI-based detection can\\nproactively address mosquito-borne disease risks. The dataset and\\nimplementation code are publicly available at GitHub:\\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14629.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;667dcb41bc9abbfa3408382a&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg&quot;,&quot;fullname&quot;:&quot;Md. Faiyaz Abdullah Sayeedi&quot;,&quot;name&quot;:&quot;FaiyazAbdullah114708&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.13922&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6852c1cf8dd3b4a7bdb8d9a9&quot;,&quot;user&quot;:{&quot;_id&quot;:&quot;64e153cf75fae2212e8a140c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Maximilian Du&quot;,&quot;user&quot;:&quot;MaxDu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;name&quot;:&quot;Maximilian Du&quot;,&quot;status&quot;:&quot;extracted_confirmed&quot;,&quot;statusLastChangedAt&quot;:&quot;2025-06-18T15:41:45.280Z&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852c1cf8dd3b4a7bdb8d9aa&quot;,&quot;name&quot;:&quot;Shuran Song&quot;,&quot;hidden&quot;:false}],&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/64e153cf75fae2212e8a140c/fCFLL8CQmmy_9LeN0YPhG.mp4&quot;],&quot;publishedAt&quot;:&quot;2025-06-16T19:00:54.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T12:14:52.738Z&quot;,&quot;title&quot;:&quot;DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;64e153cf75fae2212e8a140c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Maximilian Du&quot;,&quot;user&quot;:&quot;MaxDu&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Deploying large, complex policies in the real world requires the ability to\\nsteer them to fit the needs of a situation. Most common steering approaches,\\nlike goal-conditioning, require training the robot policy with a distribution\\nof test-time objectives in mind. To overcome this limitation, we present\\nDynaGuide, a steering method for diffusion policies using guidance from an\\nexternal dynamics model during the diffusion denoising process. DynaGuide\\nseparates the dynamics model from the base policy, which gives it multiple\\nadvantages, including the ability to steer towards multiple objectives, enhance\\nunderrepresented base policy behaviors, and maintain robustness on low-quality\\nobjectives. The separate guidance signal also allows DynaGuide to work with\\noff-the-shelf pretrained diffusion policies. We demonstrate the performance and\\nfeatures of DynaGuide against other steering approaches in a series of\\nsimulated and real experiments, showing an average steering success of 70% on a\\nset of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x\\nwhen steered with low-quality objectives. We also successfully steer an\\noff-the-shelf real robot policy to express preference for particular objects\\nand even create novel behavior. Videos and more can be found on the project\\nwebsite: https://dynaguide.github.io&quot;,&quot;upvotes&quot;:0,&quot;discussionId&quot;:&quot;6852c1cf8dd3b4a7bdb8d9ab&quot;,&quot;projectPage&quot;:&quot;https://dynaguide.github.io/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/MaxDu17/DynaGuide&quot;,&quot;ai_summary&quot;:&quot;DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.&quot;,&quot;ai_keywords&quot;:[&quot;diffusion policies&quot;,&quot;guidance&quot;,&quot;external dynamics model&quot;,&quot;diffusion denoising process&quot;,&quot;articulated CALVIN tasks&quot;,&quot;goal-conditioning&quot;,&quot;off-the-shelf real robot policy&quot;]},&quot;publishedAt&quot;:&quot;2025-06-16T15:00:54.000Z&quot;,&quot;title&quot;:&quot;DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance&quot;,&quot;summary&quot;:&quot;Deploying large, complex policies in the real world requires the ability to\\nsteer them to fit the needs of a situation. Most common steering approaches,\\nlike goal-conditioning, require training the robot policy with a distribution\\nof test-time objectives in mind. To overcome this limitation, we present\\nDynaGuide, a steering method for diffusion policies using guidance from an\\nexternal dynamics model during the diffusion denoising process. DynaGuide\\nseparates the dynamics model from the base policy, which gives it multiple\\nadvantages, including the ability to steer towards multiple objectives, enhance\\nunderrepresented base policy behaviors, and maintain robustness on low-quality\\nobjectives. The separate guidance signal also allows DynaGuide to work with\\noff-the-shelf pretrained diffusion policies. We demonstrate the performance and\\nfeatures of DynaGuide against other steering approaches in a series of\\nsimulated and real experiments, showing an average steering success of 70% on a\\nset of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x\\nwhen steered with low-quality objectives. We also successfully steer an\\noff-the-shelf real robot policy to express preference for particular objects\\nand even create novel behavior. Videos and more can be found on the project\\nwebsite: https://dynaguide.github.io&quot;,&quot;mediaUrls&quot;:[&quot;https://cdn-uploads.huggingface.co/production/uploads/64e153cf75fae2212e8a140c/fCFLL8CQmmy_9LeN0YPhG.mp4&quot;],&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13922.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;64e153cf75fae2212e8a140c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg&quot;,&quot;fullname&quot;:&quot;Maximilian Du&quot;,&quot;name&quot;:&quot;MaxDu&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:true},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.12015&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;68529976a523e43c8c016672&quot;,&quot;name&quot;:&quot;Hsi-Che Lin&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68529976a523e43c8c016673&quot;,&quot;name&quot;:&quot;Yu-Chu Yu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68529976a523e43c8c016674&quot;,&quot;name&quot;:&quot;Kai-Po Chang&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;68529976a523e43c8c016675&quot;,&quot;name&quot;:&quot;Yu-Chiang Frank Wang&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-13T17:59:58.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T09:33:40.094Z&quot;,&quot;title&quot;:&quot;EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;65099a88c9aa376f76bf756e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1e9643721c152f9999b6f35ba117a0d6.svg&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;HSI CHE LIN&quot;,&quot;user&quot;:&quot;hsichelin&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Open-source foundation models have seen rapid adoption and development,\\nenabling powerful general-purpose capabilities across diverse domains. However,\\nfine-tuning large foundation models for domain-specific or personalized tasks\\nremains prohibitively expensive for most users due to the significant memory\\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\\nmodel fine-tuning within the same memory budget required for inference. EMLoC\\nconstructs a task-specific light-weight emulator using activation-aware\\nsingular value decomposition (SVD) on a small downstream calibration set.\\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\\nthe misalignment between the original model and the compressed emulator, we\\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\\nwhich thus can be merged into the original model for inference. EMLoC supports\\nflexible compression ratios and standard training pipelines, making it\\nadaptable to a wide range of applications. Extensive experiments demonstrate\\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\\nindividual users.&quot;,&quot;upvotes&quot;:0,&quot;discussionId&quot;:&quot;68529976a523e43c8c016676&quot;,&quot;projectPage&quot;:&quot;https://hsi-che-lin.github.io/EMLoC/&quot;,&quot;githubRepo&quot;:&quot;https://github.com/hsi-che-lin/EMLoC&quot;,&quot;ai_summary&quot;:&quot;EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.&quot;,&quot;ai_keywords&quot;:[&quot;activation-aware singular value decomposition&quot;,&quot;SVD&quot;,&quot;Emulator-based Memory-efficient fine-tuning framework&quot;,&quot;LoRA Correction&quot;,&quot;LoRA&quot;,&quot;fine-tuning&quot;,&quot;model fine-tuning&quot;,&quot;standard training pipelines&quot;,&quot;model adaptation&quot;]},&quot;publishedAt&quot;:&quot;2025-06-13T13:59:58.000Z&quot;,&quot;title&quot;:&quot;EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction&quot;,&quot;summary&quot;:&quot;Open-source foundation models have seen rapid adoption and development,\\nenabling powerful general-purpose capabilities across diverse domains. However,\\nfine-tuning large foundation models for domain-specific or personalized tasks\\nremains prohibitively expensive for most users due to the significant memory\\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\\nmodel fine-tuning within the same memory budget required for inference. EMLoC\\nconstructs a task-specific light-weight emulator using activation-aware\\nsingular value decomposition (SVD) on a small downstream calibration set.\\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\\nthe misalignment between the original model and the compressed emulator, we\\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\\nwhich thus can be merged into the original model for inference. EMLoC supports\\nflexible compression ratios and standard training pipelines, making it\\nadaptable to a wide range of applications. Extensive experiments demonstrate\\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\\nindividual users.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12015.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;65099a88c9aa376f76bf756e&quot;,&quot;avatarUrl&quot;:&quot;/avatars/1e9643721c152f9999b6f35ba117a0d6.svg&quot;,&quot;fullname&quot;:&quot;HSI CHE LIN&quot;,&quot;name&quot;:&quot;hsichelin&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:1},&quot;isAuthorParticipating&quot;:false},{&quot;paper&quot;:{&quot;id&quot;:&quot;2506.03939&quot;,&quot;authors&quot;:[{&quot;_id&quot;:&quot;6852e674e6284bcd92d04b09&quot;,&quot;name&quot;:&quot;Junqi Gao&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852e674e6284bcd92d04b0a&quot;,&quot;name&quot;:&quot;Xiang Zou&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852e674e6284bcd92d04b0b&quot;,&quot;name&quot;:&quot;YIng Ai&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852e674e6284bcd92d04b0c&quot;,&quot;name&quot;:&quot;Dong Li&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852e674e6284bcd92d04b0d&quot;,&quot;name&quot;:&quot;Yichen Niu&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852e674e6284bcd92d04b0e&quot;,&quot;name&quot;:&quot;Biqing Qi&quot;,&quot;hidden&quot;:false},{&quot;_id&quot;:&quot;6852e674e6284bcd92d04b0f&quot;,&quot;name&quot;:&quot;Jianxing Liu&quot;,&quot;hidden&quot;:false}],&quot;publishedAt&quot;:&quot;2025-06-04T13:31:21.000Z&quot;,&quot;submittedOnDailyAt&quot;:&quot;2025-06-18T14:48:45.738Z&quot;,&quot;title&quot;:&quot;Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning&quot;,&quot;submittedOnDailyBy&quot;:{&quot;_id&quot;:&quot;67ab05fe4c6ca2d5db4c0c52&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png&quot;,&quot;isPro&quot;:false,&quot;fullname&quot;:&quot;Junqi Gao&quot;,&quot;user&quot;:&quot;ChetKao&quot;,&quot;type&quot;:&quot;user&quot;},&quot;summary&quot;:&quot;Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.&quot;,&quot;upvotes&quot;:0,&quot;discussionId&quot;:&quot;6852e674e6284bcd92d04b10&quot;,&quot;ai_summary&quot;:&quot;Graph Counselor enhances Large Language Models by using multi-agent collaboration and adaptive reasoning to integrate knowledge effectively, improving factual accuracy and generation quality in specialized domains.&quot;,&quot;ai_keywords&quot;:[&quot;Graph Retrieval Augmented Generation (GraphRAG)&quot;,&quot;Large Language Models (LLMs)&quot;,&quot;Information Aggregation&quot;,&quot;Reasoning Mechanism&quot;,&quot;Multi-agent collaboration&quot;,&quot;Adaptive Graph Information Extraction Module (AGIEM)&quot;,&quot;Planning Agents&quot;,&quot;Thought Agents&quot;,&quot;Execution Agents&quot;,&quot;Self-Reflection with Multiple Perspectives (SR)&quot;,&quot;multi-level dependency modeling&quot;,&quot;adaptive reasoning depth&quot;,&quot;graph reasoning tasks&quot;,&quot;reasoning accuracy&quot;,&quot;generalization ability&quot;]},&quot;publishedAt&quot;:&quot;2025-06-04T09:31:21.000Z&quot;,&quot;title&quot;:&quot;Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning&quot;,&quot;summary&quot;:&quot;Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.&quot;,&quot;thumbnail&quot;:&quot;https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03939.png&quot;,&quot;numComments&quot;:2,&quot;upvoted&quot;:false,&quot;submittedBy&quot;:{&quot;_id&quot;:&quot;67ab05fe4c6ca2d5db4c0c52&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png&quot;,&quot;fullname&quot;:&quot;Junqi Gao&quot;,&quot;name&quot;:&quot;ChetKao&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false},&quot;isAuthorParticipating&quot;:false}],&quot;prevDate&quot;:&quot;2025-06-17&quot;,&quot;publisher&quot;:{&quot;_id&quot;:&quot;60f1abe7544c2adfd699860c&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg&quot;,&quot;isPro&quot;:true,&quot;fullname&quot;:&quot;AK&quot;,&quot;user&quot;:&quot;akhaliq&quot;,&quot;type&quot;:&quot;user&quot;},&quot;periodType&quot;:&quot;day&quot;,&quot;nextDate&quot;:&quot;2025-06-19&quot;,&quot;query&quot;:{}}\">\n",
      "\n",
      "<section class=\"container relative mb-20 mt-8 md:mt-14\"><div class=\"mb-8 grid grid-cols-6 items-start md:mb-12 md:grid-cols-12 md:gap-x-4\"><div class=\"order-1 col-span-5 md:order-none md:col-span-10 lg:col-span-8 xl:col-span-4 xl:pl-0\"><div class=\"flex items-center gap-3\"><h1 class=\"text-2xl font-bold md:text-3xl\"><a href=\"/papers\" class=\"hover:text-gray-600 dark:hover:text-gray-300\">Daily Papers</a></h1>\n",
      "\t\t\t\t</div>\n",
      "\t\t\t<h2 class=\"whitespace-nowrap text-gray-500 xl:text-lg\">by<a href=\"/akhaliq\"><img alt=\"\" class=\"mx-1.5 inline h-4\" src=\"/front/assets/papers-by.png\"><span class=\"underline\">AK</span></a> and the research community\n",
      "\t\t\t</h2></div>\n",
      "\n",
      "\t\t<div class=\"order-1 col-span-1 flex justify-end pt-1 md:order-none md:col-span-2 md:mt-0 lg:col-span-4 xl:hidden\"></div>\n",
      "\n",
      "\t\t<div class=\"order-3 col-span-6 mt-3 flex items-center self-start md:col-span-12 md:mt-4 lg:col-span-12 xl:order-none xl:col-span-4 xl:mt-0\"><div class=\"relative z-1 flex w-full items-center\"><svg class=\"absolute left-[1.1rem] select-none text-gray-700 dark:text-gray-300\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 11 11\"><path fill=\"currentColor\" d=\"M4.881 4.182c0 .101-.031.2-.087.283a.5.5 0 0 1-.242.18l-.65.217a1.3 1.3 0 0 0-.484.299 1.3 1.3 0 0 0-.298.484l-.222.639a.46.46 0 0 1-.18.242.5.5 0 0 1-.288.092.5.5 0 0 1-.294-.097.5.5 0 0 1-.175-.242l-.211-.644a1.26 1.26 0 0 0-.299-.48 1.14 1.14 0 0 0-.479-.298L.328 4.64a.48.48 0 0 1-.247-.18.515.515 0 0 1 .247-.758l.644-.21a1.28 1.28 0 0 0 .788-.789l.211-.634a.5.5 0 0 1 .165-.242.5.5 0 0 1 .283-.103.5.5 0 0 1 .294.083c.086.058.152.14.19.237l.217.659a1.28 1.28 0 0 0 .788.788l.644.222a.476.476 0 0 1 .237.18.5.5 0 0 1 .092.288\"></path><path fill=\"currentColor\" d=\"M10.031 7.458a.5.5 0 0 1-.098.314.5.5 0 0 1-.267.196l-.881.293c-.272.09-.519.242-.721.443a1.8 1.8 0 0 0-.443.721l-.31.876a.5.5 0 0 1-.185.263.56.56 0 0 1-.319.098.515.515 0 0 1-.515-.366l-.294-.88a1.8 1.8 0 0 0-.443-.722c-.204-.2-.45-.353-.72-.448l-.881-.288a.57.57 0 0 1-.263-.191.56.56 0 0 1-.014-.64.5.5 0 0 1 .271-.194l.886-.294A1.82 1.82 0 0 0 6.01 5.465l.293-.87a.515.515 0 0 1 .49-.377c.11 0 .219.03.314.088a.56.56 0 0 1 .206.263l.298.896a1.82 1.82 0 0 0 1.175 1.174l.875.31a.5.5 0 0 1 .263.195c.07.09.108.2.108.314\"></path><path fill=\"currentColor\" d=\"M7.775 1.684a.5.5 0 0 0 .088-.262.45.45 0 0 0-.088-.263.5.5 0 0 0-.21-.155L7.24.896a.5.5 0 0 1-.165-.103.5.5 0 0 1-.103-.17l-.108-.33a.5.5 0 0 0-.165-.21A.5.5 0 0 0 6.426 0a.5.5 0 0 0-.252.098.5.5 0 0 0-.145.206l-.108.32a.5.5 0 0 1-.103.17.5.5 0 0 1-.17.102L5.334 1a.45.45 0 0 0-.216.155.5.5 0 0 0-.088.262c0 .094.029.186.083.263a.5.5 0 0 0 .216.16l.32.103q.095.03.164.103a.37.37 0 0 1 .103.165l.108.319c.031.09.088.17.165.227a.56.56 0 0 0 .252.077.42.42 0 0 0 .268-.093.5.5 0 0 0 .15-.2l.113-.325a.43.43 0 0 1 .268-.268l.32-.108a.42.42 0 0 0 .215-.155\"></path></svg>\n",
      "\t\t\t\t<input type=\"text\" class=\"hidden\" name=\"daily-papers-search-fake\">\n",
      "\t\t\t\t\n",
      "\t\t\t\t<input type=\"search\" name=\"daily-papers-search\" autocomplete=\"off\" class=\"shadow-alternate-sm max-w-full flex-1 rounded-full border-gray-200 py-2 pl-10 pr-12 placeholder:text-gray-400 focus:ring-2 focus:ring-blue-500/30 dark:bg-gray-850 dark:focus:ring-blue-500 svelte-hhewj3\" placeholder=\"Search any paper with AI\" value=\"\" maxlength=\"250\">\n",
      "\t\t\t\t<button class=\"absolute grid size-7 place-items-center rounded-full border bg-gray-200 transition-all hover:brightness-95 dark:border-gray-700 dark:bg-gray-800 dark:hover:brightness-110 right-2\"><svg class=\"text-sm text-gray-600\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1.1em\" height=\"1.1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M24 9.4L22.6 8L16 14.6L9.4 8L8 9.4l6.6 6.6L8 22.6L9.4 24l6.6-6.6l6.6 6.6l1.4-1.4l-6.6-6.6L24 9.4z\" fill=\"currentColor\"></path></svg></button>\n",
      "\t\t\t\t<button class=\"dice-container absolute right-2 grid !size-7 place-items-center rounded-full border bg-gray-200 transition-all dark:border-gray-700 dark:bg-gray-800 svelte-1bquw\"><div class=\"dice svelte-1bquw\" style=\"transform: rotateX(-25deg)\n",
      "                  rotateY(45deg)\n",
      "                  rotateZ(0deg);\"><div class=\"face front svelte-1bquw\"></div>\n",
      "\t\t<div class=\"face up svelte-1bquw\"></div>\n",
      "\t\t<div class=\"face left svelte-1bquw\"></div>\n",
      "\t\t<div class=\"face right svelte-1bquw\"></div>\n",
      "\t\t<div class=\"face bottom svelte-1bquw\"></div>\n",
      "\t\t<div class=\"face back svelte-1bquw\"></div></div>\n",
      "</button></div></div>\n",
      "\n",
      "\t\t<div class=\"order-2 col-span-6 mt-3 flex w-full justify-center md:col-span-12 md:mt-4 lg:col-span-12 xl:order-none xl:col-span-4 xl:mt-0 xl:items-center xl:justify-end\"><div class=\"flex w-full items-center justify-between w-full xl:w-auto\"><ul class=\"flex gap-1 text-sm  \"><li><button class=\"flex items-center whitespace-nowrap rounded-lg px-2  bg-black text-white dark:bg-gray-800\">Daily\n",
      "\t\t\t\t</button>\n",
      "\t\t</li><li><button class=\"flex items-center whitespace-nowrap rounded-lg px-2  text-gray-500 hover:bg-gray-100 hover:text-gray-700 dark:hover:bg-gray-900 dark:hover:text-gray-300\">Weekly\n",
      "\t\t\t\t</button>\n",
      "\t\t</li><li><button class=\"flex items-center whitespace-nowrap rounded-lg px-2  text-gray-500 hover:bg-gray-100 hover:text-gray-700 dark:hover:bg-gray-900 dark:hover:text-gray-300\">Monthly\n",
      "\t\t\t\t</button>\n",
      "\t\t</li></ul>\n",
      "\n",
      "\t<div class=\"ml-6 flex items-center overflow-hidden rounded-lg bg-gray-100 dark:bg-gray-800\"><a href=\"/papers/date/2025-06-17\" class=\"flex size-8 items-center justify-center bg-gray-200 text-gray-800 hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-200 dark:hover:bg-gray-600 \"><svg class=\"h-2\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" fill=\"currentColor\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 10 10\"><path d=\"M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z\" fill=\"currentColor\"></path></svg></a>\n",
      "\t\t<div class=\"w-24 whitespace-nowrap text-center text-sm font-semibold text-gray-900 dark:text-gray-100\">Jun 18</div>\n",
      "\t\t<a href=\"/papers/date/2025-06-19\" class=\"flex size-8 items-center justify-center bg-gray-200 text-gray-800 hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-200 dark:hover:bg-gray-600 \"><svg class=\"h-2 rotate-180\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" fill=\"currentColor\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 10 10\"><path d=\"M-2.30478e-07 4.95458L7.90909 0.388266L7.90909 9.5209L-2.30478e-07 4.95458Z\" fill=\"currentColor\"></path></svg></a></div></div></div></div>\n",
      "\n",
      "\t<div class=\"relative grid grid-cols-1 gap-5 lg:grid-cols-2 xl:grid-cols-3\"><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14028\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14028.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tXueqing</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14028\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">74</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14028\" class=\"line-clamp-3 cursor-pointer text-balance\">MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n",
      "  for Financial LLM Evaluation</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14028\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"yilunzhao\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"0oJ1mmyo0\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/678ab76d27bb31ad067cbffd/l_5mRG6_BmqmADKs2Kb3W.png\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yueru1\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/76eaad15bf32eba75271f3dc315527c2.svg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"YanAdjeNole\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/d95232cd0c307efab6197ade1a66190b.svg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Xueqing\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t44 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.14028#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t3</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.12928\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12928.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tzhangysk</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.12928\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">45</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.12928\" class=\"line-clamp-3 cursor-pointer text-balance\">Scaling Test-time Compute for LLM Agents</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.12928\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Siwei Wu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Hanhao Li\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"King Zhu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Wangchunshu\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"zhangysk\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t15 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.12928#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t3</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.12285\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12285.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tnicolaus625</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.12285\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">43</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.12285\" class=\"line-clamp-3 cursor-pointer text-balance\">CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n",
      "  Following</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.12285\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Akira Maezawa\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Emmanouil Benetos\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Juntao Yu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Siyou Li\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yinghao Ma\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t5 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.12285#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14429\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14429.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/51b93fea7fd68b4274ee03701245dcca.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tLiuXR</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14429\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">35</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14429\" class=\"line-clamp-3 cursor-pointer text-balance\">LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14429\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Ziwei He\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Qipeng Guo\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zengfeng Huang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zhigeng Liu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"LiuXR\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/51b93fea7fd68b4274ee03701245dcca.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t6 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.14429#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14245\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14245.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/2433104071e4ae1c3e2d755d81d7964b.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tshun-zheng</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14245\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">27</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14245\" class=\"line-clamp-3 cursor-pointer text-balance\">Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n",
      "  Correct Reasoning in Base LLMs</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14245\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"yangwang92\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/604714a0c82d59b7347b55ae/WZFKDDUPi8JS0BK8t7mIv.jpeg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"MasterVito\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"VEWOXIC\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hCmEo__IWO_R_Ps8Q52Os.png\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"shun-zheng\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/2433104071e4ae1c3e2d755d81d7964b.svg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"XumengWen\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/27044caec57d8d68d700208fae78b6c6.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t12 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.14245#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t5</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14234\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14234.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/b065a857dd763410caadea37a2dc01c4.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tmparvez</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14234\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">25</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14234\" class=\"line-clamp-3 cursor-pointer text-balance\">Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n",
      "  Like an Olympiad Team</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14234\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Md Rizwan Parvez\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Md Kishor Morol\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Salman Rahman\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Md Tanzib Hosain\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t4 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.14234#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13363\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><video src=\"https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt\" loop muted playsinline preload=\"metadata\" class=\"pointer-events-none h-full w-full object-cover object-center\"></video></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tzhaocheng</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13363\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">24</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13363\" class=\"line-clamp-3 cursor-pointer text-balance\">Efficient Medical VIE via Reinforcement Learning</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13363\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Chong Li\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Chenglin Zhu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Lijun Liu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"zhaocheng\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"lryyyy\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/866699ffee8a1d7cf2c9bebe0d3d58fe.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t8 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.13363#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13642\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><video src=\"https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4\" loop muted playsinline preload=\"metadata\" class=\"pointer-events-none h-full w-full object-cover object-center\"></video></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tzhangshaolei</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13642\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">21</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13642\" class=\"line-clamp-3 cursor-pointer text-balance\">Stream-Omni: Simultaneous Multimodal Interactions with Large\n",
      "  Language-Vision-Speech Model</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13642\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yang Feng\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yan Zhou\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Qingkai Fang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Shoutao Guo\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Shaolei Zhang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t5 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.13642#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14758\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14758.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tdaixuancheng</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14758\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">19</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14758\" class=\"line-clamp-3 cursor-pointer text-balance\">Reasoning with Exploration: An Entropy Perspective</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14758\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Wayne Xin Zhao\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Bo Dai\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Xuekai Zhu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Shaohan Huang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"daixuancheng\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t7 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.14758#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t4</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.09985\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09985.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/1647023505150-622b93067b9143726fbedc37.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tkoustuvs</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.09985\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">16</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.09985\" class=\"line-clamp-3 cursor-pointer text-balance\">V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n",
      "  and Planning</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.09985\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Russell Howes\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Quentin Garrido\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"David Fan\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Adrien Bardes\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Mido Assran\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t30 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.09985#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.12278\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12278.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tyilunzhao</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.12278\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">15</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.12278\" class=\"line-clamp-3 cursor-pointer text-balance\">Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n",
      "  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.12278\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Xue Xia\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zexi Kuang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zheyuan Yang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"yilunzhao\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t4 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.12278#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14603\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14603.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tamsabour</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14603\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">14</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14603\" class=\"line-clamp-3 cursor-pointer text-balance\">Align Your Flow: Scaling Continuous-Time Flow Map Distillation</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14603\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Karsten Kreis\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Sanja Fidler\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Amirmojtaba Sabour\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t3 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.14603#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t4</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.12860\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12860.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/a0d875b49d1c56be88f34854647306da.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tlwl-uestc</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.12860\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">13</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.12860\" class=\"line-clamp-3 cursor-pointer text-balance\">QFFT, Question-Free Fine-Tuning for Adaptive Reasoning</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.12860\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Ke Ji\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yukang Lin\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Fei Yu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Junxiao Xu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"lwl-uestc\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/a0d875b49d1c56be88f34854647306da.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t10 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.12860#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14606\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14606.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tahmedheakl</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14606\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">10</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14606\" class=\"line-clamp-3 cursor-pointer text-balance\">Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n",
      "  Transpilation with Testing Guarantees</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14606\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Abdulrahman Mahmoud\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Celine Lee\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Chaimaa Abi\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Sarim-Hash\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/0ead41b44957eb30564ea685ed22781a.svg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"ahmedheakl\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t5 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.14606#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.10100\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10100.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/b68880022e14556d0be58c69615db3be.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tzichenwen</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.10100\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">8</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.10100\" class=\"line-clamp-3 cursor-pointer text-balance\">EfficientVLA: Training-Free Acceleration and Compression for\n",
      "  Vision-Language-Action Models</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.10100\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Chang Zou\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Luo Zhongwei\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zichen Wen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yuhao Wang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yantai Yang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t8 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.10100#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13977\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13977.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tCostaliyA</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13977\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">8</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13977\" class=\"line-clamp-3 cursor-pointer text-balance\">CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n",
      "  Models in Tool-Calling Error Scenarios</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13977\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Junjie Ye\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Siyu Yuan\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zehui Chen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Shiting Huang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"CostaliyA\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t9 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.13977#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14755\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14755.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tzhoutianyi</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14755\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">6</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14755\" class=\"line-clamp-3 cursor-pointer text-balance\">Optimizing Length Compression in Large Reasoning Models</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14755\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Mingyang Fu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Dongping Chen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"zhoutianyi\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"zx10086\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/5dd096cb7360682016d0fca909ab9744.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t4 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.14755#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13651\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13651.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/bbf781594fc8c812316711aa8e2797aa.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tLiuff23</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13651\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">6</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13651\" class=\"line-clamp-3 cursor-pointer text-balance\">xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n",
      "  Real-World Evaluations</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13651\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Xiaobo Hu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yang Liu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yixin Ren\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Kaiyuan Chen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Liuff23\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/bbf781594fc8c812316711aa8e2797aa.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t33 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.13651#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.10038\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10038.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tgiannisdaras</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.10038\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">6</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.10038\" class=\"line-clamp-3 cursor-pointer text-balance\">Ambient Diffusion Omni: Training Good Models with Bad Data</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.10038\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Constantinos Daskalakis\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Antonio Torralba\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Adam Klivans\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Giannis Daras\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"adrianrm\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/8a9ac73d93785f48e63184d612b9fff1.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t5 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.10038#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14002\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14002.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png\" crossorigin=\"anonymous\">\n",
      "\t\t\tSiyuc</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14002\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">5</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14002\" class=\"line-clamp-3 cursor-pointer text-balance\">Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n",
      "  Autoencoders</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14002\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zhuoran Yang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Tianhao Wang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Xuyuan Xiong\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Heejune Sheen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Siyu Chen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t5 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.14002#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.05336\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><video src=\"https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4\" loop muted playsinline preload=\"metadata\" class=\"pointer-events-none h-full w-full object-cover object-center\"></video></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tahmedheakl</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.05336\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">5</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.05336\" class=\"line-clamp-3 cursor-pointer text-balance\">VideoMolmo: Spatio-Temporal Grounding Meets Pointing</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.05336\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zhiqiang Shen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Abdelrahman Shaker\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Hanan Gani\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Ghazi Shazan Ahmad\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"ahmedheakl\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t8 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.05336#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t6</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14761\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14761.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/94b5810ec4a167d9e2b523c0626a23b4.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tcetosignis</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14761\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">4</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14761\" class=\"line-clamp-3 cursor-pointer text-balance\">From Bytes to Ideas: Language Modeling with Autoregressive U-Nets</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14761\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Olivier Teytaud\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Marc Schoenauer\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Alessandro Leite\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Badr Youbi Idrissi\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Mathurin Videau\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t6 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.14761#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.09033\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09033.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/88be16ee80da7d2eaa0feae878375001.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tXaiverZ</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.09033\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">3</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.09033\" class=\"line-clamp-3 cursor-pointer text-balance\">Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n",
      "  Reinforcement Learning</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.09033\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Jiaxuan You\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Tao Feng\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Haozhen Zhang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t3 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.09033#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14731\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14731.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\takhaliq</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14731\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">2</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14731\" class=\"line-clamp-3 cursor-pointer text-balance\">Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n",
      "  for LLMs</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14731\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Ding Liu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Deng Zhao\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Cai Chen\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Bin Hu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Ring Team\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t46 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.14731#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14702\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14702.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tdsouzadaniel</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14702\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">2</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14702\" class=\"line-clamp-3 cursor-pointer text-balance\">Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n",
      "  Markers</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14702\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Sara Hooker\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Ahmet Üstün\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Adrien Morisot\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Julia Kreutzer\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Daniel D'souza\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t5 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.14702#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13599\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13599.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\tJJ-TMT</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13599\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">2</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13599\" class=\"line-clamp-3 cursor-pointer text-balance\">CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n",
      "  Simulation</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13599\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yong Li\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Jian Yuan\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yuwei Du\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"JJ-TMT\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t4 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.13599#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.12880\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12880.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tMatanBT</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.12880\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">2</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.12880\" class=\"line-clamp-3 cursor-pointer text-balance\">Universal Jailbreak Suffixes Are Strong Attention Hijackers</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.12880\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Mahmood Sharif\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"mega\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/1628140189042-noauth.jpeg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"MatanBT\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t3 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.12880#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.05426\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05426.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tWenhao0</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.05426\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">2</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.05426\" class=\"line-clamp-3 cursor-pointer text-balance\">Mixture-of-Experts Meets In-Context Reinforcement Learning</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.05426\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Daoyi Dong\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Zican Hu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Haoru Li\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Fuhong Liu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Wenhao0\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t7 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.05426#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14205\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14205.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png\" crossorigin=\"anonymous\">\n",
      "\t\t\tXuandong</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14205\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">1</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14205\" class=\"line-clamp-3 cursor-pointer text-balance\">AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14205\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Dawn Song\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Xuandong Zhao\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Dylan Xu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Jingxu Xie\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t4 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.14205#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13901\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13901.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tamanchadha</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13901\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">1</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13901\" class=\"line-clamp-3 cursor-pointer text-balance\">Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n",
      "  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n",
      "  Pooled Representations</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13901\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Utkarsh Bhatt\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Danush Khanna\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Chhavi Sharma\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Abhilekh Borah\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"amanchadha\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t15 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.13901#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13387\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13387.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png\" crossorigin=\"anonymous\">\n",
      "\t\t\tBeileiCui</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13387\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">1</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13387\" class=\"line-clamp-3 cursor-pointer text-balance\">TR2M: Transferring Monocular Relative Depth to Metric Depth with\n",
      "  Language Descriptions and Scale-Oriented Contrast</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13387\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Hongliang Ren\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Long Bai\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yiming Huang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Beilei Cui\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t4 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.13387#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.14629\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14629.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tFaiyazAbdullah114708</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.14629\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">-</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.14629\" class=\"line-clamp-3 cursor-pointer text-balance\">VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n",
      "  Mosquito Breeding Site Detection and Reasoning</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.14629\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Shahanur Rahman Bappy\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Muhammad Ziaur Rahman\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Md. Asaduzzaman Shuvo\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"FaiyazAbdullah114708\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Adnanul\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aw0TJjO3ohRlEYWlCzxQY.png\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t7 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.14629#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.13922\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><video src=\"https://cdn-uploads.huggingface.co/production/uploads/64e153cf75fae2212e8a140c/fCFLL8CQmmy_9LeN0YPhG.mp4\" loop muted playsinline preload=\"metadata\" class=\"pointer-events-none h-full w-full object-cover object-center\"></video></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg\" crossorigin=\"anonymous\">\n",
      "\t\t\tMaxDu</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.13922\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">-</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.13922\" class=\"line-clamp-3 cursor-pointer text-balance\">DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.13922\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Shuran Song\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"MaxDu\" style=\"content-visibility:auto;\"><img class=\"overflow-hidden rounded-full\" alt=\"\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t2 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t\n",
      "\n",
      "<span class=\"inline-block \"><span class=\"contents\"><a slot=\"anchor\" href=\"/papers/2506.13922#community\" class=\"ml-2 flex translate-y-px items-center gap-1 rounded-md border border-blue-200 bg-blue-600/10 px-1 text-sm text-blue-500 dark:border-blue-800/60 dark:bg-blue-800/20\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t\t2</a></span>\n",
      "\t</span></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.12015\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12015.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"/avatars/1e9643721c152f9999b6f35ba117a0d6.svg\" crossorigin=\"anonymous\">\n",
      "\t\t\thsichelin</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.12015\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">-</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.12015\" class=\"line-clamp-3 cursor-pointer text-balance\">EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.12015\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yu-Chiang Frank Wang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Kai-Po Chang\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yu-Chu Yu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Hsi-Che Lin\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t4 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.12015#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article><article class=\"relative flex flex-col overflow-hidden rounded-xl border\"><a href=\"/papers/2506.03939\" class=\"shadow-alternate-sm peer relative block h-56 w-full cursor-pointer overflow-hidden rounded-xl bg-white md:h-64\"><img src=\"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03939.png\" loading=\"lazy\" decoding=\"async\" alt=\"\" class=\"h-full w-full object-cover object-top opacity-80 dark:opacity-70 dark:invert\"></a>\n",
      "\n",
      "\t<div class=\"shadow-xs pointer-events-none absolute right-2 top-56 -mt-8 flex h-6 items-center gap-1 self-end whitespace-nowrap rounded-md border bg-white px-2 text-xs leading-none text-gray-700 dark:bg-gray-900 dark:text-gray-400 sm:text-sm md:top-64\">Submitted by\n",
      "\t\t<img alt=\"\" loading=\"lazy\" class=\"size-2.5 rounded-full  flex-none\" src=\"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png\" crossorigin=\"anonymous\">\n",
      "\t\t\tChetKao</div>\n",
      "\n",
      "\t\n",
      "\n",
      "\t<div class=\"from-gray-50-to-white bg-linear-to-b -mt-2 flex px-6 pb-6 pt-8\"><div class=\"flex w-full gap-6\"><div class=\"flex flex-wrap items-center gap-2.5 pt-1  z-1 lg:sticky lg:top-8\"><a href=\"/login?next=%2Fpapers%2F2506.03939\" class=\"self-start\">\n",
      "\n",
      "<div class=\"shadow-alternate flex h-14 w-12 gap-1 rounded-xl flex-none cursor-pointer select-none flex-col items-center justify-center self-start border-gray-300 bg-white dark:bg-gray-850\"><input disabled type=\"checkbox\"  class=\"peer hidden\">\n",
      "\t\t<svg class=\"text-sm peer-checked:text-gray-500 group-hover:text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 12 12\"><path fill=\"currentColor\" d=\"M5.19 2.67a.94.94 0 0 1 1.62 0l3.31 5.72a.94.94 0 0 1-.82 1.4H2.7a.94.94 0 0 1-.82-1.4l3.31-5.7v-.02Z\"></path></svg>\n",
      "\t\t<div class=\"leading-none\">-</div></div></a>\n",
      "\t</div>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t<div class=\"w-full\"><h3 class=\"mb-1 text-lg/6 font-semibold hover:underline peer-hover:underline 2xl:text-[1.2rem]/6\"><a href=\"/papers/2506.03939\" class=\"line-clamp-3 cursor-pointer text-balance\">Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning</a></h3>\n",
      "\t\t\t\t<div class=\"flex items-center justify-between\"><a href=\"/papers/2506.03939\" class=\"flex\"><ul class=\"flex items-center  flex-row-reverse   text-sm  \"><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Yichen Niu\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Dong Li\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"YIng Ai\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Xiang Zou\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li><li class=\"  -mr-2 h-4 w-4 md:h-5 md:w-5  bg-linear-to-br block flex-none rounded-full border-2 border-white from-gray-300 to-gray-100 dark:border-gray-900 dark:from-gray-600 dark:to-gray-800\" title=\"Junqi Gao\" style=\"content-visibility:auto;\">\n",
      "\t\t\t</li>\n",
      "\n",
      "\t\t<li class=\"text-gray-600 hover:text-gray-700 order-first ml-3\"><div class=\"flex truncate text-base text-gray-350\"><div class=\"ml-1 mr-2.5\">·</div>\n",
      "\t\t\t\t\t\t\t\t7 authors\n",
      "\t\t\t\t\t\t\t</div></li></ul></a>\n",
      "\t\t\t\t\t<a href=\"/papers/2506.03939#community\" class=\"ml-2 flex translate-y-px items-center gap-1 text-sm text-gray-400\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 24 24\"><path fill=\"none\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M21 15a2 2 0 0 1-2 2H7l-4 4V5a2 2 0 0 1 2-2h14a2 2 0 0 1 2 2z\"></path></svg>\n",
      "\t\t\t\t\t\t\t2</a></div></div></div></div></article>\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t<div class=\"col-span-1 mt-8 flex lg:col-span-2 xl:col-span-3\"><a class=\"btn gap-2\" href=\"/papers/date/2025-06-17\"><svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M14 26l1.41-1.41L7.83 17H28v-2H7.83l7.58-7.59L14 6L4 16l10 10z\" fill=\"currentColor\"></path></svg>Previous\n",
      "\t\t\t\t</a>\n",
      "\t\t\t<a class=\"btn ml-auto gap-2\" href=\"/papers/date/2025-06-19\">Next<svg class=\"\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" aria-hidden=\"true\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M18 6l-1.4 1.4l7.5 7.6H3v2h21.1l-7.5 7.6L18 26l10-10z\" fill=\"currentColor\"></path></svg></a></div></div>\n",
      "</section></div></main>\n",
      "\n",
      "\t<footer class=\"b-12 mb-2 flex border-t border-gray-100 md:h-14\"><nav class=\"container relative flex flex-col justify-between space-y-2 py-6 text-gray-500 max-md:*:self-start md:flex-row md:items-center md:space-y-0 md:py-0 md:text-sm\"><div class=\"SVELTE_HYDRATER contents\" data-target=\"ThemeSwitcher\" data-props=\"{&quot;theme&quot;:&quot;system&quot;,&quot;isLoggedIn&quot;:false,&quot;menuClassNames&quot;:&quot;md:-top-24&quot;,&quot;classNames&quot;:&quot;max-md:mb-5 max-md:*:self-start&quot;}\">\n",
      "<div class=\"relative inline-block max-md:mb-5 max-md:*:self-start\">\n",
      "\t<button class=\"rounded-full border border-gray-100 pl-2 py-1 pr-2.5  flex items-center text-sm text-gray-500 bg-white hover:bg-purple-50 hover:border-purple-200 dark:hover:bg-gray-800 dark:hover:border-gray-950 dark:border-gray-800 \" type=\"button\">\n",
      "\t\t<svg class=\"mr-1.5 text-gray-500\" xmlns=\"http://www.w3.org/2000/svg\" aria-hidden=\"true\" fill=\"currentColor\" focusable=\"false\" role=\"img\" width=\"1em\" height=\"1em\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 32 32\"><path d=\"M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z\" fill=\"currentColor\"></path><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M6 22.5h20a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2H6a2 2 0 0 0-2 2v13.5a2 2 0 0 0 2 2ZM7 7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h18a1 1 0 0 0 1-1V8a1 1 0 0 0-1-1H7Z\" fill=\"currentColor\"></path><path d=\"M6 8a1 1 0 0 1 1-1h18a1 1 0 0 1 1 1v11a1 1 0 0 1-1 1H7a1 1 0 0 1-1-1V8Z\" fill=\"currentColor\" fill-opacity=\".4\"></path><path d=\"M29 25H3a1 1 0 1 0 0 2h26a1 1 0 1 0 0-2Z\" fill=\"currentColor\"></path></svg>\n",
      "\t\t\tSystem theme\n",
      "\t\t</button>\n",
      "\t\n",
      "\t\n",
      "\t</div></div>\n",
      "\t\t<div class=\"font-semibold text-black md:hidden\">Company</div>\n",
      "\t\t<a class=\"hover:underline\" href=\"/terms-of-service\">TOS</a>\n",
      "\t\t<a class=\"hover:underline\" href=\"/privacy\">Privacy</a>\n",
      "\t\t<a class=\"hover:underline\" href=\"/huggingface\">About</a>\n",
      "\t\t<a class=\"hover:underline\" href=\"https://apply.workable.com/huggingface/\">Jobs</a>\n",
      "\t\t<a href=\"/\" class=\"max-md:mb-4! max-md:mt-8! group flex-none max-md:order-last\"><svg class=\"h-7 w-7 transition-transform group-hover:-translate-y-px\" viewBox=\"0 0 95 88\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5Z\" fill=\"#FFD21E\"></path><path d=\"M81.9619 41.75C81.9619 22.5581 66.4037 7 47.2119 7C28.02 7 12.4619 22.5581 12.4619 41.75C12.4619 60.9419 28.02 76.5 47.2119 76.5C66.4037 76.5 81.9619 60.9419 81.9619 41.75ZM8.46185 41.75C8.46185 20.349 25.8108 3 47.2119 3C68.6129 3 85.9619 20.349 85.9619 41.75C85.9619 63.151 68.6129 80.5 47.2119 80.5C25.8108 80.5 8.46185 63.151 8.46185 41.75Z\" fill=\"#FF9D0B\"></path><path d=\"M58.5024 32.2915C59.7768 32.7415 60.2839 35.3615 61.5713 34.6769C64.0095 33.3805 64.9351 30.353 63.6387 27.9148C62.3423 25.4767 59.3148 24.5511 56.8766 25.8475C54.4384 27.1439 53.5128 30.1714 54.8092 32.6096C55.4211 33.7604 57.3632 31.8892 58.5024 32.2915Z\" fill=\"#3A3B45\"></path><path d=\"M34.9454 32.2915C33.671 32.7415 33.164 35.3615 31.8766 34.6769C29.4384 33.3805 28.5128 30.353 29.8092 27.9148C31.1056 25.4767 34.1331 24.5511 36.5713 25.8475C39.0095 27.1439 39.9351 30.1714 38.6387 32.6096C38.0268 33.7604 36.0846 31.8892 34.9454 32.2915Z\" fill=\"#3A3B45\"></path><path d=\"M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z\" fill=\"#3A3B45\"></path><mask id=\"mask0\" style=\"mask-type:alpha\" maskUnits=\"userSpaceOnUse\" x=\"33\" y=\"41\" width=\"27\" height=\"16\"><path d=\"M46.9619 56.289C56.7903 56.289 59.9619 47.5261 59.9619 43.0262C59.9619 40.6875 58.3898 41.4236 55.8718 42.6702C53.5449 43.8222 50.4102 45.4101 46.9619 45.4101C39.7822 45.4101 33.9619 38.5263 33.9619 43.0262C33.9619 47.5261 37.1334 56.289 46.9619 56.289Z\" fill=\"white\"></path></mask><g mask=\"url(#mask0)\"><path d=\"M47.2119 66.5C52.0018 66.5 55.8848 62.617 55.8848 57.8271C55.8848 54.0962 53.5291 50.9156 50.224 49.6915C50.1023 49.6464 49.9794 49.604 49.8553 49.5643C49.0219 49.2979 48.1337 52.1623 47.2119 52.1623C46.3506 52.1623 45.5186 49.2797 44.7332 49.5135C41.151 50.5799 38.5389 53.8984 38.5389 57.8271C38.5389 62.617 42.4219 66.5 47.2119 66.5Z\" fill=\"#F94040\"></path></g><path d=\"M70.7119 37C72.5068 37 73.9619 35.5449 73.9619 33.75C73.9619 31.9551 72.5068 30.5 70.7119 30.5C68.9169 30.5 67.4619 31.9551 67.4619 33.75C67.4619 35.5449 68.9169 37 70.7119 37Z\" fill=\"#FF9D0B\"></path><path d=\"M24.2119 37C26.0068 37 27.4619 35.5449 27.4619 33.75C27.4619 31.9551 26.0068 30.5 24.2119 30.5C22.4169 30.5 20.9619 31.9551 20.9619 33.75C20.9619 35.5449 22.4169 37 24.2119 37Z\" fill=\"#FF9D0B\"></path><path class=\"origin-bottom-right transition-transform group-hover:-rotate-6\" d=\"M17.5238 48C15.9048 48 14.4578 48.665 13.4488 49.871C12.8248 50.618 12.1728 51.822 12.1198 53.625C11.4408 53.43 10.7878 53.321 10.1778 53.321C8.6278 53.321 7.2278 53.915 6.2378 54.994C4.9658 56.379 4.4008 58.081 4.6468 59.784C4.7638 60.595 5.0348 61.322 5.4398 61.995C4.5858 62.686 3.9568 63.648 3.6528 64.805C3.4148 65.712 3.1708 67.601 4.4448 69.547C4.3638 69.674 4.2878 69.806 4.2168 69.941C3.4508 71.395 3.4018 73.038 4.0778 74.568C5.1028 76.887 7.6498 78.714 12.5958 80.675C15.6728 81.895 18.4878 82.675 18.5128 82.682C22.5808 83.737 26.2598 84.273 29.4448 84.273C35.2988 84.273 39.4898 82.48 41.9018 78.944C45.7838 73.25 45.2288 68.042 40.2058 63.022C37.4258 60.244 35.5778 56.148 35.1928 55.249C34.4168 52.587 32.3648 49.628 28.9538 49.628H28.9528C28.6658 49.628 28.3758 49.651 28.0898 49.696C26.5958 49.931 25.2898 50.791 24.3568 52.085C23.3498 50.833 22.3718 49.837 21.4868 49.275C20.1528 48.429 18.8198 48 17.5238 48ZM17.5238 52C18.0338 52 18.6568 52.217 19.3438 52.653C21.4768 54.006 25.5928 61.081 27.0998 63.833C27.6048 64.755 28.4678 65.145 29.2448 65.145C30.7868 65.145 31.9908 63.612 29.3858 61.664C25.4688 58.733 26.8428 53.942 28.7128 53.647C28.7948 53.634 28.8758 53.628 28.9538 53.628C30.6538 53.628 31.4038 56.558 31.4038 56.558C31.4038 56.558 33.6018 62.078 37.3778 65.851C41.1538 69.625 41.3488 72.654 38.5968 76.69C36.7198 79.442 33.1268 80.273 29.4448 80.273C25.6258 80.273 21.7108 79.379 19.5168 78.81C19.4088 78.782 6.0658 75.013 7.7558 71.805C8.0398 71.266 8.5078 71.05 9.0968 71.05C11.4768 71.05 15.8058 74.592 17.6668 74.592C18.0828 74.592 18.3758 74.415 18.4958 73.983C19.2888 71.138 6.4388 69.942 7.5218 65.821C7.7128 65.092 8.2308 64.796 8.9588 64.797C12.1038 64.797 19.1598 70.328 20.6388 70.328C20.7518 70.328 20.8328 70.295 20.8768 70.225C21.6178 69.029 21.2118 68.194 15.9888 65.033C10.7658 61.871 7.0998 59.969 9.1848 57.699C9.4248 57.437 9.7648 57.321 10.1778 57.321C13.3488 57.322 20.8408 64.14 20.8408 64.14C20.8408 64.14 22.8628 66.243 24.0858 66.243C24.3668 66.243 24.6058 66.132 24.7678 65.858C25.6348 64.396 16.7148 57.636 16.2118 54.847C15.8708 52.957 16.4508 52 17.5238 52Z\" fill=\"#FF9D0B\"></path><path class=\"origin-bottom-right transition-transform group-hover:-rotate-6\" d=\"M38.5967 76.6898C41.3487 72.6538 41.1537 69.6248 37.3777 65.8508C33.6017 62.0778 31.4037 56.5578 31.4037 56.5578C31.4037 56.5578 30.5827 53.3518 28.7127 53.6468C26.8427 53.9418 25.4697 58.7328 29.3867 61.6638C33.3037 64.5938 28.6067 66.5848 27.0997 63.8328C25.5927 61.0808 21.4777 54.0058 19.3437 52.6528C17.2107 51.2998 15.7087 52.0578 16.2117 54.8468C16.7147 57.6358 25.6357 64.3958 24.7677 65.8588C23.8997 67.3208 20.8407 64.1398 20.8407 64.1398C20.8407 64.1398 11.2687 55.4288 9.18465 57.6988C7.10065 59.9688 10.7657 61.8708 15.9887 65.0328C21.2127 68.1938 21.6177 69.0288 20.8767 70.2248C20.1347 71.4208 8.60465 61.6998 7.52165 65.8208C6.43965 69.9418 19.2887 71.1378 18.4957 73.9828C17.7027 76.8288 9.44465 68.5978 7.75565 71.8048C6.06565 75.0128 19.4087 78.7818 19.5167 78.8098C23.8267 79.9278 34.7727 82.2968 38.5967 76.6898Z\" fill=\"#FFD21E\"></path><path class=\"origin-bottom-left transition-transform group-hover:rotate-6\" d=\"M77.3999 48C79.0189 48 80.4659 48.665 81.4749 49.871C82.0989 50.618 82.7509 51.822 82.8039 53.625C83.4829 53.43 84.1359 53.321 84.7459 53.321C86.2959 53.321 87.6959 53.915 88.6859 54.994C89.9579 56.379 90.5229 58.081 90.2769 59.784C90.1599 60.595 89.8889 61.322 89.4839 61.995C90.3379 62.686 90.9669 63.648 91.2709 64.805C91.5089 65.712 91.7529 67.601 90.4789 69.547C90.5599 69.674 90.6359 69.806 90.7069 69.941C91.4729 71.395 91.5219 73.038 90.8459 74.568C89.8209 76.887 87.2739 78.714 82.3279 80.675C79.2509 81.895 76.4359 82.675 76.4109 82.682C72.3429 83.737 68.6639 84.273 65.4789 84.273C59.6249 84.273 55.4339 82.48 53.0219 78.944C49.1399 73.25 49.6949 68.042 54.7179 63.022C57.4979 60.244 59.3459 56.148 59.7309 55.249C60.5069 52.587 62.5589 49.628 65.9699 49.628H65.9709C66.2579 49.628 66.5479 49.651 66.8339 49.696C68.3279 49.931 69.6339 50.791 70.5669 52.085C71.5739 50.833 72.5519 49.837 73.4369 49.275C74.7709 48.429 76.1039 48 77.3999 48ZM77.3999 52C76.8899 52 76.2669 52.217 75.5799 52.653C73.4469 54.006 69.3309 61.081 67.8239 63.833C67.3189 64.755 66.4559 65.145 65.6789 65.145C64.1369 65.145 62.9329 63.612 65.5379 61.664C69.4549 58.733 68.0809 53.942 66.2109 53.647C66.1289 53.634 66.0479 53.628 65.9699 53.628C64.2699 53.628 63.5199 56.558 63.5199 56.558C63.5199 56.558 61.3219 62.078 57.5459 65.851C53.7699 69.625 53.5749 72.654 56.3269 76.69C58.2039 79.442 61.7969 80.273 65.4789 80.273C69.2979 80.273 73.2129 79.379 75.4069 78.81C75.5149 78.782 88.8579 75.013 87.1679 71.805C86.8839 71.266 86.4159 71.05 85.8269 71.05C83.4469 71.05 79.1179 74.592 77.2569 74.592C76.8409 74.592 76.5479 74.415 76.4279 73.983C75.6349 71.138 88.4849 69.942 87.4019 65.821C87.2109 65.092 86.6929 64.796 85.9649 64.797C82.8199 64.797 75.7639 70.328 74.2849 70.328C74.1719 70.328 74.0909 70.295 74.0469 70.225C73.3059 69.029 73.7119 68.194 78.9349 65.033C84.1579 61.871 87.8239 59.969 85.7389 57.699C85.4989 57.437 85.1589 57.321 84.7459 57.321C81.5749 57.322 74.0829 64.14 74.0829 64.14C74.0829 64.14 72.0609 66.243 70.8379 66.243C70.5569 66.243 70.3179 66.132 70.1559 65.858C69.2889 64.396 78.2089 57.636 78.7119 54.847C79.0529 52.957 78.4729 52 77.3999 52Z\" fill=\"#FF9D0B\"></path><path class=\"origin-bottom-left transition-transform group-hover:rotate-6\" d=\"M56.3271 76.6898C53.5751 72.6538 53.7701 69.6248 57.5461 65.8508C61.3221 62.0778 63.5201 56.5578 63.5201 56.5578C63.5201 56.5578 64.3411 53.3518 66.2111 53.6468C68.0811 53.9418 69.4541 58.7328 65.5371 61.6638C61.6201 64.5938 66.3171 66.5848 67.8241 63.8328C69.3311 61.0808 73.4461 54.0058 75.5801 52.6528C77.7131 51.2998 79.2151 52.0578 78.7121 54.8468C78.2091 57.6358 69.2881 64.3958 70.1561 65.8588C71.0241 67.3208 74.0831 64.1398 74.0831 64.1398C74.0831 64.1398 83.6551 55.4288 85.7391 57.6988C87.8231 59.9688 84.1581 61.8708 78.9351 65.0328C73.7111 68.1938 73.3061 69.0288 74.0471 70.2248C74.7891 71.4208 86.3191 61.6998 87.4021 65.8208C88.4841 69.9418 75.6351 71.1378 76.4281 73.9828C77.2211 76.8288 85.4791 68.5978 87.1681 71.8048C88.8581 75.0128 75.5151 78.7818 75.4071 78.8098C71.0971 79.9278 60.1511 82.2968 56.3271 76.6898Z\" fill=\"#FFD21E\"></path></svg></a>\n",
      "\t\t<div class=\"max-md:mt-8! font-semibold text-black md:hidden\">Website</div>\n",
      "\n",
      "\t\t<a class=\"hover:underline\" href=\"/models\">Models</a>\n",
      "\t\t<a class=\"hover:underline\" href=\"/datasets\">Datasets</a>\n",
      "\t\t<a class=\"hover:underline\" href=\"/spaces\">Spaces</a>\n",
      "\t\t<a class=\"hover:underline\" href=\"/pricing\">Pricing</a>\n",
      "\t\t<a class=\"hover:underline\" href=\"/docs\">Docs</a></nav></footer></div>\n",
      "\n",
      "\t\t<script>\n",
      "\t\t\timport(\"\\/front\\/build\\/kube-3562a0d\\/index.js\");\n",
      "\t\t\twindow.moonSha = \"kube-3562a0d\\/\";\n",
      "\t\t\twindow.__hf_deferred = {};\n",
      "\t\t</script>\n",
      "\n",
      "\t\t<!-- Stripe -->\n",
      "\t\t<script>\n",
      "\t\t\tif ([\"hf.co\", \"huggingface.co\"].includes(window.location.hostname)) {\n",
      "\t\t\t\tconst script = document.createElement(\"script\");\n",
      "\t\t\t\tscript.src = \"https://js.stripe.com/v3/\";\n",
      "\t\t\t\tscript.async = true;\n",
      "\t\t\t\tdocument.head.appendChild(script);\n",
      "\t\t\t}\n",
      "\t\t</script>\n",
      "\t</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/papers/date/2025-06-18\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"请求成功！\")\n",
    "    print(response.text)  # 打印网页 HTML 内容\n",
    "else:\n",
    "    print(f\"请求失败，状态码：{response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb7da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2506.15925v1  [cs.CL]  19 Jun 2025Reranking-based Generation for Unbiased Perspective Summarization\n",
      "Narutatsu Ri and Nicholas Deas and Kathleen McKeown\n",
      "Department of Computer Science, Columbia University\n",
      "{wl2787, nid2107, km}@columbia.edu\n",
      "Abstract\n",
      "Generating unbiased summaries in real-world\n",
      "settings such as political perspective summa-\n",
      "rization remains a crucial application of Large\n",
      "Language Models (LLMs). Yet, existing evalu-\n",
      "ation frameworks rely on traditional metrics for\n",
      "measuring key attributes such as coverage and\n",
      "faithfulness without verifying their applicabil-\n",
      "ity, and efforts to develop improved summariz-\n",
      "ers are still nascent. We address these gaps by\n",
      "(1) identifying reliable metrics for measuring\n",
      "perspective summary quality, and (2) investigat-\n",
      "ing the efficacy of LLM-based methods beyond\n",
      "zero-shot inference. Namely, we build a test\n",
      "set for benchmarking metric reliability using\n",
      "human annotations and show that traditional\n",
      "metrics underperform compared to language\n",
      "model–based metrics, which prove to be strong\n",
      "evaluators. Using these metrics, we show that\n",
      "reranking-based methods yield strong results,\n",
      "and preference tuning with synthetically gener-\n",
      "ated and reranking-labeled data further boosts\n",
      "performance. Our findings aim to contribute\n",
      "to the reliable evaluation and development of\n",
      "perspective summarization methods.\n",
      "1 Introduction\n",
      "Article summarization is a key application of Large\n",
      "Language Models (LLMs) given their recent break-\n",
      "throughs in text generation capabilities (Goyal\n",
      "et al., 2023; Zhang et al., 2024a). Critically, how-\n",
      "ever, LLMs often exhibit undesirable behaviors and\n",
      "input-level biases toward spurious features (e.g.,\n",
      "position) (Jung et al., 2019; Chhabra et al., 2024;\n",
      "Liu et al., 2024a), resulting in unbalanced input\n",
      "coverage (Zhang et al., 2024c) and hallucination\n",
      "(Maynez et al., 2020). These issues are especially\n",
      "problematic in opinionated article summarization\n",
      "(Amplayo et al., 2021; Iso et al., 2022), where unbi-\n",
      "ased representation of diverse viewpoints is crucial.\n",
      "Recent studies in opinion summarization ad-\n",
      "dress these risks by developing tasks and meth-\n",
      "ods that generate summaries free of framing bias\n",
      "(Lee et al., 2022a), fairly represent input diversity(Zhang et al., 2024c; Feng et al., 2024), or pre-\n",
      "serve the source perspectives (Lei et al., 2024; Liu\n",
      "et al., 2024b). Within this domain, perspective sum-\n",
      "marization (Deas and McKeown, 2025) serves as\n",
      "a representative evaluation setting, where models\n",
      "are tasked to generate precise, perspective-specific\n",
      "summaries from multi-document inputs containing\n",
      "diverse political views. However, two gaps remain\n",
      "unaddressed in this setting: (1) existing evaluation\n",
      "metrics are primarily derived from news summa-\n",
      "rization domains and have not been validated for\n",
      "measuring perspective summary quality, and (2) the\n",
      "effectiveness of LLM-based methods beyond zero-\n",
      "shot inference in generating unbiased, high-quality\n",
      "perspective summaries remains underexplored.\n",
      "To address these gaps, we first identify effective\n",
      "metrics for measuring summary quality by con-\n",
      "structing a test set to evaluate existing metrics. We\n",
      "focus on two key attributes that a desirable sum-\n",
      "mary should have: perspective coverage —the ex-\n",
      "tent to which the summary includes all key con-\n",
      "tent from the intended perspective, and perspec-\n",
      "tive faithfulness —the degree to which the sum-\n",
      "mary excludes content unsupported by the source\n",
      "articles of the target perspective. We collect key\n",
      "point annotations from articles to create controlled\n",
      "summaries with varied key point selections and\n",
      "assigned ground truth scores. We find that lan-\n",
      "guage model-based metrics such as ALIGN SCORE\n",
      "(Zha et al., 2023) and prompting-based scoring\n",
      "(Zheng et al., 2023) serve as strong evaluators,\n",
      "whereas traditional metrics ( ROUGE (Lin, 2004),\n",
      "BERTS CORE (Zhang et al., 2020)) underperform.\n",
      "Following this, we evaluate methods for generat-\n",
      "ing perspective summaries with improved coverage\n",
      "and faithfulness beyond zero-shot inference. We\n",
      "benchmark prompting frameworks, mechanistic\n",
      "methods for mitigating input biases, and reranking-\n",
      "based methods that select the best candidate based\n",
      "on proxy metrics. Using both human and automatic\n",
      "evaluations, we show that reranking outperforms\n",
      "\n",
      "zero-shot inference and prompting-based methods,\n",
      "while prompting only yields marginal improve-\n",
      "ments over zero-shot inference. Notably, prefer-\n",
      "ence tuning with Direct Preference Optimization\n",
      "(DPO) (Rafailov et al., 2023) on reranked genera-\n",
      "tions further boosts performance on both attributes\n",
      "and particularly improving faithfulness. Our re-\n",
      "sults suggest that current LLMs can generate high-\n",
      "quality perspective summaries with strong cover-\n",
      "age and faithfulness, and that preference-based\n",
      "training can further boost performance.\n",
      "In summary, our contributions are as follows:\n",
      "•We construct a controlled test set and identify\n",
      "effective metrics for measuring coverage and\n",
      "faithfulness for perspective summarization.\n",
      "•We evaluate various generation methods and\n",
      "demonstrate that reranking-based approaches\n",
      "deliver the best performance in producing\n",
      "summaries with improved coverage perspec-\n",
      "tive and faithfulness. Notably, preference\n",
      "tuning on reranked generations significantly\n",
      "improves both attributes, with the most pro-\n",
      "nounced gains in faithfulness.\n",
      "•We conduct ablation studies and show\n",
      "that commonly employed prompting frame-\n",
      "works consistently underperform relative to\n",
      "reranking-based methods, even when scaled\n",
      "to high-resource settings.1\n",
      "2 Related Work\n",
      "Summary Evaluation. Summary evaluation tra-\n",
      "ditionally relies on reference-based metrics, in-\n",
      "cluding n-gram-based methods ( ROUGE (Lin,\n",
      "2004), BLEU (Papineni et al., 2002), CHRF\n",
      "(Popovi ´c, 2015)), model-based coverage scores\n",
      "(BERTS CORE (Zhang et al., 2020), BLEURT\n",
      "(Sellam et al., 2020)), and composite measures\n",
      "(METEOR (Banerjee and Lavie, 2005)). In re-\n",
      "sponse to unreliable references, recent work pro-\n",
      "poses reference-free metrics that target aspects such\n",
      "as faithfulness and factual consistency. Neural ap-\n",
      "proaches dominate this space, including end-to-\n",
      "end classifiers (FactCC (Kryscinski et al., 2020)),\n",
      "QA-based methods (QAGS (Wang et al., 2020),\n",
      "QAFactEval (Fabbri et al., 2022)), NLI models\n",
      "(SUMMA C(Laban et al., 2022)), and informa-\n",
      "tion alignment models ( ALIGN SCORE (Zha et al.,\n",
      "2023)). Here, we focus on automatic, reference-\n",
      "free measures of coverage and faithfulness, but\n",
      "conduct a novel evaluation of their reliability in a\n",
      "1Our code is available at https://github.com/\n",
      "narutatsuri/Unbiased-Perspective-Summarization .multi-document perspective summarization task.\n",
      "Beyond developing improved faithfulness met-\n",
      "rics, prior works focus on improving the factual\n",
      "consistency of summarizers, with studies noting the\n",
      "tradeoff between abstractiveness and faithfulness\n",
      "(Durmus et al., 2020; Dreyer et al., 2023). Accord-\n",
      "ingly, some methods improve faithfulness without\n",
      "increasing extraction (Ladhak et al., 2022), while\n",
      "others modify training via contrastive (Nan et al.,\n",
      "2021), multi-task (Chen et al., 2022), or reinforce-\n",
      "ment learning (Roit et al., 2023) methods. In con-\n",
      "trast, we show that reranking-based methods serve\n",
      "as a strong baseline that yields high faithfulness\n",
      "without sacrificing abstractiveness, and a DPO-\n",
      "based approach trained on reranked self-generated\n",
      "summaries further improves both qualities.\n",
      "Perspective-Conditioned Summarization. Ex-\n",
      "isting research on opinion summarization and re-\n",
      "lated tasks has primarily focused on domains such\n",
      "as product reviews (Bražinskas et al., 2020), while\n",
      "recent work has broadened to a range of tasks on\n",
      "opinionated texts. Most single-document meth-\n",
      "ods aim to preserve authorial intent (Liu et al.,\n",
      "2024b) or polarity (Lei et al., 2024), whereas multi-\n",
      "document summarization must integrate varied per-\n",
      "spectives. For instance, Lee et al. (2022b) gen-\n",
      "erates politically neutral summaries from sets of\n",
      "left-, right-, and center-leaning news articles. Other\n",
      "approaches aim to fairly represent diverse perspec-\n",
      "tives in reviews (Zhang et al., 2024c), controllably\n",
      "represent community perspectives (Feng et al.,\n",
      "2024), generate consensus summaries (Bakker\n",
      "et al., 2022), or produce multiple summaries re-\n",
      "flecting distinct political perspectives (Deas and\n",
      "McKeown, 2025). In line with these works, we\n",
      "summarize the political perspective among a set of\n",
      "input passages while addressing the coverage and\n",
      "faithfulness issues observed in existing models as\n",
      "highlighted in these studies.\n",
      "3 Measuring Summary Quality\n",
      "In perspective summarization, the summarizer is\n",
      "given two perspectives θ1, θ2, each with a source\n",
      "article Dt,θ, θ∈ {θ1, θ2}, comprising a set of doc-\n",
      "uments Dt,θ={d(i)\n",
      "t,θ|i∈N}, that present opin-\n",
      "ions on topic t. We study the setting where the\n",
      "summarizer is tasked to generate a summary that\n",
      "encapsulates all key points directly supporting a\n",
      "specified perspective’s stance. Concretely, a high-\n",
      "quality perspective summary should: (1) include\n",
      "all key points from each relevant document, and\n",
      "(2) avoid including any content unsupported by or\n",
      "\n",
      "Article Topic Ron DeSantis\n",
      "Perspective\n",
      "Source Article\n",
      "(Key Points)DeSantishasshown authoritariantendenciesthrough -\n",
      "outhistime inoffice.\n",
      "DeSantis’electionpoliceproposal chills legitimate\n",
      "electionwork andthreat ensdemoc racy.\n",
      "DeSantis’claim thatFlorida isthefreest state contra-\n",
      "dicts restrictions onhealth, protest, andeducation.\n",
      "Synthetic\n",
      "(High-Quality)The article contends that DeSantis’sproposal foran\n",
      "electionpolicesquad undermines legitimate election\n",
      "activitiesanddemoc racy, contradicts hisclaim of\n",
      "Florida beingthefreest state byrestrict ingvarious\n",
      "freedoms, and highlights hispersistentauthoritarian\n",
      "inclinations duringhistenure.\n",
      "Synthetic\n",
      "(Low-Quality)The article highlights DeSantis’s authoritarianten-\n",
      "denciesandhiscontradictionincallingFlorida the\n",
      "freest state while restrict ingfreedoms, but praises his\n",
      "electionpoliceproposal forprotectingelections and\n",
      "strength eningdemoc racy and urges Trump topriori-\n",
      "tizeGOP leadershipinFlorida andretakingtheHouse\n",
      "over personal pride.\n",
      "Table 1: Examples of constructed summaries. For\n",
      "brevity, only curated key points are shown for the source\n",
      "article. Purple,blue, and green highlights denote rele-\n",
      "vant key points, while red and orange highlights respec-\n",
      "tively indicate adversarial and opposite key points.\n",
      "in opposition to the perspective’s documents. We\n",
      "formalize these properties as follows:\n",
      "•Perspective Coverage : The ratio of key points\n",
      "included in the summary relative to the total\n",
      "number of key points.\n",
      "•Perspective Faithfulness : The ratio of relevant\n",
      "key points included in the summary relative\n",
      "to the total number of included key points.\n",
      "Although metrics for similar properties exist in\n",
      "other summarization domains, it is unclear whether\n",
      "they effectively measure the properties as defined\n",
      "above for the perspective summarization task. We\n",
      "therefore evaluate how well these metrics capture\n",
      "our definitions of coverage and faithfulness.2\n",
      "3.1 Assessing Metric Quality\n",
      "Quantifying the efficacy of existing metrics re-\n",
      "quires article-summary pairs with ground truth\n",
      "scores for evaluation. Although perspective sum-\n",
      "marization datasets such as POLISUM(Deas and\n",
      "McKeown, 2025) include reference summaries,\n",
      "each document is paired with only one summary\n",
      "without assigned scores for coverage and faithful-\n",
      "ness. Hence, we construct a test set of article-\n",
      "summary pairs with assigned ground truth scores\n",
      "for coverage and faithfulness and evaluate how well\n",
      "existing metrics align with these scores.\n",
      "Test Set Construction. To assign meaningful\n",
      "ground truth scores for both attributes, we identify\n",
      "2We note that our notions of coverage and faithfulness\n",
      "differ from prior work (Zhang and Bansal, 2021; Tang et al.,\n",
      "2024; Song et al., 2024), as we assess both attributes with\n",
      "respect to the correct inclusion of key points. For brevity,\n",
      "we use perspective coverage and faithfulness interchangeably\n",
      "with coverage and faithfulness.\n",
      "(2) Generate Key PointsHuman \n",
      "Annotators\n",
      "(3) Generate Contr olled Summaries\n",
      "...Coverage Faithfulness\n",
      "1.000 0.500\n",
      "0.333 0.333\n",
      "...0.750 1.000\n",
      "0.250 0.200(1) Extract Document Excerpts\n",
      "LLMExcerpts\n",
      "ParaphraseFigure 1: Pipeline for curating the synthetic testbed for\n",
      "metric evaluation. Annotators extract the most impor-\n",
      "tant excerpts Et,θfrom articles Dt,θ, which are para-\n",
      "phrased into key points Kt,θand adversarial key points\n",
      "Kt,θ. We then curate summaries with a diverse range of\n",
      "coverage and faithfulness scores using the key points.\n",
      "all key points in an article and create summaries\n",
      "using different combinations of these points. We\n",
      "begin with articles from POLISUM3and collect hu-\n",
      "man annotations in which annotators highlight doc-\n",
      "ument excerpts supporting the perspective’s stance.\n",
      "See §D.2 for the annotation interface.\n",
      "Formally, given an article Dt,θ, we collect a set\n",
      "of excerpts Et,θdefined as:\n",
      "Et,θ={e(i)\n",
      "t,θ|e(i)\n",
      "t,θ⊆d(i)\n",
      "t,θ, d(i)\n",
      "t,θcontains a key point },\n",
      "where |Et,θ| ≤ | Dt,θ|(i.e., not all documents\n",
      "contain key points, and each document has at\n",
      "most one key point). As an excerpt e(i)\n",
      "t,θmay not\n",
      "clearly convey the main argument, we use an LLM\n",
      "f:Et,θ→Kt,θto rewrite excerpts into key points\n",
      "to form the set Kt,θ:4\n",
      "Kt,θ={k(i)\n",
      "t,θ|k(i)\n",
      "t,θ=f(e(i)\n",
      "t,θ), e(i)\n",
      "t,θ∈Et,θ}.\n",
      "Given Kt,θ, we construct summaries S(i)\n",
      "t,θby se-\n",
      "lecting kgkey points from Kt,θandkbfrom a set\n",
      "3In the POLISUMdataset, |Dt,θ|has a mean of 5.31with\n",
      "a standard deviation of 1.45.\n",
      "4See §B.4 for further details.\n",
      "\n",
      "You are an evaluator. Your task is to determine how well a\n",
      "generated summary captures all of the main arguments from a\n",
      "source article. This is a measure of \"coverage,\" which does\n",
      "not necessarily address factual accuracy (faithfulness) but\n",
      "focuses on completeness of content. The scale for coverage is:\n",
      "1. No Coverage: The summary does not include any of the main\n",
      "arguments from the article.\n",
      "2. Low Coverage: The summary includes only a few of the main\n",
      "arguments from the article, omitting most.\n",
      "3. Medium Coverage: The summary contains around half of the\n",
      "article 's main arguments.\n",
      "4. High Coverage: The summary contains most of the main\n",
      "arguments from the article, missing only a few.\n",
      "5. Perfect Coverage: The summary includes all major points\n",
      "mentioned in the article, leaving out nothing important.\n",
      "Follow these steps carefully:\n",
      "(Omitted for Brevity)\n",
      "# Source Article:\n",
      "(article)\n",
      "# Summary:\n",
      "(summary)\n",
      "# Coverage Score (1~5 only):\n",
      "Figure 2: Example prompt for LLM-Coverage. We\n",
      "follow the prompt instruction format in Wu et al. (2024).\n",
      "Portions of the prompt are omitted for brevity. See §B.2\n",
      "for complete prompt instructions.\n",
      "of unfaithful key points. We generate unfaithful\n",
      "key points by sampling key points from the op-\n",
      "posing perspective (e.g., using key points from the\n",
      "left-leaning document for right-perspective sum-\n",
      "maries), and by reversing the content of key points\n",
      "inKt,θto form adversarial key points Kt,θ(Laban\n",
      "et al., 2022). We then define:\n",
      "Coverage (S(i)\n",
      "t,θ) =kg\n",
      "|Kt,θ|, (1)\n",
      "Faithfulness (S(i)\n",
      "t,θ) =kg\n",
      "kg+kb. (2)\n",
      "We provide examples of summaries with varying\n",
      "scores in Table 1. With this procedure, we pro-\n",
      "duce summaries with error levels ranging from few\n",
      "minor omissions to many faithfulness errors. We\n",
      "collect annotations for 50documents from 5anno-\n",
      "tators and generate a varying number of summaries\n",
      "for each document, ultimately curating 370article-\n",
      "summary pairs in total. We illustrate the process in\n",
      "Figure 1. See §D.1 for further annotation details.\n",
      "Benchmarked Metrics. As baselines, we re-\n",
      "spectively use the recall and precision variants of\n",
      "ROUGE (Lin, 2004) and BERTS CORE (Zhang\n",
      "et al., 2020) for measuring coverage and faithful-\n",
      "ness. We also report BLEURT (Sellam et al., 2020)\n",
      "as an additional coverage metric. For faithfulness,\n",
      "we test SUMMA C(Laban et al., 2022) (NLI-based\n",
      "inconsistency detection metric), ALIGN SCORE\n",
      "(Zha et al., 2023) (factual consistency metric), the\n",
      "consistency dimension of UniEval (Zhong et al.,\n",
      "2022) (T5-based multi-task evaluator), MiniCheckCoverage Faithfulness\n",
      "Metric Corr. ( ρs) Winrate Corr. ( ρs) Winrate\n",
      "ROUGE L(R) 0.473∗∗∗0.780±0.048 −0.038 0.393±0.063\n",
      "BERTS CORE (R) 0.527∗∗∗0.815±0.018 −0.032∗∗0.415±0.015\n",
      "BLEURT 0.086 0.530±0.067 −0.014 0.527±0.063\n",
      "LLM-Coverage 0.707∗∗∗0.739±0.047 0.393∗∗∗0.431±0.115\n",
      "ROUGE L(P) −0.169∗∗0.443±0.056 0.333∗∗∗0.714±0.076\n",
      "BERTS CORE (P) 0.073∗∗0.510±0.030 0.366∗∗∗0.655±0.020\n",
      "SUMMA C 0.028 0.491±0.084 −0.016 0.315±0.066\n",
      "ALIGN SCORE 0.261∗∗∗0.503±0.074 0.650∗∗∗0.773±0.061\n",
      "UniEval ( C) 0.267∗∗∗0.545±0.055 0.629∗∗∗0.768±0.054\n",
      "MiniCheck 0.099 0.435±0.066 0.578∗∗∗0.747±0.074\n",
      "FineSurE ( F) 0.271∗∗∗0.288±0.076 0.084 0.216±0.072\n",
      "LLM-Faithfulness 0.462∗∗∗0.398±0.055 0.706∗∗∗0.537±0.091\n",
      "Table 2: Comparison of Spearman correlation ( ρs) and\n",
      "Winrate with 95% Confidence Interval (CI) across all\n",
      "metrics. Darker shading indicates better performance.\n",
      "Asterisks indicate significance levels (∗,∗∗,∗∗∗forp <\n",
      "0.05,0.01,0.001, respectively). PandRdenote the\n",
      "precision and recall variants of each metric. Note the\n",
      "random baseline for Winrate is 0.500.\n",
      "(Tang et al., 2024) ( FLAN-T5 model for fact-\n",
      "checking via entailment), and the faithfulness di-\n",
      "mension of FineSurE (Song et al., 2024) (span-\n",
      "level fact verification). See §B.1 for details on\n",
      "metric configurations and model checkpoints.\n",
      "Furthermore, recent studies suggest that LLMs\n",
      "serve as effective evaluators (Chiang and Lee, 2023;\n",
      "Dubois et al., 2023; Chen et al., 2023), including\n",
      "for some dimensions of summary qualities (Jain\n",
      "et al., 2023; Wu et al., 2024). Hence, we examine\n",
      "two LLM-as-a-Judge settings where the source ar-\n",
      "ticle and generated summary are passed as input\n",
      "alongside tailored prompts (Liu et al., 2023). We\n",
      "respectively term these LLM-Coverage andLLM-\n",
      "Faithfulness for convenience. As an example, see\n",
      "Figure 2 for the LLM-Coverage prompt instruc-\n",
      "tion. We use Mistral-7B-Instruct-v0.3 as the\n",
      "default backbone based on evaluation performance.\n",
      "See §B.3 for results using alternative models.\n",
      "Note that, by our formulation, coverage corre-\n",
      "sponds to recall and faithfulness to precision in key\n",
      "point inclusion. Hence, we report results on both\n",
      "attributes for all metrics and show that recall-based\n",
      "metrics do not capture faithfulness and vice versa\n",
      "to verify the reliability of our curated test set.\n",
      "Evaluation Criteria. We examine two measures\n",
      "of evaluating metrics: (1) Correlation , assessed\n",
      "via Spearman correlation between metric-assigned\n",
      "and ground truth scores, and (2) Winrate , the ac-\n",
      "curacy for which the metric correctly selects the\n",
      "summary with the higher ground truth score. For\n",
      "each source article, we form summary pairs and\n",
      "compute the average ratio of correctly ranked pairs.\n",
      "A desirable metric should achieve high scores for\n",
      "both measures, as correlation gauges true model\n",
      "\n",
      "performance whereas winrate measures the met-\n",
      "ric’s accuracy in selecting the better summarizer.\n",
      "3.2 Results\n",
      "We present our results in Table 2. Overall, LLM-\n",
      "Coverage and ALIGN SCORE serve as reliable met-\n",
      "rics for coverage and faithfulness respectively,\n",
      "which we use as automatic evaluators in §5. No-\n",
      "tably, metrics for coverage do not effectively mea-\n",
      "sure faithfulness and vice versa, indicating that our\n",
      "testbed assesses these dimensions separately.\n",
      "We see that although both variants of ROUGE\n",
      "andBERTS CORE do not achieve the highest corre-\n",
      "lation, they display moderate correlation ( 0.376∼\n",
      "0.527) and winrate alignment ( 0.722∼0.815on\n",
      "average). In contrast, we see that BLEURT and\n",
      "SUMMA C exhibit poor results for both attributes.\n",
      "In particular, LLM-Coverage exhibits strong cov-\n",
      "erage performance with a Spearman correlation of\n",
      "0.707and a winrate of 0.739. For faithfulness,\n",
      "LLM-Faithfulness performs the best on correlation,\n",
      "butALIGN SCORE , UniEval, and MiniCheck ex-\n",
      "hibit better winrates, also corroborating prior work\n",
      "that suggest LLMs are not yet reliable as standalone\n",
      "measures of faithfulness (Parcalabescu and Frank,\n",
      "2024; Siegel et al., 2024).\n",
      "4 Method Evaluation\n",
      "With reliable metrics established in §3, we now\n",
      "investigate methods for generating improved per-\n",
      "spective summaries beyond zero-shot prompting.\n",
      "Notably, due to the absence of large-scale training\n",
      "data, we examine several well-established methods\n",
      "and variants that do not rely on training data. We\n",
      "useLlama-3.1-8B-Instruct as the default back-\n",
      "bone for all methods.\n",
      "Prompting-Based Approaches. Much work on\n",
      "LLMs proposes inference-time methods that elicit\n",
      "reasoning and planning (Wang et al., 2023a; Press\n",
      "et al., 2023; Huang et al., 2023; Weng et al., 2023;\n",
      "Zhang et al., 2024b). Such methods have proven\n",
      "effective across various tasks (Wang et al., 2023b;\n",
      "Jacob et al., 2024; Saha et al., 2024; Dhuliawala\n",
      "et al., 2024) and improve factual consistency (Xu\n",
      "et al., 2024). As such, we consider two methods:\n",
      "(1)Multi-Agent Debate (Du et al., 2024), where\n",
      "multiple LLMs iteratively update their responses\n",
      "based on one another, and (2) Self-Refine (Madaan\n",
      "et al., 2023), where an LLM iteratively critiques\n",
      "and revises its own output. We use the default set-\n",
      "tings of three agents over three rounds for Debate\n",
      "and three iterations for Self-Refine.Mechanistic Approach. A natural alternative to\n",
      "zero-shot inference is to direct the model’s atten-\n",
      "tion to salient input segments that support the over-\n",
      "all perspective. Similar methods have been pro-\n",
      "posed to mitigate position biases in LLMs using\n",
      "calibration-based (Hsieh et al., 2024) and mechanis-\n",
      "tic approaches (Ratner et al., 2023; Hu et al., 2024;\n",
      "Liu et al., 2024a). In particular, PINE (Wang et al.,\n",
      "2024) modifies causal attention bidirectionally and\n",
      "increases the weight on specified segments. We ex-\n",
      "amine whether controlling the model’s attention to\n",
      "segments corresponding to the desired perspective\n",
      "can improve coverage and faithfulness. See §A.1\n",
      "for additional details.\n",
      "Reranking Generations. We examine a Rerank-\n",
      "ing (RR) approach in which an untrained back-\n",
      "bone generates multiple summaries and we se-\n",
      "lect the highest-scoring summary based on LLM-\n",
      "Coverage and LLM-Faithfulness. Prior work has\n",
      "explored similar methods (Vijayakumar et al.,\n",
      "2018; Suzgun et al., 2022) with notable success\n",
      "(Wei et al., 2022; Xu et al., 2024). Benchmark-\n",
      "ing reranking examines whether the backbone\n",
      "is inherently capable of generating high-quality\n",
      "summaries. In particular, comparing reranking\n",
      "with prompting-based methods, which are more\n",
      "commonly used to improve inference-time perfor-\n",
      "mance, assesses the optimal approach for perspec-\n",
      "tive summarization. For reranking-based methods,\n",
      "we use Qwen2.5-14B-Instruct as the scorer back-\n",
      "bone to avoid incorporating signals from the auto-\n",
      "matic coverage evaluation, and we generate nine\n",
      "summaries per input for reranking by default.\n",
      "Preference Tuning with Reranking. Many stud-\n",
      "ies employ reinforcement learning-based training\n",
      "for instruction following (Ouyang et al., 2022; Bai\n",
      "et al., 2022; Nakano et al., 2022), with applica-\n",
      "tions in summary generation (Stiennon et al., 2020;\n",
      "Gooding and Mansoor, 2023; Huang et al., 2024;\n",
      "Lee et al., 2024). However, these approaches typ-\n",
      "ically rely on human feedback (e.g., RLHF) and\n",
      "labeled preference pairs (e.g., DPO (Rafailov et al.,\n",
      "2023)). Here, we examine whether preference-\n",
      "based training on synthetic, reranking-generated\n",
      "data can improve perspective summarization per-\n",
      "formance. Namely, we consider a DPO with\n",
      "Reranking (DPO+RR) approach that iteratively\n",
      "repeats the procedure of generating summaries\n",
      "with the backbone model, scoring them with LLM-\n",
      "Coverage and LLM-Faithfulness, and creating pref-\n",
      "erence pairs by designating higher-scoring sum-\n",
      "\n",
      "Article\n",
      "HighlightsTwo years afteraRuth Bader Ginsburg’s death made theeliminationofthatright more likely than not,President JoeBiden entered thechat. . . The\n",
      "fumbling about foraspell beforeheawak enstothefireisarecurringpattern.\n",
      "Biden hasbeen notably tucked away. . . With Roe, thesituationisparticularly galling. . . damning indictment oftheadministration’s messag-\n",
      "ing. . . Biden appears tobetrapped inavicious cycle. . . he andhisadvisersappear tobegripped with anxietythatdoinganything willonly make\n",
      "things worse.\n",
      "Democrats need togive votersreasons tobelieve. . . the only way toreverse Biden’s slidingpopularityisforhim tostep forward andstart\n",
      "providingthese reasons. Ourimmediategoal within theDemo cratic Party isto\"dump Biden\"\n",
      "Zero-Shot The Left believes that President JoeBiden’s slow response totheSupreme Court’s eliminationoftheconstitutional right toanabortionandhis\n",
      "overalllack ofeffectivecommunicationanddecisiveactionaremajorreasons forhisdecliningpopularityandtheDemocrats’ electoral strug gles.\n",
      "DPO+RR The Left views President JoeBiden’s delayed response totheSCO TUS abortionruling,characterized by\"fum bling\" actions, asfurtherexacerbat-\n",
      "inghisdecliningpublicimage and hinderinghisabilitytoprovide mean ingfulsolutions amidst variousnational crises, underscoring concerns\n",
      "thathisleadership style may undermine demo cratic valuesandultimately harm Democrats’ chances atre-election.\n",
      "Table 3: Example summary generated by Zero-Shot and DPO+RR. Highlights indicate excerpts marked by an\n",
      "annotator. Zero-Shot captures only one of the three excerpts, whereas DPO+RR captures all three key points.\n",
      "maries as preferred over lower-scoring ones, which\n",
      "are then used to train the backbone. We split the\n",
      "POLISUMdataset (1816 article pairs) into train\n",
      "(1716) and test (100) splits to ensure that synthetic\n",
      "training data is generated exclusively from the train\n",
      "split, and repeat over 10 epochs.\n",
      "4.1 Evaluation Setup\n",
      "Automatic Evaluation. We automatically evalu-\n",
      "ate summary quality using two criteria. First, we\n",
      "assign numerical scores to summaries using LLM-\n",
      "Coverage and ALIGN SCORE (cf. §3). Second, we\n",
      "compute instance-level rankings across all test arti-\n",
      "cles to assess relative method performance. How-\n",
      "ever, as automatic metrics do not rank methods\n",
      "perfectly (cf. Table 2), we address this by fitting a\n",
      "Bradley-Terry model to the pairwise comparisons\n",
      "derived from raw scores and performing bootstrap\n",
      "resampling over the test documents to obtain 95%\n",
      "confidence intervals. This avoids naive \"rank-then-\n",
      "average\" methods that can yield cyclic or inconsis-\n",
      "tent preferences when pairwise comparisons do not\n",
      "form a strict total ordering. We use the split test set\n",
      "of 200 input documents for automatic evaluation.\n",
      "Refer to §A.2 for details on the ranking procedure.\n",
      "Human Evaluation. We collect human judg-\n",
      "ments on summary quality by having annotators\n",
      "review input documents and their corresponding\n",
      "model-generated summaries. Analogous to the pro-\n",
      "cedure in §3.1, annotators first extract key points\n",
      "from both documents and summaries, then identify\n",
      "which document key points each summary includes\n",
      "or omits, and which summary key points appear in\n",
      "the document. This process yields coverage and\n",
      "faithfulness scores computed as in Eqs. (1)and(2).\n",
      "See §D.1 for further annotation details.\n",
      "5 Results\n",
      "We present coverage and faithfulness results in Fig-\n",
      "ures 3a (automatic evaluation) and 3b (human eval-\n",
      "uation), and provide generated example summariesRatio Document Summary\n",
      "R(· | ·)0.672±0.262 0 .918±0.173\n",
      "Random 0.235±0.322 0 .650±0.380\n",
      "Table 4: Inter-Annotator Agreement (IAA) results. Val-\n",
      "ues lie between the interval [0,1]. We observe substan-\n",
      "tial agreement for both document- and summary-level\n",
      "key point extraction.\n",
      "in Table 3. We include additional examples in §C.3.\n",
      "5.1 Automatic Evaluation\n",
      "We observe that DPO+RR achieves the highest\n",
      "performance on both metrics, improving coverage\n",
      "and faithfulness scores by 0.590and0.081, cor-\n",
      "responding to approximately 12% and 8% gains,\n",
      "respectively. Reranking is a strong baseline, out-\n",
      "performing all other methods by considerable mar-\n",
      "gins, corroborating prior work on the benefits of\n",
      "re-ranking (e.g., (Horvitz et al., 2024)). In contrast,\n",
      "zero-shot inference, prompting methods, and PINE\n",
      "show minimal score differences. Although Self-\n",
      "Refine marginally improves coverage over zero-\n",
      "shot inference, all methods except reranking yield\n",
      "lower faithfulness scores.\n",
      "5.2 Human Evaluation\n",
      "DPO+RR achieves the highest human evaluation\n",
      "scores ( 0.437for coverage and 0.724for faithful-\n",
      "ness), with Reranking close behind ( 0.410 and\n",
      "0.673, respectively). Prompting-based methods\n",
      "improve coverage over zero-shot inference ( 0.347\n",
      "for coverage and 0.642for faithfulness) but yield\n",
      "similar faithfulness scores. PINE shows no perfor-\n",
      "mance gains for either attribute.\n",
      "Inter-Annotator Agreement (IAA). We mea-\n",
      "sure IAA by counting the number of excerpts\n",
      "with non-trivial overlap between annotators. For-\n",
      "mally, given excerpts from two annotators Aand\n",
      "B(from a document or a summary), denoted as\n",
      "EA={eA\n",
      "1, eA\n",
      "2, . . .}andEB={eB\n",
      "1, eB\n",
      "2, . . .},\n",
      "we define a matching function M(EA, EB)that\n",
      "\n",
      "2.252.502.753.003.253.503.754.004.25Score3.1003.1253.0803.1053.6703.690\n",
      "123456Ranking\n",
      "0.400.450.500.550.600.650.700.750.80Score0.604\n",
      "0.5800.5950.5860.6470.685\n",
      "123456RankingCoverage Faithfulness\n",
      "Zero-Shot\n",
      "Self-Refine\n",
      "Debate\n",
      "PINE\n",
      "Reranking\n",
      "DPO+RR(a) Automatic evaluation results. Higher values indicate better performance for Score\n",
      "(Bars), while lower values are better for Ranking (Lolipops). Coverage scores range\n",
      "from 1 to 5, while faithfulness scores lie in the interval [0,1]. DPO+RR achieves the\n",
      "highest scores and best average rank, followed by Reranking. Other methods show\n",
      "similar performance in both coverage and faithfulness.\n",
      "0.200.250.300.350.400.450.500.55\n",
      "0.3470.3820.375\n",
      "0.3250.4100.437Coverage\n",
      "0.500.550.600.650.700.750.800.85\n",
      "0.6420.6250.6340.6220.6730.724Faithfulness(b) Human evaluation results. Higher\n",
      "is better for both attributes. DPO+RR\n",
      "achieves the best performance for\n",
      "both attributes, followed by Reranking.\n",
      "Scores lie in [0,1]for both attributes.\n",
      "Figure 3: Automatic (left) and human (right) evaluation results. For clarity, note that y-axes do not begin at 0 in\n",
      "score plots. Reranking-based methods perform best across both evaluation regimes, with DPO+RR achieving the\n",
      "highest overall performance in both coverage and faithfulness. Error bars represent 95% confidence intervals (CI).\n",
      "Method |KD∩KS| | KD\\KS| | KS\\KD|\n",
      "Zero-Shot 1.338±0.894 3.059±1.254 0.765±0.855\n",
      "Self-Refine 1.412±1.097 2.912±1.288 0.794±0.729\n",
      "Debate 1.368±0.847 2.971±1.291 0.735±0.790\n",
      "PINE 1.206±0.854 3.235±1.350 0.706±0.799\n",
      "Reranking 1.544±0.916 2.882±1.320 0.735±0.828\n",
      "DPO+RR 1.721±0.889 2.500±1.080 0.618±0.739\n",
      "Table 5: Statistics for key point inclusion for each\n",
      "method with standard deviation. |KD∩KS|,|KD\\KS|,\n",
      "and|KS\\KD|denote the average number of key points\n",
      "included, omitted, and hallucinated, respectively.\n",
      "counts the number of strings in EAmatched to\n",
      "at most one string in EB. We then compute\n",
      "R(A|B) =|M(EA, EB)|/|EA|andR(B|\n",
      "A) =|M(EA, EB)|/|EB|, and take their average\n",
      "to obtain the overall annotator overlap R(· | ·). To\n",
      "assess overlap, we provide overlapping annotations\n",
      "to pairs of annotators across five documents and\n",
      "evaluate agreement for both document-level and\n",
      "summary-level key point extraction. Additionally,\n",
      "we establish a random baseline for annotator over-\n",
      "lap by sampling highlight counts and lengths for\n",
      "documents and summaries that match the observed\n",
      "mean and variance in the real annotations. Further\n",
      "details are provided in §C.1.\n",
      "Results are presented in Table 4. Observe\n",
      "that annotators exhibit substantial overlap in both\n",
      "document- and summary-level annotations that con-\n",
      "siderably exceed the random baseline. We also\n",
      "see higher agreement for summaries than for docu-\n",
      "ments, which we attribute to summaries being more\n",
      "concise and explicitly including key points.\n",
      "Overall, our results show that while prompting-\n",
      "based and attention modification methods of-\n",
      "fer little improvement over zero-shot prompting,\n",
      "reranking-based methods significantly improves\n",
      "coverage and faithfulness. In particular, employ-\n",
      "ing DPO-based training further boosts faithfulness,\n",
      "even when using self-generated synthetic data.Method Novel 4-gram ( ↑) EF Density ( ↓)\n",
      "Zero-Shot 0.930±0.104 1.815±1.614\n",
      "Self-Refine 0.946±0.094 1.470±1.307\n",
      "Debate 0.954±0.088 1.571±1.162\n",
      "PINE 0.848±0.074 3.340±4.801\n",
      "Reranking 0.949±0.217 1.445±0.914\n",
      "DPO+RR 0.953±0.079 1.415±1.039\n",
      "Table 6: Abstractiveness statistics for each method, mea-\n",
      "sured by novel n-gram ratios and extractive fragment\n",
      "density. Arrows indicate higher abstractiveness.\n",
      "6 Analysis\n",
      "6.1 Summary Characteristics\n",
      "Here, we examine the summaries generated by each\n",
      "method and assess their key point inclusion pat-\n",
      "terns, abstractiveness, and length.\n",
      "Key Point Inclusion. Beyond coverage and faith-\n",
      "fulness, we evaluate how each method incorporates\n",
      "key points. For an article Dwith key points KD\n",
      "and a summary Swith key points KS, we com-\n",
      "pute the average number of key points included\n",
      "(|KD∩KS|), omitted ( |KD\\KS|), and halluci-\n",
      "nated ( |KS\\KD|).\n",
      "Results are shown in Table 5. We observe that\n",
      "DPO+RR includes more relevant key points while\n",
      "minimizing hallucinations and omissions compared\n",
      "to other methods. In contrast, PINE is more conser-\n",
      "vative, reducing hallucinations but omitting more\n",
      "key points. Self-Refine retains additional key\n",
      "points yet introduces more hallucinations, while\n",
      "Debate shows only slight improvements over the\n",
      "zero-shot baseline.\n",
      "Summary Abstractiveness. We assess abstrac-\n",
      "tiveness using two metrics: (1) Novel n-gram ratios\n",
      "(See et al., 2017), which measure the proportion\n",
      "ofn-grams in the summary absent from the source\n",
      "(with n= 4), and (2) Extractive fragment density\n",
      "\n",
      "2 3 4 5 6 7 8 9\n",
      "Debate Rounds3.03.54.0Score\n",
      "Coverage\n",
      "DPO+RR (n=3)\n",
      "DPO+RR (n=18)3 Agents\n",
      "5 Agents7 Agents\n",
      "9 Agents\n",
      "2 3 4 5 6 7 8 9\n",
      "Debate Rounds0.450.500.550.600.650.700.75Score\n",
      "Faithfulness(a) Debate performance across varying\n",
      "agent counts and debate rounds.\n",
      "2 4 6 8 10 12 14 16 18\n",
      "Refinement Rounds3.03.54.0Score\n",
      "Coverage\n",
      "DPO+RR (n=3)\n",
      "DPO+RR (n=18)\n",
      "2 4 6 8 10 12 14 16 18\n",
      "Refinement Rounds0.450.500.550.600.650.700.75Score\n",
      "Faithfulness(b) Self-Refine performance across vary-\n",
      "ing refinement rounds.\n",
      "4 6 8 10 12 14\n",
      "Number of Generations2.753.003.253.503.754.00ScoreCoverage\n",
      "DPO+RR (LLM)\n",
      "DPO+RR (ROUGE)Reranking (LLM)\n",
      "Reranking (ROUGE)\n",
      "4 6 8 10 12 14\n",
      "Number of Generations0.550.600.650.700.75ScoreFaithfulness(c) Comparison of LLM- and ROUGE -\n",
      "based proxies for reranking methods.\n",
      "Figure 4: Ablation study results. Figures 4a and 4b show that both prompting-based methods consistently\n",
      "underperform compared to reranking-based methods across all resource settings. Figure 4c shows that using\n",
      "a ROUGE-based proxy metric yields worse performance than LLM-based proxy metrics.\n",
      "(Grusky et al., 2018), which quantifies the con-\n",
      "tinuity of extracted spans. Higher novel n-gram\n",
      "ratios and lower extractive fragment density indi-\n",
      "cate greater abstractiveness. We include additional\n",
      "results for analysis in §C.2.\n",
      "Table 6 shows our results. Notably, PINE ex-\n",
      "hibits lower novel n-gram ratios and higher extrac-\n",
      "tive fragment density than other methods, indicat-\n",
      "ing that PINE favors more extractive summaries.\n",
      "In contrast, DPO+RR yields higher abstractiveness\n",
      "than zero-shot inference while also improving faith-\n",
      "fulness (cf. §5). This suggests that DPO+RR not\n",
      "only encourages extraction of source content but\n",
      "also generates summaries with more novel tokens.\n",
      "6.2 Ablation Studies\n",
      "We conduct ablation studies to determine whether\n",
      "prompting-based methods outperform Reranking\n",
      "and DPO+RR under more resourceful generation\n",
      "settings, as measured by automated metrics. See\n",
      "Figure 4 for all results.\n",
      "Debate: Agents and Rounds. We vary the num-\n",
      "ber of rounds n∈ {2,3, . . . , 9}and agents m∈\n",
      "{3,5,7,9}in Multi-Agent Debate. For reference,\n",
      "we report results for DPO+RR in two settings: gen-\n",
      "erating 3 (base setting) and generating 18 sum-\n",
      "maries (approximate upper bound).\n",
      "From Figure 4a, we observe that increasing the\n",
      "number of agents improves coverage but not faith-\n",
      "fulness. With m= 9agents, Debate slightly out-\n",
      "performs DPO+RR with 3 reranked generations for\n",
      "n≥4in coverage but falls short of DPO+RR with\n",
      "18 generations. For faithfulness, Debate remains\n",
      "consistently below DPO+RR in all settings.Self-Refine: Refinement Rounds. We evaluate\n",
      "Self-Refine over various numbers of refinement\n",
      "rounds ( n∈ {2,3, . . . , 18}). Results are shown\n",
      "in Figure 4b. We observe that coverage improves\n",
      "with more rounds, whereas faithfulness does not.\n",
      "Nevertheless, Self-Refine underperforms DPO+RR\n",
      "in both settings across all rounds.\n",
      "Reranking: ROUGE as Proxy Metric. To as-\n",
      "sess the effectiveness of LLM-based proxy metrics,\n",
      "we compare Reranking and DPO+RR with vari-\n",
      "ants that use ROUGE as the proxy. Following\n",
      "the training procedure in §4, we replace the orig-\n",
      "inal proxy with the average ROUGE nscore (for\n",
      "n∈ {1,2, L}) computed across both precision and\n",
      "recall. As shown in Figure 4c, the ROUGE -based\n",
      "variant underperforms across all settings.\n",
      "7 Conclusion\n",
      "In this paper, we identify reliable evaluation met-\n",
      "rics for measuring perspective summary quality and\n",
      "investigate LLM-based methods for generating im-\n",
      "proved summaries beyond zero-shot inference. We\n",
      "construct a test dataset using human annotations\n",
      "to benchmark existing summarization metrics for\n",
      "coverage and faithfulness. We find that traditional\n",
      "metrics such as ROUGE andBERTS CORE un-\n",
      "derperform, while language model–based metrics\n",
      "such as ALIGN SCORE and prompting-based scor-\n",
      "ing serve as strong evaluators. Using these metrics,\n",
      "we show that reranking-based methods outperform\n",
      "prompting frameworks and significantly improve\n",
      "performance over zero-shot inference. Moreover,\n",
      "preference tuning with self-generated, reranking-\n",
      "labeled data further boosts performance, particu-\n",
      "\n",
      "larly in terms of faithfulness. We recommend that\n",
      "future work examine the transferability of our find-\n",
      "ings to domains beyond political perspectives and\n",
      "whether similar improvements can be achieved in\n",
      "other multi-document summarization tasks.\n",
      "Limitations\n",
      "We acknowledge two limitations in our work. First,\n",
      "we focus on evaluating existing summarization met-\n",
      "rics commonly used in the literature and bench-\n",
      "mark those applied to perspective summarization.\n",
      "As we show that existing metrics achieve satisfac-\n",
      "tory accuracy for evaluating perspective summaries,\n",
      "we do not investigate the development of a novel\n",
      "metric tailored specifically for measuring coverage\n",
      "and faithfulness in this setting. We leave this as a\n",
      "promising direction for future work. Second, we\n",
      "primarily investigated methods for perspective sum-\n",
      "mary generation that do not rely on human-labeled\n",
      "training data, given the infeasibility of collecting\n",
      "such data. Although our experiments with pref-\n",
      "erence tuning using synthetically generated data\n",
      "show performance improvements, future studies\n",
      "should examine the benefits of human-curated train-\n",
      "ing data.\n",
      "Ethical Considerations\n",
      "In this paper, we focus on metrics to accurately\n",
      "measure the unbiasedness of perspective sum-\n",
      "maries through the attributes of coverage and faith-\n",
      "fulness, and we show that certain methods yield\n",
      "higher performance on these attributes. Our work\n",
      "aims to ensure fair representation and reduce hallu-\n",
      "cinations in opinion-based summarization. While it\n",
      "is unclear whether these findings could be misused\n",
      "to generate more biased summaries, we acknowl-\n",
      "edge that such risks are not negligible.\n",
      "Acknowledgements\n",
      "This work was supported in part by the Knight First\n",
      "Amendment Institute at Columbia University, Na-\n",
      "tional Science Foundation Graduate Research Fel-\n",
      "lowship DGE-2036197, the Columbia University\n",
      "Provost Diversity Fellowship, and the Columbia\n",
      "School of Engineering and Applied Sciences Pres-\n",
      "idential Fellowship. Any opinion, findings, and\n",
      "conclusions or recommendations expressed in this\n",
      "material are those of the authors and do not neces-\n",
      "sarily reflect the views of the Knight First Amend-\n",
      "ment Institute or National Science Foundation. Wethank the anonymous reviewers for providing feed-\n",
      "back on an earlier draft of the work.\n",
      "References\n",
      "Reinald Kim Amplayo, Stefanos Angelidis, and Mirella\n",
      "Lapata. 2021. Aspect-controllable opinion summa-\n",
      "rization. In Proceedings of the 2021 Conference on\n",
      "Empirical Methods in Natural Language Processing ,\n",
      "pages 6578–6593, Online and Punta Cana, Domini-\n",
      "can Republic. Association for Computational Lin-\n",
      "guistics.\n",
      "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda\n",
      "Askell, Anna Chen, Nova DasSarma, Dawn Drain,\n",
      "Stanislav Fort, Deep Ganguli, Tom Henighan,\n",
      "Nicholas Joseph, Saurav Kadavath, Jackson Kernion,\n",
      "Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac\n",
      "Hatfield-Dodds, Danny Hernandez, Tristan Hume,\n",
      "Scott Johnston, Shauna Kravec, Liane Lovitt, Neel\n",
      "Nanda, Catherine Olsson, Dario Amodei, Tom\n",
      "Brown, Jack Clark, Sam McCandlish, Chris Olah,\n",
      "Ben Mann, and Jared Kaplan. 2022. Training\n",
      "a helpful and harmless assistant with reinforce-\n",
      "ment learning from human feedback. Preprint ,\n",
      "arXiv:2204.05862.\n",
      "Michiel Bakker, Martin Chadwick, Hannah Sheahan,\n",
      "Michael Tessler, Lucy Campbell-Gillingham, Jan\n",
      "Balaguer, Nat McAleese, Amelia Glaese, John\n",
      "Aslanides, Matt Botvinick, and Christopher Sum-\n",
      "merfield. 2022. Fine-tuning language models to find\n",
      "agreement among humans with diverse preferences.\n",
      "InAdvances in Neural Information Processing Sys-\n",
      "tems, volume 35, pages 38176–38189. Curran Asso-\n",
      "ciates, Inc.\n",
      "Satanjeev Banerjee and Alon Lavie. 2005. METEOR:\n",
      "An automatic metric for MT evaluation with im-\n",
      "proved correlation with human judgments. In Pro-\n",
      "ceedings of the ACL Workshop on Intrinsic and Ex-\n",
      "trinsic Evaluation Measures for Machine Transla-\n",
      "tion and/or Summarization , pages 65–72, Ann Arbor,\n",
      "Michigan. Association for Computational Linguis-\n",
      "tics.\n",
      "Ralph Allan Bradley and Milton E. Terry. 1952. Rank\n",
      "analysis of incomplete block designs: I. the method\n",
      "of paired comparisons. Biometrika , 39:324.\n",
      "Arthur Bražinskas, Mirella Lapata, and Ivan Titov. 2020.\n",
      "Unsupervised opinion summarization as copycat-\n",
      "review generation. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics , pages 5151–5169, Online. Association for\n",
      "Computational Linguistics.\n",
      "Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang\n",
      "Zhang. 2022. Towards improving faithfulness in\n",
      "abstractive summarization. In Advances in Neural\n",
      "Information Processing Systems , volume 35, pages\n",
      "24516–24528. Curran Associates, Inc.\n",
      "\n",
      "Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and\n",
      "Ruifeng Xu. 2023. Exploring the use of large lan-\n",
      "guage models for reference-free text quality evalua-\n",
      "tion: An empirical study. In Findings of the Associa-\n",
      "tion for Computational Linguistics: IJCNLP-AACL\n",
      "2023 (Findings) , pages 361–374, Nusa Dua, Bali.\n",
      "Association for Computational Linguistics.\n",
      "Anshuman Chhabra, Hadi Askari, and Prasant Moha-\n",
      "patra. 2024. Revisiting zero-shot abstractive sum-\n",
      "marization in the era of large language models\n",
      "from the perspective of position bias. Preprint ,\n",
      "arXiv:2401.01989.\n",
      "Cheng-Han Chiang and Hung-yi Lee. 2023. Can large\n",
      "language models be an alternative to human evalua-\n",
      "tions? In Proceedings of the 61st Annual Meeting of\n",
      "the Association for Computational Linguistics (Vol-\n",
      "ume 1: Long Papers) , pages 15607–15631, Toronto,\n",
      "Canada. Association for Computational Linguistics.\n",
      "Tri Dao. 2023. Flashattention-2: Faster attention with\n",
      "better parallelism and work partitioning. Preprint ,\n",
      "arXiv:2307.08691.\n",
      "Nicholas Deas and Kathleen McKeown. 2025. Sum-\n",
      "marization of opinionated political documents with\n",
      "varied perspectives. In Proceedings of the 31st Inter-\n",
      "national Conference on Computational Linguistics ,\n",
      "pages 8088–8108, Abu Dhabi, UAE. Association for\n",
      "Computational Linguistics.\n",
      "Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\n",
      "Roberta Raileanu, Xian Li, Asli Celikyilmaz, and\n",
      "Jason Weston. 2024. Chain-of-verification reduces\n",
      "hallucination in large language models. In Findings\n",
      "of the Association for Computational Linguistics:\n",
      "ACL 2024 , pages 3563–3578, Bangkok, Thailand.\n",
      "Association for Computational Linguistics.\n",
      "Markus Dreyer, Mengwen Liu, Feng Nan, Sandeep\n",
      "Atluri, and Sujith Ravi. 2023. Evaluating the tradeoff\n",
      "between abstractiveness and factuality in abstractive\n",
      "summarization. In Findings of the Association for\n",
      "Computational Linguistics: EACL 2023 , pages 2089–\n",
      "2105, Dubrovnik, Croatia. Association for Computa-\n",
      "tional Linguistics.\n",
      "Yilun Du, Shuang Li, Antonio Torralba, Joshua B.\n",
      "Tenenbaum, and Igor Mordatch. 2024. Improving\n",
      "factuality and reasoning in language models through\n",
      "multiagent debate. In Proceedings of the 41st Inter-\n",
      "national Conference on Machine Learning , ICML’24.\n",
      "JMLR.org.\n",
      "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\n",
      "Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\n",
      "Liang, and Tatsunori Hashimoto. 2023. Alpacafarm:\n",
      "A simulation framework for methods that learn from\n",
      "human feedback. In Thirty-seventh Conference on\n",
      "Neural Information Processing Systems .\n",
      "Esin Durmus, He He, and Mona Diab. 2020. FEQA: A\n",
      "question answering evaluation framework for faith-\n",
      "fulness assessment in abstractive summarization. InProceedings of the 58th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics , pages 5055–\n",
      "5070, Online. Association for Computational Lin-\n",
      "guistics.\n",
      "Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and\n",
      "Caiming Xiong. 2022. QAFactEval: Improved QA-\n",
      "based factual consistency evaluation for summariza-\n",
      "tion. In Proceedings of the 2022 Conference of the\n",
      "North American Chapter of the Association for Com-\n",
      "putational Linguistics: Human Language Technolo-\n",
      "gies, pages 2587–2601, Seattle, United States. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian\n",
      "Fisher, Chan Young Park, Yejin Choi, and Yulia\n",
      "Tsvetkov. 2024. Modular pluralism: Pluralistic align-\n",
      "ment via multi-LLM collaboration. In Proceedings\n",
      "of the 2024 Conference on Empirical Methods in\n",
      "Natural Language Processing , pages 4151–4171, Mi-\n",
      "ami, Florida, USA. Association for Computational\n",
      "Linguistics.\n",
      "Sian Gooding and Hassan Mansoor. 2023. The impact\n",
      "of preference agreement in reinforcement learning\n",
      "from human feedback: A case study in summariza-\n",
      "tion. Preprint , arXiv:2311.04919.\n",
      "Tanya Goyal, Junyi Jessy Li, and Greg Durrett. 2023.\n",
      "News summarization and evaluation in the era of\n",
      "gpt-3. Preprint , arXiv:2209.12356.\n",
      "Max Grusky, Mor Naaman, and Yoav Artzi. 2018.\n",
      "Newsroom: A dataset of 1.3 million summaries with\n",
      "diverse extractive strategies. In Proceedings of the\n",
      "2018 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies, Volume 1 (Long Pa-\n",
      "pers) , pages 708–719, New Orleans, Louisiana. As-\n",
      "sociation for Computational Linguistics.\n",
      "Zachary Horvitz, Ajay Patel, Kanishk Singh, Chris\n",
      "Callison-Burch, Kathleen McKeown, and Zhou Yu.\n",
      "2024. TinyStyler: Efficient few-shot text style trans-\n",
      "fer with authorship embeddings. In Findings of the\n",
      "Association for Computational Linguistics: EMNLP\n",
      "2024 , pages 13376–13390, Miami, Florida, USA.\n",
      "Association for Computational Linguistics.\n",
      "Cheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li,\n",
      "Zifeng Wang, Long Le, Abhishek Kumar, James\n",
      "Glass, Alexander Ratner, Chen-Yu Lee, Ranjay Kr-\n",
      "ishna, and Tomas Pfister. 2024. Found in the middle:\n",
      "Calibrating positional attention bias improves long\n",
      "context utilization. In Findings of the Association\n",
      "for Computational Linguistics: ACL 2024 , pages\n",
      "14982–14995, Bangkok, Thailand. Association for\n",
      "Computational Linguistics.\n",
      "Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao,\n",
      "Tianfu Wang, Zhengyu Chen, Nicholas Jing Yuan,\n",
      "Jianxun Lian, Kaize Ding, and Hui Xiong. 2024. Ex-\n",
      "plaining length bias in llm-based preference evalua-\n",
      "tions. Preprint , arXiv:2407.01085.\n",
      "\n",
      "Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi\n",
      "Wang, Hongkun Yu, and Jiawei Han. 2023. Large\n",
      "language models can self-improve. In Proceedings\n",
      "of the 2023 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing , pages 1051–1068, Singa-\n",
      "pore. Association for Computational Linguistics.\n",
      "Shengyi Huang, Michael Noukhovitch, Arian Hosseini,\n",
      "Kashif Rasul, Weixun Wang, and Lewis Tunstall.\n",
      "2024. The n+ implementation details of RLHF with\n",
      "PPO: A case study on TL;DR summarization. In\n",
      "First Conference on Language Modeling .\n",
      "Hayate Iso, Xiaolan Wang, Stefanos Angelidis, and\n",
      "Yoshihiko Suhara. 2022. Comparative opinion sum-\n",
      "marization via collaborative decoding. In Findings of\n",
      "the Association for Computational Linguistics: ACL\n",
      "2022 , pages 3307–3324, Dublin, Ireland. Association\n",
      "for Computational Linguistics.\n",
      "Athul Paul Jacob, Yikang Shen, Gabriele Farina, and\n",
      "Jacob Andreas. 2024. The consensus game: Lan-\n",
      "guage model generation via equilibrium search. In\n",
      "The Twelfth International Conference on Learning\n",
      "Representations .\n",
      "Sameer Jain, Vaishakh Keshava, Swarnashree\n",
      "Mysore Sathyendra, Patrick Fernandes, Pengfei\n",
      "Liu, Graham Neubig, and Chunting Zhou. 2023.\n",
      "Multi-dimensional evaluation of text summarization\n",
      "with in-context learning. In Findings of the Asso-\n",
      "ciation for Computational Linguistics: ACL 2023 ,\n",
      "pages 8487–8495, Toronto, Canada. Association for\n",
      "Computational Linguistics.\n",
      "Taehee Jung, Dongyeop Kang, Lucas Mentch, and Ed-\n",
      "uard Hovy. 2019. Earlier isn’t always better: Sub-\n",
      "aspect analysis on corpus and system biases in sum-\n",
      "marization. Preprint , arXiv:1908.11723.\n",
      "Wojciech Kryscinski, Bryan McCann, Caiming Xiong,\n",
      "and Richard Socher. 2020. Evaluating the factual\n",
      "consistency of abstractive text summarization. In\n",
      "Proceedings of the 2020 Conference on Empirical\n",
      "Methods in Natural Language Processing (EMNLP) ,\n",
      "pages 9332–9346, Online. Association for Computa-\n",
      "tional Linguistics.\n",
      "Philippe Laban, Tobias Schnabel, Paul N. Bennett, and\n",
      "Marti A. Hearst. 2022. SummaC: Re-visiting NLI-\n",
      "based models for inconsistency detection in summa-\n",
      "rization. Transactions of the Association for Compu-\n",
      "tational Linguistics , 10:163–177.\n",
      "Faisal Ladhak, Esin Durmus, He He, Claire Cardie, and\n",
      "Kathleen McKeown. 2022. Faithful or extractive?\n",
      "on mitigating the faithfulness-abstractiveness trade-\n",
      "off in abstractive summarization. In Proceedings\n",
      "of the 60th Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 1410–1421, Dublin, Ireland. Association for\n",
      "Computational Linguistics.\n",
      "Harrison Lee, Samrat Phatale, Hassan Mansoor, Kel-\n",
      "lie Ren Lu, Thomas Mesnard, Johan Ferret, Colton\n",
      "Bishop, Ethan Hall, Victor Carbune, and AbhinavRastogi. 2024. RLAIF: Scaling reinforcement learn-\n",
      "ing from human feedback with AI feedback.\n",
      "Nayeon Lee, Yejin Bang, Tiezheng Yu, Andrea Madotto,\n",
      "and Pascale Fung. 2022a. NeuS: Neutral multi-news\n",
      "summarization for mitigating framing bias. In Pro-\n",
      "ceedings of the 2022 Conference of the North Amer-\n",
      "ican Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies , pages\n",
      "3131–3148, Seattle, United States. Association for\n",
      "Computational Linguistics.\n",
      "Nayeon Lee, Yejin Bang, Tiezheng Yu, Andrea Madotto,\n",
      "and Pascale Fung. 2022b. NeuS: Neutral multi-news\n",
      "summarization for mitigating framing bias. In Pro-\n",
      "ceedings of the 2022 Conference of the North Amer-\n",
      "ican Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies , pages\n",
      "3131–3148, Seattle, United States. Association for\n",
      "Computational Linguistics.\n",
      "Yuanyuan Lei, Kaiqiang Song, Sangwoo Cho, Xiaoyang\n",
      "Wang, Ruihong Huang, and Dong Yu. 2024. Polarity\n",
      "calibration for opinion summarization. In Proceed-\n",
      "ings of the 2024 Conference of the North American\n",
      "Chapter of the Association for Computational Lin-\n",
      "guistics: Human Language Technologies (Volume\n",
      "1: Long Papers) , pages 5211–5224, Mexico City,\n",
      "Mexico. Association for Computational Linguistics.\n",
      "Chin-Yew Lin. 2004. ROUGE: A package for auto-\n",
      "matic evaluation of summaries. In Text Summariza-\n",
      "tion Branches Out , pages 74–81, Barcelona, Spain.\n",
      "Association for Computational Linguistics.\n",
      "Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\n",
      "jape, Michele Bevilacqua, Fabio Petroni, and Percy\n",
      "Liang. 2024a. Lost in the middle: How language\n",
      "models use long contexts. Transactions of the Asso-\n",
      "ciation for Computational Linguistics , 12:157–173.\n",
      "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\n",
      "Ruochen Xu, and Chenguang Zhu. 2023. G-eval:\n",
      "NLG evaluation using gpt-4 with better human align-\n",
      "ment. In Proceedings of the 2023 Conference on\n",
      "Empirical Methods in Natural Language Processing ,\n",
      "pages 2511–2522, Singapore. Association for Com-\n",
      "putational Linguistics.\n",
      "Yuhan Liu, Shangbin Feng, Xiaochuang Han, Vidhisha\n",
      "Balachandran, Chan Young Park, Sachin Kumar, and\n",
      "Yulia Tsvetkov. 2024b. P3Sum: Preserving author‘s\n",
      "perspective in news summarization with diffusion\n",
      "language models. In Proceedings of the 2024 Con-\n",
      "ference of the North American Chapter of the Asso-\n",
      "ciation for Computational Linguistics: Human Lan-\n",
      "guage Technologies (Volume 1: Long Papers) , pages\n",
      "2154–2173, Mexico City, Mexico. Association for\n",
      "Computational Linguistics.\n",
      "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler\n",
      "Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\n",
      "Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,\n",
      "Shashank Gupta, Bodhisattwa Prasad Majumder,\n",
      "\n",
      "Katherine Hermann, Sean Welleck, Amir Yazdan-\n",
      "bakhsh, and Peter Clark. 2023. Self-refine: Itera-\n",
      "tive refinement with self-feedback. In Thirty-seventh\n",
      "Conference on Neural Information Processing Sys-\n",
      "tems.\n",
      "Joshua Maynez, Shashi Narayan, Bernd Bohnet, and\n",
      "Ryan McDonald. 2020. On faithfulness and factu-\n",
      "ality in abstractive summarization. In Proceedings\n",
      "of the 58th Annual Meeting of the Association for\n",
      "Computational Linguistics , pages 1906–1919, On-\n",
      "line. Association for Computational Linguistics.\n",
      "Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\n",
      "Long Ouyang, Christina Kim, Christopher Hesse,\n",
      "Shantanu Jain, Vineet Kosaraju, William Saunders,\n",
      "Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\n",
      "Krueger, Kevin Button, Matthew Knight, Benjamin\n",
      "Chess, and John Schulman. 2022. Webgpt: Browser-\n",
      "assisted question-answering with human feedback.\n",
      "Preprint , arXiv:2112.09332.\n",
      "Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu,\n",
      "Patrick Ng, Kathleen McKeown, Ramesh Nallapati,\n",
      "Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and\n",
      "Bing Xiang. 2021. Improving factual consistency\n",
      "of abstractive summarization via question answering.\n",
      "InProceedings of the 59th Annual Meeting of the\n",
      "Association for Computational Linguistics and the\n",
      "11th International Joint Conference on Natural Lan-\n",
      "guage Processing (Volume 1: Long Papers) , pages\n",
      "6881–6894, Online. Association for Computational\n",
      "Linguistics.\n",
      "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\n",
      "Carroll Wainwright, Pamela Mishkin, Chong Zhang,\n",
      "Sandhini Agarwal, Katarina Slama, Alex Ray, John\n",
      "Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\n",
      "Maddie Simens, Amanda Askell, Peter Welinder,\n",
      "Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.\n",
      "Training language models to follow instructions with\n",
      "human feedback. In Advances in Neural Information\n",
      "Processing Systems , volume 35, pages 27730–27744.\n",
      "Curran Associates, Inc.\n",
      "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-\n",
      "Jing Zhu. 2002. Bleu: a method for automatic evalu-\n",
      "ation of machine translation. In Proceedings of the\n",
      "40th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 311–318, Philadelphia,\n",
      "Pennsylvania, USA. Association for Computational\n",
      "Linguistics.\n",
      "Letitia Parcalabescu and Anette Frank. 2024. On mea-\n",
      "suring faithfulness or self-consistency of natural lan-\n",
      "guage explanations. In Proceedings of the 62nd An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 6048–\n",
      "6089, Bangkok, Thailand. Association for Computa-\n",
      "tional Linguistics.\n",
      "Maja Popovi ´c. 2015. chrF: character n-gram F-score\n",
      "for automatic MT evaluation. In Proceedings of the\n",
      "Tenth Workshop on Statistical Machine Translation ,\n",
      "pages 392–395, Lisbon, Portugal. Association for\n",
      "Computational Linguistics.Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\n",
      "Noah Smith, and Mike Lewis. 2023. Measuring and\n",
      "narrowing the compositionality gap in language mod-\n",
      "els. In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2023 , pages 5687–5711, Singa-\n",
      "pore. Association for Computational Linguistics.\n",
      "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\n",
      "pher D Manning, Stefano Ermon, and Chelsea Finn.\n",
      "2023. Direct preference optimization: Your language\n",
      "model is secretly a reward model. In Thirty-seventh\n",
      "Conference on Neural Information Processing Sys-\n",
      "tems.\n",
      "Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,\n",
      "Inbal Magar, Omri Abend, Ehud Karpas, Amnon\n",
      "Shashua, Kevin Leyton-Brown, and Yoav Shoham.\n",
      "2023. Parallel context windows for large language\n",
      "models. In Proceedings of the 61st Annual Meet-\n",
      "ing of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers) , pages 6383–6402, Toronto,\n",
      "Canada. Association for Computational Linguistics.\n",
      "Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Ge-\n",
      "offrey Cideron, Robert Dadashi, Matthieu Geist, Ser-\n",
      "tan Girgin, Leonard Hussenot, Orgad Keller, Nikola\n",
      "Momchev, Sabela Ramos Garea, Piotr Stanczyk,\n",
      "Nino Vieillard, Olivier Bachem, Gal Elidan, Avinatan\n",
      "Hassidim, Olivier Pietquin, and Idan Szpektor. 2023.\n",
      "Factually consistent summarization via reinforce-\n",
      "ment learning with textual entailment feedback. In\n",
      "Proceedings of the 61st Annual Meeting of the As-\n",
      "sociation for Computational Linguistics (Volume 1:\n",
      "Long Papers) , pages 6252–6272, Toronto, Canada.\n",
      "Association for Computational Linguistics.\n",
      "Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit\n",
      "Bansal, Jason Weston, and Xian Li. 2024. Branch-\n",
      "solve-merge improves large language model evalu-\n",
      "ation and generation. In Proceedings of the 2024\n",
      "Conference of the North American Chapter of the\n",
      "Association for Computational Linguistics: Human\n",
      "Language Technologies (Volume 1: Long Papers) ,\n",
      "pages 8352–8370, Mexico City, Mexico. Association\n",
      "for Computational Linguistics.\n",
      "Abigail See, Peter J. Liu, and Christopher D. Manning.\n",
      "2017. Get to the point: Summarization with pointer-\n",
      "generator networks. In Proceedings of the 55th An-\n",
      "nual Meeting of the Association for Computational\n",
      "Linguistics (Volume 1: Long Papers) , pages 1073–\n",
      "1083, Vancouver, Canada. Association for Computa-\n",
      "tional Linguistics.\n",
      "Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\n",
      "BLEURT: Learning robust metrics for text genera-\n",
      "tion. In Proceedings of the 58th Annual Meeting of\n",
      "the Association for Computational Linguistics , pages\n",
      "7881–7892, Online. Association for Computational\n",
      "Linguistics.\n",
      "Noah Siegel, Oana-Maria Camburu, Nicolas Heess, and\n",
      "Maria Perez-Ortiz. 2024. The probabilities also mat-\n",
      "ter: A more faithful metric for faithfulness of free-\n",
      "text explanations in large language models. In Pro-\n",
      "\n",
      "ceedings of the 62nd Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics (Volume 2: Short\n",
      "Papers) , pages 530–546, Bangkok, Thailand. Associ-\n",
      "ation for Computational Linguistics.\n",
      "Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai,\n",
      "and Saab Mansour. 2024. FineSurE: Fine-grained\n",
      "summarization evaluation using LLMs. In Proceed-\n",
      "ings of the 62nd Annual Meeting of the Association\n",
      "for Computational Linguistics (Volume 1: Long Pa-\n",
      "pers) , pages 906–922, Bangkok, Thailand. Associa-\n",
      "tion for Computational Linguistics.\n",
      "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\n",
      "Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,\n",
      "Dario Amodei, and Paul F Christiano. 2020. Learn-\n",
      "ing to summarize with human feedback. In Ad-\n",
      "vances in Neural Information Processing Systems ,\n",
      "volume 33, pages 3008–3021. Curran Associates,\n",
      "Inc.\n",
      "Mirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf-\n",
      "sky. 2022. Prompt-and-rerank: A method for zero-\n",
      "shot and few-shot arbitrary textual style transfer with\n",
      "small language models. In Proceedings of the 2022\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing , pages 2195–2222, Abu Dhabi,\n",
      "United Arab Emirates. Association for Computa-\n",
      "tional Linguistics.\n",
      "Liyan Tang, Philippe Laban, and Greg Durrett. 2024.\n",
      "MiniCheck: Efficient fact-checking of LLMs on\n",
      "grounding documents. In Proceedings of the 2024\n",
      "Conference on Empirical Methods in Natural Lan-\n",
      "guage Processing , pages 8818–8847, Miami, Florida,\n",
      "USA. Association for Computational Linguistics.\n",
      "Ashwin Vijayakumar, Michael Cogswell, Ramprasaath\n",
      "Selvaraju, Qing Sun, Stefan Lee, David Crandall,\n",
      "and Dhruv Batra. 2018. Diverse beam search for\n",
      "improved description of complex scenes. Proceed-\n",
      "ings of the AAAI Conference on Artificial Intelligence ,\n",
      "32(1).\n",
      "Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\n",
      "Asking and answering questions to evaluate the fac-\n",
      "tual consistency of summaries. In Proceedings of the\n",
      "58th Annual Meeting of the Association for Compu-\n",
      "tational Linguistics , pages 5008–5020, Online. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\n",
      "Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\n",
      "and Denny Zhou. 2023a. Self-consistency improves\n",
      "chain of thought reasoning in language models. In\n",
      "The Eleventh International Conference on Learning\n",
      "Representations .\n",
      "Yiming Wang, Zhuosheng Zhang, and Rui Wang. 2023b.\n",
      "Element-aware summarization with large language\n",
      "models: Expert-aligned evaluation and chain-of-\n",
      "thought method. In Proceedings of the 61st Annual\n",
      "Meeting of the Association for Computational Lin-\n",
      "guistics (Volume 1: Long Papers) , pages 8640–8665,\n",
      "Toronto, Canada. Association for Computational Lin-\n",
      "guistics.Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang,\n",
      "Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng,\n",
      "and Heng Ji. 2024. Eliminating position bias of lan-\n",
      "guage models: A mechanistic approach. Preprint ,\n",
      "arXiv:2407.01100.\n",
      "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\n",
      "Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\n",
      "and Denny Zhou. 2022. Chain of thought prompt-\n",
      "ing elicits reasoning in large language models. In\n",
      "Advances in Neural Information Processing Systems .\n",
      "Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\n",
      "Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\n",
      "2023. Large language models are better reasoners\n",
      "with self-verification. In Findings of the Associa-\n",
      "tion for Computational Linguistics: EMNLP 2023 ,\n",
      "pages 2550–2575, Singapore. Association for Com-\n",
      "putational Linguistics.\n",
      "Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita\n",
      "Bhutani, and Estevam Hruschka. 2024. Less is more\n",
      "for long document summary evaluation by LLMs. In\n",
      "Proceedings of the 18th Conference of the European\n",
      "Chapter of the Association for Computational Lin-\n",
      "guistics (Volume 2: Short Papers) , pages 330–343,\n",
      "St. Julian’s, Malta. Association for Computational\n",
      "Linguistics.\n",
      "Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj\n",
      "Juraska, Biao Zhang, Zhongtao Liu, William Yang\n",
      "Wang, Lei Li, and Markus Freitag. 2024. LLMRefine:\n",
      "Pinpointing and refining large language models via\n",
      "fine-grained actionable feedback. In Findings of the\n",
      "Association for Computational Linguistics: NAACL\n",
      "2024 , pages 1429–1445, Mexico City, Mexico. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu.\n",
      "2023. AlignScore: Evaluating factual consistency\n",
      "with a unified alignment function. In Proceedings\n",
      "of the 61st Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 11328–11348, Toronto, Canada. Association\n",
      "for Computational Linguistics.\n",
      "Shiyue Zhang and Mohit Bansal. 2021. Finding a bal-\n",
      "anced degree of automation for summary evaluation.\n",
      "InProceedings of the 2021 Conference on Empiri-\n",
      "cal Methods in Natural Language Processing , pages\n",
      "6617–6632, Online and Punta Cana, Dominican Re-\n",
      "public. Association for Computational Linguistics.\n",
      "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\n",
      "Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-\n",
      "uating text generation with bert. In International\n",
      "Conference on Learning Representations .\n",
      "Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\n",
      "Kathleen McKeown, and Tatsunori B. Hashimoto.\n",
      "2024a. Benchmarking large language models for\n",
      "news summarization. Transactions of the Association\n",
      "for Computational Linguistics , 12:39–57.\n",
      "Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying\n",
      "Peng, Jun Wang, Yueting Zhuang, and Weiming\n",
      "\n",
      "Given texts from both Left-leaning and Right-leaning\n",
      "perspectives, summarize only the Left-leaning perspective in\n",
      "one sentence, starting with 'The Left '. ONLY RETURN THE\n",
      "SUMMARY AND NOTHING ELSE.\n",
      "Left:\n",
      "(left-perspective article)\n",
      "Right:\n",
      "(right-perspective article)\n",
      "Figure 5: Prompt instruction for zero-shot inference\n",
      "when generating summaries from the left-leaning per-\n",
      "spective.\n",
      "Lu. 2024b. Self-contrast: Better reflection through\n",
      "inconsistent solving perspectives. In Proceedings\n",
      "of the 62nd Annual Meeting of the Association for\n",
      "Computational Linguistics (Volume 1: Long Papers) ,\n",
      "pages 3602–3622, Bangkok, Thailand. Association\n",
      "for Computational Linguistics.\n",
      "Yusen Zhang, Nan Zhang, Yixin Liu, Alexander Fabbri,\n",
      "Junru Liu, Ryo Kamoi, Xiaoxin Lu, Caiming Xiong,\n",
      "Jieyu Zhao, Dragomir Radev, Kathleen McKeown,\n",
      "and Rui Zhang. 2024c. Fair abstractive summariza-\n",
      "tion of diverse perspectives. In Proceedings of the\n",
      "2024 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Hu-\n",
      "man Language Technologies (Volume 1: Long Pa-\n",
      "pers) , pages 3404–3426, Mexico City, Mexico. Asso-\n",
      "ciation for Computational Linguistics.\n",
      "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\n",
      "Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\n",
      "Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\n",
      "Joseph E. Gonzalez, and Ion Stoica. 2023. Judging\n",
      "LLM-as-a-judge with MT-bench and chatbot arena.\n",
      "InThirty-seventh Conference on Neural Information\n",
      "Processing Systems Datasets and Benchmarks Track .\n",
      "Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\n",
      "Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\n",
      "Jiawei Han. 2022. Towards a unified multi-\n",
      "dimensional evaluator for text generation. In Pro-\n",
      "ceedings of the 2022 Conference on Empirical Meth-\n",
      "ods in Natural Language Processing , pages 2023–\n",
      "2038, Abu Dhabi, United Arab Emirates. Association\n",
      "for Computational Linguistics.\n",
      "A Supplementary Details\n",
      "A.1 Experimental Setup\n",
      "Unless otherwise specified, all inference is run\n",
      "using the transformers library with 16-bit float-\n",
      "ing point precision using FLASH ATTENTION 2\n",
      "(Dao, 2023). We train the DPO-based models on\n",
      "four NVIDIA A100-SXM4-80GB GPUs, with each\n",
      "model requiring approximately 2 ∼3 days of train-\n",
      "ing. For other experiments, including inference and\n",
      "evaluation using small-scale language models, we\n",
      "use a variable number of NVIDIA A100-SXM4-\n",
      "80GB GPUs depending on the model size. Forcompleteness, we also provide the prompt instruc-\n",
      "tion for the zero-shot inference setting in Figure 5.\n",
      "DPO Training. We train our DPO-based mod-\n",
      "els using 4 batches and the default hyperparam-\n",
      "eter settings from the DPOConfig class in the\n",
      "transformers library. This corresponds to using\n",
      "an (adaptive) learning rate of 5.0×10−5, aβvalue\n",
      "of0.1, and reverse KL divergence for f-divergence\n",
      "regularization.\n",
      "PINE. We use the codebase available at github.\n",
      "com/wzq016/PINE.git for the PINE implementa-\n",
      "tion. In our setup, the input is formatted as\n",
      "[INS|d(1)\n",
      "t,θ1|d(2)\n",
      "t,θ1|. . .|d(1)\n",
      "t,θ2|d(2)\n",
      "t,θ2|. . .|EOS],\n",
      "where INSis the prompt instruction, Dt,θ1=d(1)\n",
      "t,θ1|\n",
      "d(2)\n",
      "t,θ1|. . .represents the left-leaning source docu-\n",
      "ments, Dt,θ2=d(1)\n",
      "t,θ2|d(2)\n",
      "t,θ2|. . .the right-leaning\n",
      "source documents, and EOSis the end-of-sequence\n",
      "token. PINE reformats the input by designating\n",
      "a target segment (e.g., Dt,θ1when the target per-\n",
      "spective is the left-leaning view) to ensure that all\n",
      "segments are attended to uniformly, regardless of\n",
      "their original positions.\n",
      "Dataset. We use the POLISUMdataset (Deas and\n",
      "McKeown, 2025) as our primary testbed for per-\n",
      "spective summarization. We remove duplicates\n",
      "from the dataset and obtain 1816 article pairs (left-\n",
      "and right-leaning). We split the data into 1716 arti-\n",
      "cle pairs for training DPO+RR and 100 article pairs\n",
      "for testing. Although most methods we investigate\n",
      "do not rely on training, we maintain a strict sepa-\n",
      "ration between train and test sets to avoid inflating\n",
      "DPO+RR performance.\n",
      "A.2 Ranking Methods\n",
      "To obtain accurate ranking results using automated\n",
      "metrics, we fit a Bradley-Terry model (Bradley and\n",
      "Terry, 1952) to the per-method scores for each boot-\n",
      "strap resample of the test set and derive confidence\n",
      "intervals for each method’s ability estimate.\n",
      "Specifically, let there be Mmethods with latent\n",
      "abilities {θ1, θ2, . . . , θ M}. For each pair of meth-\n",
      "ods(i, j), the model posits that the probability of i\n",
      "\"winning\" over jin a pairwise comparison is given\n",
      "by a logistic function:\n",
      "Pr[ibeats j] =1\n",
      "1 + exp\u0000\n",
      "−(θi−θj)/σ\u0001,(3)\n",
      "where σ > 0is a noise or scale parameter. We\n",
      "treat method ias having beaten method jifi’s\n",
      "\n",
      "aggregated raw score exceeds j’s, resolving exact\n",
      "ties randomly.\n",
      "We estimate the abilities by maximizing the log-\n",
      "likelihood of all observed pairwise outcomes:\n",
      "ℓ(θ1, . . . , θ M) =X\n",
      "(i,j)∈Dh\n",
      "1[ibeats j] log Pr[ ibeats j]\n",
      "+ 1[jbeats i] log\u0000\n",
      "1−Pr[ibeats j]\u0001i\n",
      ",\n",
      "whereDdenotes all pairwise comparisons from the\n",
      "current (re)sample, and 1(·)is an indicator func-\n",
      "tion. We perform this fitting procedure via numer-\n",
      "ical optimization (L-BFGS). To account for vari-\n",
      "ability, we employ bootstrap resampling over the\n",
      "test set: each resample draws the test documents\n",
      "(with replacement), averages each method’s raw\n",
      "scores within that resample, and re-fits the Bradley-\n",
      "Terry model to generate a new set of abilities {θm}.\n",
      "We repeat for B= 500 iterations and obtain an\n",
      "empirical distribution of ability estimates for each\n",
      "method. We then rank methods by their mean es-\n",
      "timated ability across all bootstrap replicates and\n",
      "derive 95% confidence intervals from the resulting\n",
      "bootstrap distributions.\n",
      "B Metric Evaluation\n",
      "Here, we provide supplementary content for bench-\n",
      "marking evaluation metrics for measuring coverage\n",
      "and faithfulness.\n",
      "B.1 Metric Configurations\n",
      "ForROUGE , we use the rouge-score Python\n",
      "library. For BERTS CORE and BLEURT , we\n",
      "use the deberta-large-xnli andBLEURT-20-D6\n",
      "checkpoints respectively, due to their higher cor-\n",
      "relations with human judgments. For ALIGN -\n",
      "SCORE , we employ the AlignScore-large check-\n",
      "point from Zha et al. (2023). For SUMMA C, we\n",
      "use the tals/albert-xlarge-vitaminc-mnli\n",
      "model, which is the default setting for the SUM-\n",
      "MAC evaluation metric.\n",
      "B.2 Prompt Instructions\n",
      "Prompt-based Scoring: LLM-Coverage and\n",
      "LLM-Faithfulness. We provide the full prompt\n",
      "instructions for both LLM-Coverage and LLM-\n",
      "Faithfulness in Figures 6b and 6a, respectively.\n",
      "While we experiment with prompt variations such\n",
      "as using binary and ternary scoring and removing\n",
      "step-by-step procedures, these modifications result\n",
      "in lower performance. We omit these alternate\n",
      "prompts for brevity.B.3 Backbone Scoring Evaluation\n",
      "Table 7 presents additional results for various LLM\n",
      "backbones. Notably, prompt-based scoring gener-\n",
      "ally performs better on coverage than on faithful-\n",
      "ness. In particular, Mistral-7B-Instruct-v0.3\n",
      "andQwen2.5-14B-Instruct exhibit the best per-\n",
      "formance across both metrics. Based on these\n",
      "results, we use Mistral-7B-Instruct-v0.3 as\n",
      "the evaluator and Qwen2.5-14B-Instruct as the\n",
      "proxy metric. For coverage, larger model sizes\n",
      "weakly correlate with higher performance, though\n",
      "the gains are marginal. To keep inference time rea-\n",
      "sonable, we therefore use smaller-scale models that\n",
      "still exhibit good performance. For faithfulness,\n",
      "theLlama models consistently underperform com-\n",
      "pared to other backbones on both correlation and\n",
      "ranking. However, as all backbones perform close\n",
      "to the random baseline on winrate, we avoid using\n",
      "prompt-based scoring for faithfulness.\n",
      "B.4 Paraphrasing Excerpts to Key Points\n",
      "As mentioned in §3.1, we use an LLM to para-\n",
      "phrase highlighted excerpts into key points. We\n",
      "also employ an LLM to generate adversarial key\n",
      "points Kt,θfrom the curated key points Kt,θ, using\n",
      "the prompts provided in Figures 7a and 7b. We\n",
      "useQwen2.5-32B-Instruct for both paraphras-\n",
      "ing and key point generation.\n",
      "C Benchmarking Methods\n",
      "Here, we provide supplementary details on our eval-\n",
      "uation procedure along with additional analysis on\n",
      "the generated summaries by each method.\n",
      "C.1 Inter-Annotator Agreement\n",
      "We first provide additional information on the\n",
      "matching function M(·,·)in §5.2. For each el-\n",
      "ement sA\n",
      "i∈SA, the function finds the first un-\n",
      "matched element sB\n",
      "j∈SBthat meets a matching\n",
      "condition. The first criterion is exact containment:\n",
      "ifsA\n",
      "iis a substring of sB\n",
      "jor vice versa, they are\n",
      "considered a match. If no exact containment is\n",
      "found, we compute the longest common subse-\n",
      "quence (LCS) between sA\n",
      "iandsB\n",
      "j. If the LCS\n",
      "length divided by the length of the shorter string\n",
      "exceeds a predefined threshold τ, we consider them\n",
      "a match. Each element in SAis matched to at most\n",
      "one element in SB, and vice versa, and is removed\n",
      "from further matching once paired. By default, we\n",
      "setτ= 0.5.\n",
      "Random Baseline for IAA. We simulate random\n",
      "highlight selection as follows. First, we compute\n",
      "the mean and variance of the number of highlights\n",
      "\n",
      "Metric Model Coverage Faithfulness\n",
      "Corr. ( ρs) Winrate Corr. ( ρs) Winrate\n",
      "LLM-CoverageMistral-7B-Instruct-v0.3 0.707∗∗∗0.739±0.047 0.393∗∗∗0.431±0.115\n",
      "Mixtral-8x7B-Instruct-v0.1 0.720∗∗∗0.771±0.050 0.335∗∗∗0.475±0.087\n",
      "Llama-3.1-8B-Instruct 0.606∗∗∗0.648±0.051 0.188∗∗∗0.313±0.093\n",
      "Llama-3.3-70B-Instruct 0.724∗∗∗0.753±0.058 0.280∗∗∗0.415±0.100\n",
      "Qwen2.5-7B-Instruct 0.650∗∗∗0.624±0.081 0.343∗∗∗0.349±0.106\n",
      "Qwen2.5-14B-Instruct 0.732∗∗∗0.749±0.049 0.334∗∗∗0.380±0.081\n",
      "Qwen2.5-32B-Instruct 0.721∗∗∗0.709±0.060 0.302∗∗∗0.343±0.097\n",
      "LLM-FaithfulnessMistral-7B-Instruct-v0.3 0.504∗∗∗0.494±0.061 0.646∗∗∗0.498±0.113\n",
      "Mistral-Large-Instruct-2411 0.722∗∗∗0.688±0.076 0.579∗∗∗0.479±0.108\n",
      "Llama-3.1-8B-Instruct 0.577∗∗∗0.303±0.074 0.439∗∗∗0.188±0.079\n",
      "Llama-3.3-70B-Instruct 0.558∗∗∗0.283±0.080 0.735∗∗∗0.343±0.112\n",
      "Qwen2.5-7B-Instruct 0.589∗∗∗0.536±0.064 0.644∗∗∗0.503±0.087\n",
      "Qwen2.5-14B-Instruct 0.702∗∗∗0.671±0.099 0.616∗∗∗0.519±0.086\n",
      "Qwen2.5-32B-Instruct 0.712∗∗∗0.675±0.063 0.670∗∗∗0.590±0.096\n",
      "Table 7: Comparison of Spearman rank correlation ( Corr. ( ρs)) and Winrate ( Winr. ) across different back-\n",
      "bone models. LLM-Coverage exhibits moderate to high correlation and winrate across all backbones, while\n",
      "Mistral-7B-Instruct-v0.3 andQwen2.5-14B-Instruct achieve the best performance for faithfulness.\n",
      "Method Summary Length EF Coverage Comp. Ratio\n",
      "Zero-Shot 40.77±6.212 0.719±0.113 14.958±4.165\n",
      "Self-Refine 43.94±10.73 0.692±0.107 15.097±5.774\n",
      "Debate 41.50±11.609 0.692±0.103 16.192±4.903\n",
      "PINE 38.17±7.401 0.776±0.125 19.589±20.23\n",
      "Reranking 37.13±13.856 0.651±0.157 14.166±4.181\n",
      "DPO+RR 42.94±8.245 0.661±0.149 14.391±4.421\n",
      "Table 8: Supplementary statistics for each method, mea-\n",
      "sured by summary length, extractive fragment coverage,\n",
      "and compression ratio.\n",
      "and their lengths separately for documents and sum-\n",
      "maries. Using these statistics, we sample the num-\n",
      "ber of highlights and the length of each highlight\n",
      "from a normal distribution N(·,·)for each docu-\n",
      "ment or summary. We repeat this process indepen-\n",
      "dently twice and compute the overlap between the\n",
      "two instances as described in §5.2. This procedure\n",
      "simulates a non-trivial, semi-realistic random high-\n",
      "lighting of excerpts in documents and summaries.\n",
      "C.2 Supplementary Analysis\n",
      "In addition to coverage-density plots, we report\n",
      "additional results for summary lengths, extrac-\n",
      "tive fragment coverage (quantifying the extent of\n",
      "copying from the source), and compression ratios\n",
      "(Grusky et al., 2018) (assessing summary length\n",
      "relative to the source document).\n",
      "Table 8 shows our results. Consistent with find-\n",
      "ings in §6.1, PINE exhibits lower abstractiveness\n",
      "compared to other methods. Moreover, both com-\n",
      "pression ratios and summary lengths indicate that\n",
      "PINE tends to generate shorter summaries relative\n",
      "to other methods.Extractive Fragment Plots. We also include\n",
      "Coverage-Density plots for both the source and\n",
      "opposing perspective documents in Figure 8. Over-\n",
      "all, we observe similar coverage-density structures\n",
      "for all by PINE, which exhibits a wider spectrum\n",
      "of coverage and density. This indicates that the\n",
      "abstractiveness of PINE exhibits high variance,\n",
      "whereas for other methods the abstractiveness is\n",
      "relatively stable. Furthermore, we also see that\n",
      "the coverage-density plots for the opposing side is\n",
      "slightly lower than for the target source articles.\n",
      "C.3 Additional Example Summaries\n",
      "In Table 9, we provide additional sampled exam-\n",
      "ples for the summaries generated by each method.\n",
      "D Annotation Information\n",
      "D.1 Annotation Details\n",
      "For both annotation procedures, annotators con-\n",
      "sented to having their annotated excerpts used for\n",
      "research purposes (cf. Figures 9 and 11). All hu-\n",
      "man evaluations in this work were conducted under\n",
      "an approved IRB protocol.\n",
      "Test Set for Metric Evaluation. We recruited 5\n",
      "graduate annotators, each assigned 10 documents\n",
      "for excerpt highlighting. Each annotator received\n",
      "$15 as compensation.\n",
      "Summary Evaluation. Annotators were re-\n",
      "cruited from undergraduate Political Science stu-\n",
      "dents with self-reported knowledge of conservative\n",
      "and liberal beliefs to ensure the required expertise\n",
      "to judge summary perspectives. Four annotators\n",
      "\n",
      "You are an evaluator. Your task is to determine how\n",
      "well a generated summary captures all of the main\n",
      "arguments from a source article. This is a measure of \"\n",
      "coverage,\" which does not necessarily address factual\n",
      "accuracy (faithfulness) but focuses on completeness of\n",
      "content.\n",
      "The scale for coverage is:\n",
      "1. No Coverage: The summary does not include any of the\n",
      "main arguments from the article.\n",
      "2. Low Coverage: The summary includes only a few of the\n",
      "main arguments from the article, omitting most.\n",
      "3. Medium Coverage: The summary contains around half of\n",
      "the article 's main arguments.\n",
      "4. High Coverage: The summary contains most of the main\n",
      "arguments from the article, missing only a few.\n",
      "5. Perfect Coverage: The summary includes all major\n",
      "points and arguments mentioned in the article, leaving\n",
      "out nothing important.\n",
      "Follow these steps carefully:\n",
      "1. **Read the Source Article**: Examine the text\n",
      "provided in the article. Identify all major points,\n",
      "arguments, or facts it contains.\n",
      "2. **Read the Summary**: Look at the text in the\n",
      "summary. List each argument or point the summary\n",
      "includes.\n",
      "3. **Compare for Completeness**:\n",
      "- Check if each major point from the source article is\n",
      "present in the summary.\n",
      "- Count how many major points are covered versus how\n",
      "many are omitted.\n",
      "4. **Determine the Score**:\n",
      "- Assign a score from 1 (no coverage) to 5 (perfect\n",
      "coverage), based on how many main arguments are\n",
      "included in the summary relative to the source.\n",
      "5. **Output Instructions**:\n",
      "- Output only the final numeric score (1, 2, 3, 4, or 5)\n",
      "without any explanation or additional text.\n",
      "---\n",
      "# Source Article:\n",
      "(article)\n",
      "# Generated Summary:\n",
      "(summary)\n",
      "# Final Coverage Score (1~5 only):\n",
      "(a) Full prompt instructions for LLM-Coverage.You are an evaluator. Your task is to analyze how faithfully a\n",
      "generated summary represents the information found in the source\n",
      "article. Faithfulness here means the absence of factual errors---\n",
      "i.e., any claims in the summary must be either directly stated,\n",
      "heavily implied, or logically entailed by the source article.\n",
      "The scale for faithfulness is:\n",
      "1. Unfaithful: The summary is almost entirely incorrect or\n",
      "unrelated to the source.\n",
      "2. Mostly Unfaithful: The summary includes only a few relevant\n",
      "arguments or correct details but is largely incorrect or missing.\n",
      "3. Neutral: The summary has about half of the important points\n",
      "correct, but also includes notable inaccuracies or omissions.\n",
      "4. Mostly Faithful: The summary reflects most of the article 's\n",
      "arguments accurately, with only minor errors or omissions.\n",
      "5. Perfectly Faithful: The summary includes all of the article 's\n",
      "main arguments accurately and does not introduce any\n",
      "contradictory or unsupported claims.\n",
      "Follow these steps carefully:\n",
      "1. **Read the Source Article**: Examine the text provided in the\n",
      "article. Identify the main points, arguments, or facts it\n",
      "contains.\n",
      "2. **Read the Summary**: Look at the text in the summary. Itemize\n",
      "or note each claim or statement made in the summary.\n",
      "3. **Compare for Accuracy**:\n",
      "- Check if each claim in the summary is explicitly or logically\n",
      "supported by the source.\n",
      "- Mark any claim that appears to be contradicting the source or\n",
      "not found in the source.\n",
      "- Check if the summary omits major arguments that are central to\n",
      "the source.\n",
      "4. **Determine the Score**:\n",
      "- Assign a score from 1 (completely unfaithful) to 5 (perfectly\n",
      "faithful), based on how many claims match (and do not contradict)\n",
      "the source article and whether key points are included.\n",
      "5. **Output Instructions**:\n",
      "- Output only the final numeric score (1, 2, 3, 4, or 5) without\n",
      "any additional explanation or text.\n",
      "---\n",
      "# Source Article:\n",
      "(article)\n",
      "# Generated Summary:\n",
      "(summary)\n",
      "# Final Faithfulness Score (1~5 only):\n",
      "(b) Full prompt instructions for LLM-Faithfulness.\n",
      "Figure 6: Complete prompt instructions for both attributes in prompting-based scoring. The model is provided with\n",
      "descriptions of each score value and a step-by-step procedure for evaluating the summary based on the article.\n",
      "participated—three annotated 20 documents each\n",
      "and one annotated 15. To measure inter-annotator\n",
      "agreement, overlapping annotations were collected\n",
      "for 10 documents, with each document annotated\n",
      "by two annotators. This process yielded a total\n",
      "of 75 document-summary annotations per method.\n",
      "Annotators were compensated at $22.50 per hour\n",
      "and spent approximately 15±2.5minutes per page.\n",
      "D.2 Annotation Interfaces\n",
      "We provide the annotation interfaces for the hu-\n",
      "man studies described in §3.1 and §4.1 in Fig-\n",
      "ures 9 and 10 (for metric evaluation) and Figures 11\n",
      "and 12 (for summary evaluation). Both interfaces\n",
      "were built using the streamlit Python library and\n",
      "hosted on the Streamlit Community Cloud plat-form. Annotator results were stored using Amazon\n",
      "Web Services (AWS) Simple Storage Service (S3).\n",
      "\n",
      "[TASK]\n",
      "You are given an article that makes an\n",
      "argument related to the provided topic. An\n",
      "excerpt from the document highlights the main\n",
      "key argument that the author of the article is\n",
      "trying to assert. Please write a concise,\n",
      "short, one-sentence paraphrase (as short as\n",
      "possible) that reflects the argument implied\n",
      "or present in the provided excerpt. **Your\n",
      "paraphrase should begin with \"The article\n",
      "argues\"**.\n",
      "---\n",
      "Topic: (topic)\n",
      "Article: (article)\n",
      "Excerpt: (excerpt)\n",
      "---\n",
      "One-Line Argument Summary starting with \"The\n",
      "article argues\":\n",
      "(a) Full prompt instructions for paraphrasing\n",
      "highlighted excerpts to key points.[TASK]\n",
      "You are given one main argument from a political news article (either left-\n",
      "leaning or right-leaning). **Rewrite the argument so that the argument is\n",
      "completely reversed or semantically opposite.** If the original argument\n",
      "supports or praises a policy/idea/group, the reversed version should\n",
      "criticize or oppose it, and vice versa. Only return the reversed argument\n",
      "itself, with no extra commentary or analysis.\n",
      "[EXAMPLES]\n",
      "1.\n",
      "ORIGINAL: The article argues that stricter immigration laws help protect\n",
      "domestic jobs and strengthen national identity.\n",
      "REVERSED: The article argues that relaxed immigration laws create more job\n",
      "opportunities and enhance cultural diversity.\n",
      "2.\n",
      "ORIGINAL: The article insists that climate change is primarily caused by\n",
      "human activity and demands immediate government intervention.\n",
      "REVERSED: The article insists that human activity has minimal impact on\n",
      "climate change and calls for minimal government involvement.\n",
      "[INFERENCE]\n",
      "ORIGINAL: (original key point)\n",
      "REVERSED:\n",
      "(b) Prompt for generating adversarial key points Kt,θfrom the curated key\n",
      "points Kt,θ.\n",
      "Figure 7: Prompts used for portions of the procedure for curating the benchmarking test set for metrics.\n",
      "0246810Zero-Shot PINE Reranking\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00246810Self-Refine\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0DPO\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Debate\n",
      "CoverageDensity\n",
      "(a) Coverage-Density plots for the source document.\n",
      "0246810Zero-Shot PINE Reranking\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00246810Self-Refine\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0DPO\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Debate\n",
      "CoverageDensity (b) Coverage-Density plots for the opposing document.\n",
      "Figure 8: Coverage-density plots for source and opposing documents. PINE exhibits higher variance in coverage,\n",
      "while other methods follow a similar structure.\n",
      "\n",
      "Topic Student Loans\n",
      "Zero-Shot The Right argues that Biden’s student loan relief plan disproportionately benefits those making less than $75,000 per year and that it is a\n",
      "Band-Aid solution that fails to address the root cause of the issue, which is the increasing cost of higher education due to colleges and universities\n",
      "jacking up their prices.\n",
      "Self-Refine The Right argues that President Biden’s student loan relief plan benefits low-income individuals, excludes the truly rich, and is not a giveaway to\n",
      "the elite, while the root cause of the student debt crisis lies in government subsidies to colleges, which have led to inflated tuition prices.\n",
      "Debate The Right argues that President Biden’s student loan relief plan is a reasonable measure to help lower-income individuals, who make up 90% of\n",
      "those benefiting, due to the government’s subsidization of colleges and universities leading to inflated tuition costs.\n",
      "PINE The Right argues that the student debt relief plan is a Band-Aid that will collapse and that colleges should be expected to finance the education\n",
      "they provide.\n",
      "Reranking The Right suggests that Biden’s student debt relief plan is a Band-Aid solution that primarily benefits those making less than $75,000 per year\n",
      "and does not address the root cause of rising college costs, which can be mitigated by requiring institutions to cut costs and improve their financial\n",
      "efficiency.\n",
      "DPO+RR The Right views Biden’s student-loan debt forgiveness plan as a Band-Aid solution that unfairly excludes the truly rich and argues that colleges\n",
      "and universities should be held accountable for their role in inflating prices and should provide debt-free education to students.\n",
      "Topic Mitt Romney\n",
      "Zero-Shot The Left sees the failure of Republicans to convict Trump as a decision made \"functionally at gunpoint\" due to the perceived threat of violence\n",
      "from the extremist right, rather than through conviction or political calculation.\n",
      "Self-Refine The Left views the failure of Republicans to convict Trump as a result of the perceived violent threat from the extremist right, which swayed their\n",
      "decision-making due to fear for their families’ safety, representing one of the most consequential decisions in American history.\n",
      "Debate The Left views Romney’s decision to stay in the Republican Party as a failure to challenge the extremist right and uphold democratic values,\n",
      "having been swayed by perceived threats of violence rather than conviction or principle.\n",
      "PINE The Left believes that Mitt Romney’s decision to leave the Republican Party is a testament to his commitment to principle and his willingness to\n",
      "stand up to the party’s extremist elements.\n",
      "Reranking The Left portrays Romney as revealing a stark contrast between his authentic disdain for the extreme ideologies within his own party in private\n",
      "conversations, versus his careful and cautious language used in public statements, highlighting the profound impact of fear generated by far-right\n",
      "extremism on congressional decision-making processes, specifically in relation to Trump’s impeachment trial and acquittal.\n",
      "DPO+RR The Left views Senator Romney’s decision not to convict Donald Trump due to fears of potential retribution from extremists as indicative of the\n",
      "broader breakdown within the Republican party, illustrating its susceptibility to authoritarianism and inability to stand up against violence-based\n",
      "intimidation despite holding supposedly \"right\" convictions.\n",
      "Topic Democrats Sign Letter Opposing Pelosi\n",
      "Zero-Shot The Left emphasizes that Nancy Pelosi’s experience, legislative skill, and ability to pass significant bills, such as the Affordable Care Act, make\n",
      "her the ideal choice to lead the House Democrats.\n",
      "Self-Refine The Left believes Nancy Pelosi’s experience and effectiveness make her the best choice for Speaker, despite potential risks to LGBTQ rights and\n",
      "ordinary women’s concerns.\n",
      "Debate The Left emphasizes the importance of retaining Nancy Pelosi as Speaker of the House due to her extensive experience, legislative skill, and\n",
      "ability to pass key progressive policies, with her being a crucial figure in landmark legislative successes and there not being a clear alternative.\n",
      "PINE The Left argues that Nancy Pelosi is a highly effective legislator who has consistently been in the top third of most liberal Democrats in the\n",
      "House, and that ousting her would be a blow to ordinary women and LGBTQ people whose concerns will lose priority status if the anti-Pelosi\n",
      "wing prevails.\n",
      "Reranking The Left believes that keeping Nancy Pelosi as Speaker due to her vast experience and proven ability to effectively navigate Congress towards\n",
      "Democratic achievements and maintain party unity is vital amidst any challenges faced or proposed replacements who do not offer comparable\n",
      "qualifications or stronger alliances, especially regarding marginalized groups interests.\n",
      "DPO+RR The Left emphasizes that retaining Nancy Pelosi as Speaker of The House is crucial due to her extensive experience, successful legislation\n",
      "passing track record, ability to effectively oversee the Democratic Party, and importance in maintaining progress on key issues like LGBTQ\n",
      "rights and access to affordable healthcare.\n",
      "Topic China and Coronavirus\n",
      "Zero-Shot The Right suggests that the United States should not completely decouple from China on public-health matters, as past collaboration has been\n",
      "effective, but rather demands institutional changes in China’s domestic food safety policies and calls for greater economic independence from\n",
      "China.\n",
      "Self-Refine The Right believes that China’s disastrous cover-up and propaganda efforts have been matched by the US’s own bumbling response to the crisis.\n",
      "Debate The Right views China’s actions as deceitful and propaganda-driven, but also acknowledges that both the US and China have mishandled the\n",
      "pandemic, with the US’s decoupling from China on public health matters and the US’s bumbling response to the pandemic exacerbating the crisis.\n",
      "PINE The Right argues that the United States should distance itself from China and prioritize economic independence as a permanent national security\n",
      "goal.\n",
      "Reranking The Right perspective emphasizes that the US and the world should be cautious of China’s intentions and actions, particularly its propaganda\n",
      "efforts, and that the US should prioritize economic independence and not rely on China for aid or information, while also acknowledging that the\n",
      "US has made mistakes in its response to the pandemic.\n",
      "DPO+RR The Right emphasizes that while China failed catastrophically during the COVID-9 crisis through its aggressive disinformation campaign and\n",
      "inadequate initial response, the sudden shift towards dependency on China-led relief efforts actually highlights broader risks inherent in the\n",
      "interconnection between nations, necessitating reconsideration of international relationships and prioritization of economic sovereignty.\n",
      "Table 9: Additional example summaries generated by each method.\n",
      "\n",
      "Figure 9: Introduction page for Annotation interface for annotating for article excerpts to evaluate metrics. Annota-\n",
      "tors are provided with definitions and an example annotated document.\n",
      "\n",
      "Figure 10: Example of annotation page for Annotation interface for annotating for article excerpts to evaluate\n",
      "metrics. Annotators are provided with an interface for highlighting sentences in the article.\n",
      "\n",
      "Figure 11: Introduction page of annotation interface for annotating for document and summary excerpts for\n",
      "evaluating method-generated summaries.\n",
      "\n",
      "Figure 12: Example of annotation page for Annotation interface for document and summary excerpts for evaluating\n",
      "method-generated summaries.\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from urllib.parse import urlparse, unquote\n",
    "\n",
    "def extract_pdf_content(pdf_url: str) -> str:\n",
    "    \"\"\"\n",
    "    从PDF文件中提取文本内容\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): PDF文件的路径\n",
    "    \n",
    "    Returns:\n",
    "        str: 提取的文本内容，如果文件不存在或发生错误则返回空字符串\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed = urlparse(pdf_url)\n",
    "    path = unquote(parsed.path)\n",
    "    \n",
    "    # 处理 Windows 路径（去除开头的斜杠）\n",
    "    if path.startswith(\"/\") and len(path) > 3 and path[2] == \":\":\n",
    "        file_path = path[1:]  # 例如：\"/C:/path\" → \"C:/path\"\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # 打开PDF文件\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # 创建PDF阅读器对象\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # 初始化文本变量\n",
    "            text = \"\"\n",
    "            \n",
    "            # 遍历所有页面\n",
    "            for page_num in range(len(pdf_reader.pages)):\n",
    "                # 获取当前页面\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                \n",
    "                # 提取当前页面的文本\n",
    "                page_text = page.extract_text()\n",
    "                \n",
    "                # 添加到整体文本中\n",
    "                text += page_text + \"\\n\\n\"  # 每页之间添加空行分隔\n",
    "            \n",
    "            return text.strip()  # 返回去除首尾空白的文本\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误: 文件 '{file_path}' 不存在\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"错误: 读取PDF文件时发生异常 - {str(e)}\")\n",
    "        return \"\"\n",
    "a = extract_pdf_content(\"file:///C:/Users/Admin/Desktop/papers/2506.15925v1.pdf\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed3e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/C:/Users/Admin/Documents/WeChat%20Files/wxid_f5pfbo9it67z12/FileStorage/File/2025-06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-PyTorch(%E7%AC%AC%E4%BA%8C%E7%89%88)%20(Aston%20Zhang,%20Zachary%20C.%20Lipton,%20%E6%9D%8E%E6%B2%90%20etc.)%20(Z-Library).pdf\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "url = \"file:///C:/Users/Admin/Documents/WeChat%20Files/wxid_f5pfbo9it67z12/FileStorage/File/2025-06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-PyTorch(%E7%AC%AC%E4%BA%8C%E7%89%88)%20(Aston%20Zhang,%20Zachary%20C.%20Lipton,%20%E6%9D%8E%E6%B2%90%20etc.)%20(Z-Library).pdf\"\n",
    "parsed = urlparse(url)\n",
    "file_path = parsed.path  # 直接获取路径部分\n",
    "print(file_path)  # 输出: \"/D:/data/report.pdf\"\n",
    "\n",
    "# 若需要去除开头的斜杠（Windows路径可能不需要）\n",
    "#file_path = file_path.lstrip('/')\n",
    "#print(file_path)  # 输出: \"D:/data/report.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe636ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_260412\\343708852.py:46: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  pdf_links = [result.pdf_url for result in search.results()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_arxiv_paper_links(date_str: str = None) -> list:\n",
    "    \"\"\"\n",
    "    爬取指定日期 arXiv 上 AI 领域的所有论文 PDF 链接\n",
    "    \n",
    "    Args:\n",
    "        str: 指定日期，默认上一个工作日\n",
    "    Returns:\n",
    "        list: 去重后的pdf links\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算日期（默认为上一个工作日）\n",
    "    if not date_str:\n",
    "        today = datetime.today()\n",
    "        offset = 1\n",
    "        while True:\n",
    "            last_working_day = today - timedelta(days=offset)\n",
    "            if last_working_day.weekday() < 5:  # 0-4是工作日\n",
    "                break\n",
    "            offset += 1\n",
    "        date_str = last_working_day.strftime(\"%Y%m%d\")\n",
    "        next_date = last_working_day + timedelta(days = 1)\n",
    "        next_date_str = next_date.strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        date_obj = datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "        next_date = date_obj + timedelta(days=1)\n",
    "        next_date_str = next_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # 构建查询\n",
    "    query = (\n",
    "        \"cat:cs.AI \"\n",
    "        f\"AND submittedDate:[{date_str}000000 TO {next_date_str}000000]\"\n",
    "    )\n",
    "    \n",
    "    # 执行查询\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=10000,  # 假设一天最多10000篇论文，可根据需要调整\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # 提取并返回 PDF 链接\n",
    "    pdf_links = [result.pdf_url for result in search.results()]\n",
    "    return pdf_links\n",
    "\n",
    "a = get_arxiv_paper_links()\n",
    "print(type(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f69b96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_260412\\1276174648.py:46: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  pdf_links = [result.pdf_url for result in search.results()]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://arxiv.org/pdf/2506.18902v1\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_arxiv_paper_links(date_str: str = None) -> list:\n",
    "    \"\"\"\n",
    "    爬取指定日期 arXiv 上 AI 领域的所有论文 PDF 链接\n",
    "    \n",
    "    Args:\n",
    "        str: 指定日期，默认上一个工作日\n",
    "    Returns:\n",
    "        list: 去重后的pdf links\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算日期（默认为上一个工作日）\n",
    "    if not date_str:\n",
    "        today = datetime.today()\n",
    "        offset = 1\n",
    "        while True:\n",
    "            last_working_day = today - timedelta(days=offset)\n",
    "            if last_working_day.weekday() < 5:  # 0-4是工作日\n",
    "                break\n",
    "            offset += 1\n",
    "        date_str = last_working_day.strftime(\"%Y%m%d\")\n",
    "        next_date = last_working_day + timedelta(days = 1)\n",
    "        next_date_str = next_date.strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        date_obj = datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "        next_date = date_obj + timedelta(days=1)\n",
    "        next_date_str = next_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # 构建查询\n",
    "    query = (\n",
    "        \"cat:cs.AI \"\n",
    "        f\"AND submittedDate:[{date_str}000000 TO {next_date_str}000000]\"\n",
    "    )\n",
    "    \n",
    "    # 执行查询\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=10000,  # 假设一天最多10000篇论文，可根据需要调整\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # 提取并返回 PDF 链接\n",
    "    pdf_links = [result.pdf_url for result in search.results()]\n",
    "    return pdf_links\n",
    "\n",
    "a=get_arxiv_paper_links()[0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cbf326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功获取98个唯一arXiv链接\n",
      "['http://arxiv.org/pdf/2506.19852v1', 'http://arxiv.org/pdf/2506.19847v1', 'http://arxiv.org/pdf/2506.19846v1', 'http://arxiv.org/pdf/2506.19843v1', 'http://arxiv.org/pdf/2506.19839v1', 'http://arxiv.org/pdf/2506.19834v1', 'http://arxiv.org/pdf/2506.19825v1', 'http://arxiv.org/pdf/2506.19823v1', 'http://arxiv.org/pdf/2506.19807v1', 'http://arxiv.org/pdf/2506.19794v1', 'http://arxiv.org/pdf/2506.19785v1', 'http://arxiv.org/pdf/2506.19783v1', 'http://arxiv.org/pdf/2506.19777v1', 'http://arxiv.org/pdf/2506.19774v1', 'http://arxiv.org/pdf/2506.19773v1', 'http://arxiv.org/pdf/2506.19769v1', 'http://arxiv.org/pdf/2506.19767v1', 'http://arxiv.org/pdf/2506.19755v1', 'http://arxiv.org/pdf/2506.19753v1', 'http://arxiv.org/pdf/2506.19742v1', 'http://arxiv.org/pdf/2506.19732v1', 'http://arxiv.org/pdf/2506.19726v1', 'http://arxiv.org/pdf/2506.19724v1', 'http://arxiv.org/pdf/2506.19708v1', 'http://arxiv.org/pdf/2506.19702v1', 'http://arxiv.org/pdf/2506.19698v1', 'http://arxiv.org/pdf/2506.19697v1', 'http://arxiv.org/pdf/2506.19689v1', 'http://arxiv.org/pdf/2506.19686v1', 'http://arxiv.org/pdf/2506.19683v1', 'http://arxiv.org/pdf/2506.19652v1', 'http://arxiv.org/pdf/2506.19650v1', 'http://arxiv.org/pdf/2506.19642v1', 'http://arxiv.org/pdf/2506.19635v1', 'http://arxiv.org/pdf/2506.19633v1', 'http://arxiv.org/pdf/2506.19630v1', 'http://arxiv.org/pdf/2506.19621v1', 'http://arxiv.org/pdf/2506.19613v1', 'http://arxiv.org/pdf/2506.19608v1', 'http://arxiv.org/pdf/2506.19599v1', 'http://arxiv.org/pdf/2506.19597v1', 'http://arxiv.org/pdf/2506.19592v1', 'http://arxiv.org/pdf/2506.19591v1', 'http://arxiv.org/pdf/2506.19579v1', 'http://arxiv.org/pdf/2506.19578v1', 'http://arxiv.org/pdf/2506.19573v1', 'http://arxiv.org/pdf/2506.19571v1', 'http://arxiv.org/pdf/2506.19567v1', 'http://arxiv.org/pdf/2506.19563v1', 'http://arxiv.org/pdf/2506.19561v1', 'http://arxiv.org/pdf/2506.19552v1', 'http://arxiv.org/pdf/2506.19549v1', 'http://arxiv.org/pdf/2506.19539v1', 'http://arxiv.org/pdf/2506.19531v1', 'http://arxiv.org/pdf/2506.19530v1', 'http://arxiv.org/pdf/2506.19525v1', 'http://arxiv.org/pdf/2506.19502v1', 'http://arxiv.org/pdf/2506.19500v1', 'http://arxiv.org/pdf/2506.19491v1', 'http://arxiv.org/pdf/2506.19486v1', 'http://arxiv.org/pdf/2506.19484v1', 'http://arxiv.org/pdf/2506.19482v1', 'http://arxiv.org/pdf/2506.19469v1', 'http://arxiv.org/pdf/2506.19468v1', 'http://arxiv.org/pdf/2506.19467v1', 'http://arxiv.org/pdf/2506.19466v1', 'http://arxiv.org/pdf/2506.19465v1', 'http://arxiv.org/pdf/2506.19461v1', 'http://arxiv.org/pdf/2506.19459v1', 'http://arxiv.org/pdf/2506.19433v1', 'http://arxiv.org/pdf/2506.19420v1', 'http://arxiv.org/pdf/2506.19410v1', 'http://arxiv.org/pdf/2506.19408v1', 'http://arxiv.org/pdf/2506.19406v1', 'http://arxiv.org/pdf/2506.19399v1', 'http://arxiv.org/pdf/2506.19387v1', 'http://arxiv.org/pdf/2506.19385v1', 'http://arxiv.org/pdf/2506.19359v1', 'http://arxiv.org/pdf/2506.19358v1', 'http://arxiv.org/pdf/2506.19352v1', 'http://arxiv.org/pdf/2506.19351v1', 'http://arxiv.org/pdf/2506.19343v1', 'http://arxiv.org/pdf/2506.19342v1', 'http://arxiv.org/pdf/2506.19325v1', 'http://arxiv.org/pdf/2506.19315v1', 'http://arxiv.org/pdf/2506.19312v1', 'http://arxiv.org/pdf/2506.19290v1', 'http://arxiv.org/pdf/2506.19283v1', 'http://arxiv.org/pdf/2506.19280v1', 'http://arxiv.org/pdf/2506.19279v1', 'http://arxiv.org/pdf/2506.19269v1', 'http://arxiv.org/pdf/2506.19256v1', 'http://arxiv.org/pdf/2506.19250v1', 'http://arxiv.org/pdf/2506.19235v1', 'http://arxiv.org/pdf/2506.19225v1', 'http://arxiv.org/pdf/2506.19224v1', 'http://arxiv.org/pdf/2506.19220v1', 'http://arxiv.org/pdf/2506.19217v1']\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# 清理链接（去除锚点和查询参数）\n",
    "def clean_link(link):\n",
    "        # 移除#后面的部分（如#community）\n",
    "        link = re.sub(r'#.*$', '', link)\n",
    "        # 移除?后面的查询参数\n",
    "        link = re.sub(r'\\?.*$', '', link)\n",
    "        return link\n",
    "\n",
    "\n",
    "def get_arxiv_paper_links(date_str: str = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    爬取指定日期 arXiv 上 AI 领域的所有论文 PDF 链接\n",
    "    \n",
    "    Args:\n",
    "        str: 指定日期，默认上一个工作日\n",
    "    Returns:\n",
    "        list: 去重后的pdf links\n",
    "    \"\"\"\n",
    "    \n",
    "    # 计算日期（默认为上一个工作日）\n",
    "    if not date_str:\n",
    "        today = datetime.today()\n",
    "        offset = 1\n",
    "        while True:\n",
    "            last_working_day = today - timedelta(days=offset)\n",
    "            if last_working_day.weekday() < 5:  # 0-4是工作日\n",
    "                break\n",
    "            offset += 1\n",
    "        date_str = last_working_day.strftime(\"%Y%m%d\")\n",
    "        next_date = last_working_day + timedelta(days = 1)\n",
    "        next_date_str = next_date.strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        date_obj = datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "        next_date = date_obj + timedelta(days=1)\n",
    "        next_date_str = next_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # 构建查询\n",
    "    query = (\n",
    "        \"cat:cs.AI \"\n",
    "        f\"AND submittedDate:[{date_str}0000 TO {next_date_str}0000]\"\n",
    "    )\n",
    "    \n",
    "    # 执行查询\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=10000,  # 假设一天最多10000篇论文，可根据需要调整\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # 创建client\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    # 提取并返回 PDF 链接\n",
    "    pdf_links = [result.pdf_url for result in client.results(search)]\n",
    "    \n",
    "    #清理PDF链接\n",
    "    pdf_links = [clean_link(link) for link in pdf_links]\n",
    "    print(f\"成功获取{len(pdf_links)}个唯一arXiv链接\")\n",
    "\n",
    "    return pdf_links\n",
    "\n",
    "\n",
    "a = get_arxiv_paper_links()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "545a94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_feishu_sheet_content(doc_token: str, sheet_id: str, range: str, access_token: str) -> list[list[any]]:\n",
    "    \"\"\"\n",
    "    通过飞书开放平台 API 获取电子表格的内容\n",
    "    Reference: https://open.larkoffice.com/document/server-docs/docs/sheets-v3/data-operation/reading-a-single-range\n",
    "    \n",
    "    Args:\n",
    "        doc_token: 电子表格的 token\n",
    "        sheet_id: 工作表 ID\n",
    "        range: 单元格范围，如 \"A1:B2\"\n",
    "        access_token: 访问令牌\n",
    "    \n",
    "    Returns:\n",
    "        表格内容，格式为嵌套列表，例如 [[1, \"a\"], [2, \"b\"]]\n",
    "    \"\"\"\n",
    "    # 构建请求 URL\n",
    "    url = f\"https://open.feishu.cn/open-apis/sheets/v2/spreadsheets/{doc_token}/values/{sheet_id}!{range}\"\n",
    "    \n",
    "    # 设置请求头\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 发送 GET 请求\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        \n",
    "        # 解析 JSON 响应\n",
    "        data = response.json()\n",
    "        \n",
    "        # 提取 values 部分\n",
    "        values = data.get(\"data\", {}).get(\"valueRange\", {}).get(\"values\", [])\n",
    "        return values\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求出错: {e}\")\n",
    "        return []\n",
    "    except (KeyError, ValueError) as e:\n",
    "        print(f\"解析响应出错: {e}\")\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11ccfc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app_access_token': 't-g1046pgDZU3QYWQISLVEBPC3HXOYYNEHO262KPKN', 'code': 0, 'expire': 5166, 'msg': 'ok', 'tenant_access_token': 't-g1046pgDZU3QYWQISLVEBPC3HXOYYNEHO262KPKN'}\n",
      "[1, 'a']\n",
      "[2, 'b']\n"
     ]
    }
   ],
   "source": [
    "from constants import APP_ID, APP_SECRET\n",
    "from utils import get_access_token\n",
    "# 示例参数\n",
    "doc_token = \"SCmtscuAch23u5tHfYLc9BxtnId\"\n",
    "sheet_id = \"d40f7b\"\n",
    "range = \"A1:B2\"\n",
    "access_token = get_access_token(APP_ID,APP_SECRET)[\"access_token\"]\n",
    "\n",
    "# 获取表格内容\n",
    "content = get_feishu_sheet_content(doc_token, sheet_id, range, access_token)\n",
    "\n",
    "# 打印结果\n",
    "for row in content:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d2ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app_access_token': 't-g1046pgDZU3QYWQISLVEBPC3HXOYYNEHO262KPKN', 'code': 0, 'expire': 5443, 'msg': 'ok', 'tenant_access_token': 't-g1046pgDZU3QYWQISLVEBPC3HXOYYNEHO262KPKN'}\n",
      "{'app_access_token': 't-g1046pgDZU3QYWQISLVEBPC3HXOYYNEHO262KPKN', 'code': 0, 'expire': 5443, 'msg': 'ok', 'tenant_access_token': 't-g1046pgDZU3QYWQISLVEBPC3HXOYYNEHO262KPKN'}\n",
      "null\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from constants import (\n",
    "    APP_ID,\n",
    "    APP_SECRET,\n",
    ")\n",
    "from utils import get_access_token\n",
    "from batch_rate_papers import save_to_feishu_sheet\n",
    "\n",
    "\n",
    "user_access_token = get_access_token(APP_ID, APP_SECRET)[\"access_token\"]\n",
    "\n",
    "\n",
    "def add_records_to_feishu_sheet(spreadsheet_token, sheet_id, range, user_access_token, results):\n",
    "    \"\"\"\n",
    "    向飞书表格指定范围写入数据\n",
    "    \n",
    "    Args:\n",
    "        spreadsheet_token: 电子表格的 token\n",
    "        sheet_id: 工作表 ID\n",
    "        range: 写入范围（如 \"A1:B5\"）\n",
    "        results: 要写入的数据（二维列表，如 [[1, \"a\"], [2, \"b\"]]）\n",
    "    \n",
    "    Returns:\n",
    "        dict: API 响应结果，包含操作状态和相关数据\n",
    "    \"\"\"\n",
    "    # 飞书 API 基础配置\n",
    "    base_url = \"https://open.feishu.cn/open-apis/sheets/v2/spreadsheets\"\n",
    "    access_token = user_access_token # 需替换为实际令牌\n",
    "    \n",
    "    # 构建请求 URL（包含完整范围）\n",
    "    url = f\"{base_url}/{spreadsheet_token}/values\"\n",
    "    \n",
    "    # 设置请求头\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # 构建请求体\n",
    "    payload = {\n",
    "        \"valueRange\": {\n",
    "            \"range\": f\"{sheet_id}!{range}\",\n",
    "            \"values\": results\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 发送 PUT 请求\n",
    "        response = requests.put(url, headers=headers, data=json.dumps(payload))\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "        \n",
    "        # 返回 API 响应\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"请求发送失败: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"响应解析失败: {e}\")\n",
    "        return {\"error\": f\"响应解析失败: {response.text}\"}\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 表格相关参数（需替换为实际值）\n",
    "    SPREADSHEET_TOKEN = \"SCmtscuAch23u5tHfYLc9BxtnId\"\n",
    "    SHEET_ID = \"d40f7b\"\n",
    "    RANGE = \"G6:G7\"\n",
    "    \n",
    "    # 要写入的数据（二维列表）\n",
    "    DATA_TO_WRITE = [\n",
    "        [ 1],\n",
    "        [ 1]\n",
    "    ]\n",
    "    \n",
    "    # 调用函数写入数据\n",
    "    result = save_to_feishu_sheet(SPREADSHEET_TOKEN, SHEET_ID, RANGE, DATA_TO_WRITE)\n",
    "    \n",
    "    # 打印结果\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
